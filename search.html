<!DOCTYPE html>
<html>
<head>

    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>Search Result</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />

    <!-- Custom.css -->
    <link rel="stylesheet" type="text/css" href="/assets/built/custom.css" />

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">

    <!-- webfont -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/earlyaccess/nanumgothic.css">

    <!-- syntax.css -->
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />

    <!-- highlight.js -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>

    <!--[if IE]>
        <style>
            p, ol, ul{
                width: 100%;
            }
            blockquote{
                width: 100%;
            }
        </style>
    <![endif]-->
    
    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="Careful Writer" />
    <link rel="shortcut icon" href="https://heuristicwave.github.io/assets/built/images/water-wave-48.png" type="image/png" />
    <link rel="canonical" href="https://heuristicwave.github.io/search" />
    <meta name="referrer" content="no-referrer-when-downgrade" />

     <!--title below is coming from _includes/dynamic_title-->
    <meta property="og:site_name" content="Heuristic Wave Blog" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Search Result" />
    <meta property="og:description" content="Careful Writer" />
    <meta property="og:url" content="https://heuristicwave.github.io/search" />
    <meta property="og:image" content="https://heuristicwave.github.io/assets/built/images/blog-cover.jpg" />
    <meta property="article:publisher" content="https://www.facebook.com/" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Search Result" />
    <meta name="twitter:description" content="Careful Writer" />
    <meta name="twitter:url" content="https://heuristicwave.github.io/" />
    <meta name="twitter:image" content="https://heuristicwave.github.io/assets/built/images/blog-cover.jpg" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Heuristic Wave Blog" />
    <meta name="twitter:site" content="@" />
    <meta name="twitter:creator" content="@" />
    <meta property="og:image:width" content="2000" />
    <meta property="og:image:height" content="666" />

    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "Heuristic Wave Blog",
        "logo": "https://heuristicwave.github.io/"
    },
    "url": "https://heuristicwave.github.io/search",
    "image": {
        "@type": "ImageObject",
        "url": "https://heuristicwave.github.io/assets/built/images/blog-cover.jpg",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://heuristicwave.github.io/search"
    },
    "description": "Careful Writer"
}
    </script>
    <script data-ad-client="ca-pub-6093187208665634" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <!-- <script type="text/javascript" src="https://demo.ghost.io/public/ghost-sdk.min.js?v=724281a32e"></script>
    <script type="text/javascript">
    ghost.init({
    	clientId: "ghost-frontend",
    	clientSecret: "f84a07a72b17"
    });
    </script> -->

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-0FTXSPJZFY"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-0FTXSPJZFY');
    </script>
    <meta name="generator" content="Jekyll 3.6.2" />
    <link rel="alternate" type="application/rss+xml" title="Search Result" href="https://heuristicwave.github.io/feed.xml" />


</head>
<body class="page-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- < default -->
<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<!-- The big featured header, it uses blog cover image as a BG if available -->
<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        
            
                <a class="site-nav-logo" href="https://heuristicwave.github.io/">Heuristic Wave Blog</a>
            
        
        
            <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-about" role="menuitem"><a href="/about/">About</a></li>
    <li class="nav-extracurricular" role="menuitem"><a href="/tag/uncategorized/">Uncategorized</a></li>
    <li class="nav-aws" role="menuitem"><a href="/tag/aws/">AWS</a></li>
    <li class="nav-backend" role="menuitem"><a href="/tag/backend/">Back-end</a></li>
    <li class="nav-devops" role="menuitem"><a href="/tag/devops/">DevOps</a></li>
    <li class="nav-backend" role="menuitem"><a href="/tag/security/">Security</a></li>
    <li class="nav-archive" role="menuitem">
        <a href="/archive">All Posts</a>
    </li>
    <li class="nav-archive" role="menuitem">
        <a href="/author_archive.html">Tag별 Posts</a>
    </li>
</ul>

        
    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
            
        </div>
        
            <a class="subscribe-button" href="#subscribe">Search</a>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full  post page no-image">

            <header class="post-full-header">
                <h1 class="post-full-title">Search Result</h1>
            </header>

            

            <section class="post-full-content">
                <form action="/search" method="get" hidden="hidden">
    <label for="search-box"></label>
    <input type="text" id="search-box" name="query">
</form>

<ul class="mylist" id="search-results"></ul>

<script>
    window.store = {
    
    "hardeneks": {
        "title": "About HardenEKS (install &amp; custom)",
            "author": "HeuristicWave",
            "category": "",
            "content": "본 글은 작년 12월 AWS Samples 깃허브에 릴리즈 된 HardenEKS를 사용해 보며, 설치 및 커스텀 방법에 대하여 작성 글입니다.Intro얼마 전 AWS Blog에 HardenEKS: Validating Best Practices For Amazon EKS Clusters Programmatically라는 글 하나가 올라왔습니다.HardenEKS는 EKS Best Practices Guides (이하 EBPG)를 기반으로 EKS 클러스터를 검사하는 오픈소스 툴입니다. 쿠버네티스 클러스터의 잠재적인 문제를 스캔하고 분석하는 Popeye를 사용해 본 기억이 떠올라, 사용해 보고 느낀 점을 몇 자 적어보겠습니다.HardenEKSHardenEKS 깃허브에는 ‘EKS 클러스터가 EKS 모범 사례를 따르고 있는지 검사’ 한다고 소개되어 있습니다.EBPG에는 글을 작성하는 시점을 기준으로 Security, Reliability, Cluster Autoscaling, Running Windows Containers, Networking, Scalability, Cluster Upgrades에 대하여 가이드하고 있습니다.HardenEKS kubernetes API를 호출하여 스캔을 진행하며, 다음 범주에 대하여 검사가 가능합니다.  cluster_wide          cluster_autoscaling      reliability      scalability      security        namespace_based          reliability      security      👀 체험하기사용 방법은 간단합니다. 다음과 같이 설치하고, EKS에 접근할 수 있는 터미널에서 리포트를 받아보면 끝납니다.# 설치python3 -m venv /tmp/.venvsource /tmp/.venv/bin/activatepip install hardeneks# 리포트 생성 후, 열기hardeneks --export-html report.htmlopen report.html그러나 위 작업만으로는 제한적인 정보로만 보고서가 생성됩니다. 그래서 다음과 같이 최소한의 ClusterRole을 생성해야 합니다.kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata:  name: hardeneks-runnerrules:- apiGroups: [\"\"]  resources: [\"namespaces\", \"resourcequotas\", \"persistentvolumes\", \"pods\", \"services\"]  verbs: [\"list\"]- apiGroups: [\"rbac.authorization.k8s.io\"]  resources: [\"clusterroles\", \"clusterrolebindings\", \"roles\", \"rolebindings\"]  verbs: [\"list\"]- apiGroups: [\"networking.k8s.io\"]  resources: [\"networkpolicies\"]  verbs: [\"list\"]- apiGroups: [\"storage.k8s.io\"]  resources: [\"storageclasses\"]  verbs: [\"list\"]- apiGroups: [\"apps\"]  resources: [\"deployments\", \"daemonsets\", \"statefulsets\"]  verbs: [\"list\", \"get\"]- apiGroups: [\"autoscaling\"]  resources: [\"horizontalpodautoscalers\"]  verbs: [\"list\"]🪓 CustomizeHardenEKS를 사용하여 생성한 리포트에 특정 Namespace 혹은 몇 Rule들을 제외하고 검사를 진행하고 싶다면 커스터마이즈가 필요합니다. 커스터마이징하는 방법을 찾기 위해 hardeneks --help 명령어를 확인해 보면, 아래와 같이 tmp/ 위치에 config.yaml을 default로 적용하고 있다는 사실을 알 수 있습니다.│ --config  TEXT  Path to a hardeneks config file.│                 [default: /tmp/.venv/lib/python3.9/site-packages/hardeneks/config.yaml]Config 값을 조정하기 위해 다음과 같이 my-config.yaml 파일을 생성합니다.cat /tmp/.venv/lib/python3.9/site-packages/hardeneks/config.yaml &gt; my-config.yamlconfig.yaml 파일의 구조는 아주 단순합니다. 아래와 같이 2가지 영역을 수정하여 config 값을 변경합니다.  ignore-namespaces : 는 스캔을 제외할 namespace를 정의  rules : iam, multi_tenancy, network_sucurity 등에 대하여 검사할 rule을 정의제외할 namespace를 기재하는 것은 쉽지만, 어떤 rule을 적용시킬지는 한 번에 찾기 쉽지 않습니다. 만약 여러분이 rule을 수정하고 싶다면, pdoc으로 생성된 HardenEKS Github Pages를 참고하시면 됩니다.Example예를 들어, Spread replicas across AZs and Nodes을 예외 처리하고 싶다 가정하고 적용하는 방법을 소개해드리겠습니다.검색을 활용한 색인을 지원하지 않으므로, 일일이 문서를 타고 들어가 rule을 확인해야 합니다. 😡 해당 과정이 매우 까다로우므로, 각 단계별로 링크를 생성해 두었습니다.  최상단 index 페이지, Sub-modules에서 namespace_based 선택  namespace_based 페이지,  Sub-modules에서 reliability 선택  reliability 페이지, Sub-modules에서 applications 선택  여기까지 진행하면 Functions이 나오는데, 찾고자 하는 rule을 Ctrl + F로 함수 명 검색  schedule_replicas_across_nodes 함수 명이 Spread replicas across AZs and Nodes 해당하는 rule 이름이므로,이를 config.yaml에서 수정  리포트를 생성하는 명령어에서 수정된 config 파일을 옵션으로 적용시키면, 커스텀 하게 바꾼 값들이 적용 예) hardeneks --config &lt;my-config&gt;.yaml --export-html &lt;Report Name&gt;.htmlReport 상단에는 어떤 대상으로 스캔을 진행했는지, 요약 정보가 나옵니다. 검사 결과와 더불어, Resolution을 통해 EBPG의 Link도 함께 안내됩니다.아래 사진은 Example에서 적용시킨 custom.yaml 적용 여부와, rule을 삭제한 결과 화면입니다.OutroHardenEKS는 아주 간단하고 빠르게, 나의 EKS가 EKS 모범 사례대로 운영하고 있는지 확인할 수 있어 아주 편리한 툴인 것 같습니다.비록 Rule 커스텀 과정에서 문서가 불친절했지만, EBPG를 기반으로 자동화된 점검을 한다는 점에서 만족합니다.AWS는 아키텍처 관련 모범 사례를 사용해 학습, 측정 및 구축하는 방법으로, AWS Well-Architected라는 방법론과 도구를 제공합니다.그동안 AWS Well-Architected은 특정 업계 및 기술 도메인에 대해서는 Lenses를 통해 지침을 제공하고 있었지만,EKS 기반 환경의 분석까지는 지원하지 않았습니다. HardenEKS 덕분에, EKS도 Well-Architected를 준수하기 더욱 수월해진 것 같습니다.과거 저는 Kubernetes 진단을 위해 k9s에 통합되어 있는 Popeye를 사용했습니다. HardenEKS와 함께 사용한다면 상호 보완을 이루며, 더 안전하고 신뢰성 있는 EKS 환경을 만드는데 도움이 될 것 같다는 팁을 드리며, 글을 마칩니다!  Popeye 분석 범위 (port mismatches, probes 등 세부적인 설정에 대하여 심각도(Level)  만족도 여부 % 제공)소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해주세요! 😃",
        "url": "/HardenEKS"
    }
    ,
    
    "chart": {
        "title": "Helm Chart Repository in AWS",
            "author": "HeuristicWave",
            "category": "",
            "content": "AWS에서 Helm chart repositories를 운영하는 방법IntroAWS 환경에서 EKS를 활용하여 서비스를 운영하다 보면, manifest 파일들을 관리하기 위해 helm을 사용하게 됩니다.이번 포스팅에서는 Helm chart에 대하여 알아보고 AWS 환경에서 Helm chart를 구축하는 방법에 대하여 이야기해 보겠습니다.해당 포스팅은 OCI(Open Container Initiative) 기반의 Registry를 사용해 차트 패키지를 관리하는 방법은 다루지 않습니다.🏞️ Background knowledgeHelm은 쿠버네티스를 위한 패키지 관리 도구입니다. Chart라는 파일 형식으로 패키징 하며, 차트를 통해 설치, 업그레이드 롤백을 간편하게 해줍니다.Repository는 차트를 모으고 공유할 수 있는 곳으로, 하나의 저장소에서 여러 개의 Chart를 관리할 수 있습니다.공식 차트 저장소 ArtifactHUB를 활용할 수도 있고, 다음과 같이 자신만의 차트를 만들 수도 있습니다.  Local 활용 (helm repo index {PATH}, 로컬을 backend로 index.yaml 파일이 생성)  GitHub Pages를 활용한 Public Helm Chart 구축  GitHub에서 Private Helm chart 저장소 설정참고 : The Chart Repository GuideAmazon S3로 Helm Repository 구축하기Helm을 패키징 하면 tgz 형식의 아카이브 파일이 생성되며, 이런 차트 파일은 주로 Amazon S3와 같은 Object storage를 백엔드로 사용합니다.AWS 환경에서 S3를 사용해 Chart를 구축하는 방법에 대하여 다음 2가지 방법으로 알아보겠습니다.  Helm Project에서 관리되는 ChartMuseum  AWS Prescriptive Guidance, Set up a Helm v3 chart repository in Amazon S3에 소개된 helm-s3🏛️ ChartMuseum using Amazon S3ChartMuseum은 Amazon 외에도 DigitalOcean, Google Cloud, Microsoft Azure 등 다양한 Storage를 백엔드로 지원합니다.ArchitectureHelm Client의 경우 AWS Cloud 내 EC2 인스턴스 혹은 개발자의 Local 작업 환경 모두 가능합니다.Process1. 준비 작업  Installation을 참고하여 GoFish 혹은 curl로 설치  테스트를 위해 mychart라는 임의의 차트 생성 : helm create mychart  helm repository에 담을 차트를 패키지화 : helm package ./mychart2. Chartmuseum 실행 및 Repository 추가  Using with Amazon S3에 기재된 대로, endpoint를 설정하고 IAM 권한을 부여  다음 명령어로 chartmuseum을 실행시키고 Helm Client의 URL에 접속하여 동작 여부 확인  Helm Client가 local인 경우, http://localhost:8080에서 확인 가능    chartmuseum --debug --port=8080 \\--storage=\"amazon\" \\--storage-amazon-bucket=\"my-s3-bucket\" \\--storage-amazon-prefix=\"\" \\--storage-amazon-region=\"us-east-1\"        다음 명령어로 repository를 추가 : helm repo add my-chart http://localhost:8080  helm repo ls 명령어로 확인 가능3. S3에 패키지 업로드  다음 명령어로 helm plugin install https://github.com/chartmuseum/helm-push helm-push 플러그인을 다운로드하고, helm cm-push --help 명령어로 설치 여부 확인  cm-push 명령어로 패키지 업로드 : helm cm-push mychart/ my-chart          ChartMuseum 공식 문서에 소개된 API 호출 방식으로도 업로드가 가능합니다.        정상적으로 패키지가 올라가면 아래와 같이 S3 콘솔에서 확인 가능☁️ Helm v3 chart repository using helm-s3이번에는 AWS 공식 문서에 소개 helm-s3 플러그인을 사용해 AWS Native 하게 구축하는 방법을 알아보겠습니다.Architecture문서에서는 AWS Native 하게 사용하는 방법을 안내하기 위해, 로컬 helm 코드를 운영하기 위해 CodeCommit과, Helm Client로 EC2 인스턴스를 사용하고 있습니다.ChartMuseum 구축 때와 마찬가지로 Helm Client를 개발자의 Local 작업 환경 혹은 CodeCommit을 다른 형상관리 도구로 대체 가능합니다.  🧐 해당 방식에서는 Source code management 목적으로 CodeCommit을 사용하고 있습니다. 즉, 소스 코드 및 리소스의 버전 관리로 CodeCommit을 사용하고 차트 파일의 저장과 배포에 S3을 사용하고 있습니다.   ❗️ CodeCommit을 소스 코드 버전 관리 목적 외에도 ArgoCD와 통합하여 배포에도 사용할 수 있습니다. AWS Workshop의 Helm Repo로 CodeCommit을 사용하는 방법도 있으니 참고하시기 바랍니다.Process1. 준비 작업  고유한 S3 버킷 생성 후, 버킷에서 stable/myapp 폴더를 생성  helm-s3 플러그인 설치 :  helm plugin install https://github.com/hypnoglow/helm-s3.git2. S3 버킷 초기화 및 추가  S3 폴더를 Helm Repository로 초기화 : helm s3 init s3://{YOUR_BUCKET}/stable/myapp  index.yaml 파일이 생성되었는지 확인 : aws s3 ls s3://{YOUR_BUCKET}/stable/myapp  Helm 클라이언트에 Repository 추가 : helm repo add stable-myapp s3://{YOUR_BUCKET}/stable/myapp/  Repository 확인 : helm repo ls3. S3에 패키지 업로드  ChartMuseum 구축에서 사용한 mychart-*.tgz 파일 활용  s3 push 명령어로 패키지 업로드 : helm s3 push ./mychart-0.1.0.tgz stable-myapp  정상적으로 패키지가 올라가면 index.yaml에 업로드 정보가 갱신되며, 다음과 같이 S3에서 확인 가능Outro첫 번째 방법에서는, ChartMuseum을 실행시키고 helm-push 플러그인을 활용하여 차트를 업로드했습니다.반면 두 번째 방법에서는 ChartMuseum과 같은 업로드 계층 없이, helm-s3 플러그인을 활용하여 Direct로 차트를 업로드했습니다.helm-s3 사용하는 방식이 ChartMuseum과 같은 Layer가 없어 사용이 편리합니다.그뿐만 아니라 여러 개의 Chart를 운용하는 경우, ChartMuseum은 --storage-amazon-prefix 옵션을 바꿔가며 실행해야 하지만,helm-s3는 helm repo add 명령어 뒤에 prefix만 바꿔 바로 사용할 수 있으므로 훨씬 유용한 것 같습니다.GCP나 Azure와 같은 다른 Object Storage를 함께 사용하는 게 아니라면, helm-s3로 구축하는 것이 좋겠네요.소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해 주세요! 😃",
        "url": "/Chart"
    }
    ,
    
    "scp": {
        "title": "SCP 알아보기",
            "author": "HeuristicWave",
            "category": "",
            "content": "AWS Service control policies로 하위 계정을 제어하기 위한 고려 사항IntroAWS 환경에서 서비스를 운영하다 보면, 서비스 확장, 보안 및 규정 준수 등 여러 이유로 다수의 AWS 계정을 운용하게 될 수 있습니다. 이때, 다수의 계정을 하나의 조직으로 결합해 중앙에서 계정을 관리하는 AWS Organizations 서비스를 사용하게 됩니다.이번 포스팅에서는 Organizations의 정책 관리 방법 중 하나인 AWS Service control policies(SCPs, 이하 SCP)에 대하여 공식 문서의 내용과 몇 가지 테스트를 통해 이야기해 보겠습니다.🏞️ Background knowledgeSCP를 이해하기 위해 공식 문서(AWS Organizations terminology and concepts)에 소개된 개념을 짚고 넘어가겠습니다.  Organizational : 계정을 통합하기 위해 생성하는 엔터티(entity)  Organizational unit (OU) : 루트 내 계정에 대한 컨테이너. 트리와 유사한 계층 구조를 가지며, OU에 정책을 부여하면 하위에도 동일하게 적용  Management account : organization을 생성하는 데 사용하며, 조직의 모던 계정에 서비스 기능을 제공  Member accounts : Management를 제외한 조직의 나머지 계정, 하나의 organization에 속함기본적인 개념을 이해하고 나서, 아래 그림을 확인하, 모든 개념(Management 계정에서 정책을 만들고, 정책은 OU 혹은 Member accounts에 적용)이 요약되어 있음을 알 수 있습니다.📑 SCPSCP를 다루기 전, 알아야 할 주요 특징  SCP는 조직의 모든 계정에서 사용 가능한 권한을 중앙에서 제어할 수 있습니다.  management account 계정을 제외하고, 하위 Root 계정을 포함한 모든 IAM 사용자 및 역할에 대하여 영향을 미칩니다.  SCP가 모든 서비스와 작업을 허용하더라도, IAM 권한 정책을 부여받아야 액세스할 수 있습니다.SCP 사용 전략SCP 다음 2가지 전략을 통해 작동하게 할 수 있습니다.  Deny list : 기본적으로 모든 actions에 대하여 허용, 특정 서비스 및 actions을 금지  Allow list : 기본적으로 모든 actions에 대하여 금지, 특정 서비스 및 actons을 허용Organizations의 기본 구성은 FullAWSAccess라는 관리형 SCP가 연결되어 Deny list 방식을 채택하고 있으므로, 위 사항을 인식하여 SCP 정책을 수립해야 합니다.‍👩🏻‍🔬 Lab Noteap-northeast-1에서만 ec2 생성을 가능하게 하는 SCP를 작성한다면 순서는 다음과 같습니다.  AWS Organizations &gt; Policies &gt; Service control policies 순으로 접속하여, Create policy  생성된 정책 Targets &gt; Attach 순으로 어떤 OU와 계정에 붙일 것인지 정합니다.정책 예시Resouce 제어{  \"Version\" : \"2012-10-17\",  \"Statement\" : {    \"Sid\" : \"DenyEC2inOtherRegions\",    \"Effect\" : \"Deny\",    \"Action\" : \"ec2:RunInstances\",    \"Resource\" : \"arn:aws:ec2:ap-northeast-1:*:instance/*\"  }}Condition 제어{  \"Version\" : \"2012-10-17\",  \"Statement\" : {    \"Sid\" : \"DenyEC2inOtherRegions\",    \"Effect\" : \"Deny\",    \"Action\" : \"ec2:RunInstances\",    \"Resource\" : \"*\",    \"Condition\" : {      \"StringNotEquals\" : {        \"aws:RequestedRegion\" : \"ap-northeast-1\"      }    }  }}TroubleshootingAllow ErrorsAllow Effect는 Deny와 달리 Resource를 특정해서 정책을 생성할 경우, 아래와 같은 Errors를 반환합니다.Allow는 Resource를 지원하지 않으며, *로만 가능합니다.그러므로 Deny list(기본적으로 모든 actions 허용) 아래에서, Resource와 StringNotEquals을 적절히 조합하여 허용하는 정책을 생성합니다.Failed to attach the policy ‘{policy}’ to X targetpolicy 적용 범위에 대하여 아래와 같이 오류가 있을 경우Targets을 확인하여 문제가 되는 타깃을 하나하나 Detach 해야 합니다. (정책 삭제의 경우도 동일합니다.)Notice  정책 생성 시, Syntax 검사를 통과하더라도 의도대로 동작하지 않을 수 있으므로 검증은 필수입니다.  정책이 복잡해지면, 다른 SCP 정책과 중복되어 의도대로 동작하지 않을 수 있으므로 영향도 체크도 필수입니다.Outro해당 포스팅의 Lab Note는 SCP 사용에 있어, 모든 Side effect를 고려하지 않았습니다. SCP를 운영환경에서 활용하기 위해서는 충분한 검토와 테스트가 필요합니다.제가 AWS IAM을 배우던 당시, “IAM은 완성하고 끝이 아닌 운영과 함께 끊임없이 관리해야 하는 관리 자산”이라는 말을 들었습니다. SCP 역시 운영과 함께 관리해야 하는 관리 자산이라는 점을 언급하며 글을 마치겠습니다.추가적으로 이번 포스팅 이후, Organizations SCP 모범 사례에 대해 더 자세히 알고 싶으시면 다음 게시물을 추천드립니다.👉 Best Practices for AWS Organizations Service Control Policies in a Multi-Account Environment소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해 주세요! 😃",
        "url": "/SCP"
    }
    ,
    
    "fullycertified": {
        "title": "AWS Fully Certified",
            "author": "HeuristicWave",
            "category": "",
            "content": "AWS Certification 12종 취득 회고IntroAWS 자격증 페이지에 들어가 보면 총 12개의 자격증이 소개되어 있습니다.저는 20년 9월 29일 Solutions Architect - Associate 자격증을 시작으로, 23년 4월 10일 Advanced Networking - Specialty 자격증까지 약 3년에 걸쳐 모든 자격증을 취득했습니다.제가 첫 번째 자격증 SAA를 취득한 20년 9월 무렵, 약 1년 3개월 동안 AWS 자격증을 모두 취득한 김태우 님의 AWS 공인 자격증 12종에 전부 합격하면 무엇이 달라질까요?글을 본 당시까지만 해도 저와는 상관없는 이야기인 줄 알았습니다. 태우 님의 합격 후기에는 참 공감 가는 내용이 많으니, 꼭 읽어보시길 추천드립니다!!처음에 저도 후기를 작성하려 했다가 3년이 지난 현시점에서, 태우 님의 합격 후기를 다시보니 정말 비슷하게 느껴지는 게 많아 제가 따로 후기를 적을 필요가 없는 것 같더라고요…그래서 제가 시험에 응시한 순서대로 회고를 진행하기로 했습니다!회고 📝시험 코드는 상단 ‘시험 기록’ 사진의 ‘시험 이름’을 통해 확인할 수 있습니다.2020 ~ 2021  SAA : 첫 시험을 준비하던 당시, 이미 AWS의 VPC, EC2, ELB, S3 등을 활용하여 프로젝트를 진행한 경험이 있었습니다. Udemy에서 판매하는 SAA 강의를 구입해 3개월 정도 수강하고 시험을 봤습니다. 초반에는 강의를 통해 기초적인 개념을 잡으면 큰 도움이 됩니다. 저는 유료 강의를 구매했으나, AWS Technical Essentials라는 굉장히 좋은 무료 강의가 있으니, 이것을 수강하면 모든 시험에 대하여 기초적인 지식을 쌓을 수 있습니다.  SAP : 지문이 길고 헷갈리는 개념이 많아 굉장히 고생했던 기억이 납니다. 자격증 취득 사이트로 유명한 EXAMTOPICS에서 결제하지 않아도 일부 문제를 확인할 수 있습니다.SAP와 관련된 문제를 풀며 관련 공식 문서를 찾아 스스로 해설을 하며 학습했습니다.  DOP : DOP의 경우 개발 관련 경험과 SAP의 지식이 남아있다 보니, SAP와 비교하여 비교적 수월하게 취득할 수 있었습니다. 개발자분들의 경우, SAP 보다 DOP 취득이 훨씬 쉬울 것 같습니다.왜? 12개의 자격증 취득에 도전했나 🤔제가 첫 자격증을 취득하고, 다음 해 2개의 Pro 자격증을 취득할 때만 해도 모든 자격증을 다 취득할 생각은 없었습니다.실제로 제 시험 기록 타임라인을 확인해 보면 20년에 Associate 1개, 21년에 Professional 2개를 취득하고 활동이 없다가 22년 4분기부터 올해 4월까지 9개의 자격증을 몰아서 취득하였습니다.작년 하반기 무슨 일이 있었길래 12개의 자격증 취득을 시작한 것일까요?저는 22년부터 AWS Ambassador로 활동하며, Global Ambassador 들과 교류할 기회를 가질 수 있었습니다.그중 작년 9월 시애틀에서 열린 AWS Ambassador Global Summit 2022의 Lightning Talk 시간에 우에노 상의 발표에 적지 않은 충격을 받았습니다.22년 5월 기준으로 일본에만 11종(22년 집계 당시, SAP on AWS를 제외하고 11종)의 AWS 자격증을 모두 취득한 엔지니어가 무려 340명이나 된다는 사실을…AWS 자격증을 다 취득한다는 이유만으로 AWS의 전문가가 되는 것은 아니지만, 궁금했습니다. 일본 사람들은 왜 그렇게 자격증 취득에 열을 올릴까?‘내가 직접 12개를 다 따보면 알 수 있지 않을까?’라는 생각과 ‘다시는 한국인을 무시하지 마라!(인터넷 밈)’라는 생각이 겹치며, 시애틀에서 돌아오며 모든 자격증을 취득하기로 결심했습니다.아니, 사실은 AWS Ambassador가 모든 자격증을 취득하면 자격이 주어지는 Gold Jacket Club이 부러웠습니다. 👉 관련 링크2022  DVA : DOP를 수월하게 딴 기억이 있어, 상대적으로 만만해 보이는 Associate 단계에 도전했습니다.  CLF : 12종의 자격증 중 가장 기초 과정이라 별다른 공부 없이 바로 도전했습니다. 이 당시 이미 Pro 자격증 2개가 있는 저에게는, 쉬어가기 찬스와 같은 느낌이었습니다.  SCS : 실습형 시험이 존재하는 SOA를 바로 도전하기는 두려워, Specialty 중에서도 제가 가장 자신 있었던 ‘보안’에 먼저 도전했습니다.과거 보안 기사 필기를 무난하게 합격한 경험(실기는 떨어짐😭)이 있는 저에게는 기존 보안 지식에 AWS 보안 솔루션 지식만 탑재하면 되는지라 그리 어렵지 않았습니다.SCS 이후 저는 AWS re:Invent 2022에 참석하게 됩니다. 행사장에는 ‘AWS Certification Lounge’라고 AWS 자격증이 하나라도 있다면, 간식과 아래와 같은 사진을 찍을 수 있는 라운지가 있습니다.당시 6개의 자격증을 보유했으므로, 손가락 6개를 펴고 사진을 찍었습니다.(S3 버킷과도 📸) 다음에 또 찍으러 가면, 12개는 어떤 포즈를 취해야 할까요?2023  DBS : 라스베이거스로 가는 비행기 안에서도 공부했는데, 연말에 느슨해져서 그런지 SCS 이후 3개월이 더 걸렸습니다. Database와 관련한 기초 지식 위주의 문제라 Specialty 6종 중에서는 가장 무난한 시험 같습니다.  SOA : C02로 시험이 개편되고 AWS의 첫 실습형 시험이 두렵게 느껴져, 응시를 매번 미뤘습니다. 이후 업무를 하며, 나름 AWS Systems Manager의 기능을 이것저것 사용해 보았다는 사실에 자신 있게 도전했습니다.역시 Associate는 Associate입니다. Pro와 비교하여 간단하게 답이 도출되고, 걱정했던 실습형 시험도 콘솔 환경을 만지작거리다 보면 답을 제출할 수 있습니다.13인치 맥북으로 시험을 응시하니, 실습 환경에서 제출 버튼이 보이지 않아(시험 환경 외부 스크롤 바로 조정) 애를 먹었던 것 외에는 가장 재미있었던 시험이었습니다.  DAS : 해당 도메인에 대하여 관련 지식이 가장 부족해, 시험을 준비하며 가장 막막했던 시험이었습니다. 또한, 이쯤 되니 퇴근 이후 및 주말에 자격증 공부를 하는 것이 매우 지겨웠습니다.DAS의 경우 SOA 취득 이후 2주 뒤에 합격했으나, 실제로는 SOA만 공부하기 너무 지겨워 SOA, DAS, MLS를 돌아가며 공부했습니다.  MLS : ML 관련 도메인 지식이 없다면 굉장히 어렵습니다. 저는 과거 혼자 공부하는 머신러닝+딥러닝 &amp; 케라스 창시자에게 배우는 딥러닝두 권의 책으로 ML과 딥러닝에 대하여 학습한 경험이 있습니다. 운 좋게도 저는 ML 엔지니어 동기에게 과외를 받아 해당 시험을 통과할 수 있었습니다.  PAS : SAP 지식이 필요할 것 같지만 대부분의 문제 출제 포인트가 DR 전략 및 기본적인 고가용성을 보장하기 위한 설계에 관한 문제라 비교적 수월했습니다.부족한 Sap on AWS 지식을 채우기 위해서, SAP 가이드 문서와 AWS Skill Builder에 올라온 SAP 강의를 수강했습니다.  ANS : 자격증을 취득하고 다음 자격증을 취득하기까지, 가장 짧은 시간이 소요된 시험입니다. 시험이 쉬워 빠르게 딴 것이 아니라, 11개를 취득하고 나니 빨리 12개를 취득하고 싶은 엄청난 동기 부여가 생겨 출퇴근 지하철, 주말, 퇴근 이후 모든 시간을 투자했습니다. 또한, 3년간 AWS 시험공부와 업무를 하다 보니 대부분 알고 있는 네트워크 지식이라 비교적 수월하게 합격할 수 있었습니다.유튜브 주도 학습 📽️유튜브 주도 학습은 유튜브 중독자인 제가 자주 사용하는 말입니다. 유튜브에는 AWS가 올려놓은 강의 영상이 무척 많습니다.AWS가 제작한 영상은 자격증 시험을 준비하는데도 굉장한 도움이 됩니다. 특히 DAS 시험을 준비할 때, 실시간 스트리밍 분석 : Amazon Kinesis Data Analytics Deep Dive - 전소영 &amp; 주혜령, AWS 영상이 엄청난 도움이 되었습니다.AWS 채널과 달리, AWS Korea 채널의 영상에는 Timestamp가 제공되지 않습니다. 그래서 저는 제가 학습한 일부 영상에 대하여 아래와 같이 타임스탬프를 댓글로 남겨 둡니다.이렇게 하면, 나중에 기억이 흐릿해질 때 다시 찾아와 빠르게 지식 보충이 가능할뿐더러, 그냥 영상을 주입식으로 시청할 때보다 머릿속에 오래 남아있게 되는 것 같습니다.Thanks To 💐12종의 자격증을 취득해나가며, 각 분야에서 도움을 준 고마운 분들이 많습니다. 고맙습니다 🙏12종 도전을 시작하도록 열정과 응원을 준, 일본의 Ambassador Ueno, Kumagai 지루한 여정 동안, 퇴근 이후까지 남으며 스터디메이트가 되어준 nuatmochoi, 길팡, MLS 과외해준 ksh, 내 AWS 학습에 빠질 수 없는 조력자 SSH",
        "url": "/FullyCertified"
    }
    ,
    
    "srd": {
        "title": "SRD Protocol 알아보기",
            "author": "HeuristicWave",
            "category": "",
            "content": "Elastic Network Adapter (ENA) Express를 지탱하는 SRD 프로토콜 Get read with me~ 🧐🚨 이번 포스팅은 SRD에 대하여 잘못 설명하고 있는 내용이 매우 많을 수 있음을 알립니다.해당 포스팅은 SRD와 관련된 논문을 이해하기 위해 공부한 과정을 담은 산출물로 봐주세요!Intro작년 11월 28일 What’s New with AWS?에는 ENA Express라는 기술을 사용 가능해졌음을 알렸습니다. ️🔗 Introducing Elastic Network Adapter (ENA) Express for Amazon EC2 instancesENA Express를 사용하면, single flow 대역폭을 5 Gbps에서 최대 25 Gbps까지 늘릴 수 있다고 합니다. 해당 기능을 활성화시키는 방법은 AWS News Blog에 잘 소개되어 있습니다. ️🔗️ New – ENA Express: Improved Network Latency and Per-Flow Performance on EC2어떻게 ENA Express는 비약적인 성능 향상을 일으킬 수 있었을까요? 이번 포스팅에서는 아마존의 독자 프로세서(Graviton)를 만든 Annapurna Labs가 IEEE에 개재한 paper를 통해 ENA Express 기술을 지탱하는 SRD 프로토콜에 대하여 알아보겠습니다. ️🔗️ A Cloud-Optimized Transport Protocol for Elastic and Scalable HPC📄 모양은 Paper에 실린 내용을 DeepL과 papago 번역을 바탕으로 요약한 내용이며, 🗣️ 모양에서 배경지식과 부연 설명 등을 언급합니다.Abstract📄 안나푸르나 연구소는 현재 상용되는 multitenant 데이터 센터 네트워크는 부하의 불균형(load imbalance) 및 일관되지 않은 지연 시간 등의 제약 사항을 극복할 수 있도록 새로운 네트워크 전송 프로토콜인 Scalable Reliable Datagram (SRD)를 만들었다고 합니다.SRD는 패킷 순서를 유지하는 대신, overload된 경로를 피하며 가능한 많은 네트워크 경로를 통해 패킷을 전송합니다. SRD는 지터를 최소화하고 네트워크 혼잡 변동에 가장 빠르게 대응하기 위해 Nitro 네트워킹 카드에 구현되었습니다.SRD는 AWS EFA 커널 바이패스 인터페이스를 통해 HPC(고성능 컴퓨팅)/ML 프레임워크에서 사용됩니다.  Multitenant : 서버 리소스가 서로 다른 사용자 간에 분할되는 공유 호스팅  Jitter : 네트워크에서 종단 간 지연 시간에 따른 변동성에서 측정된 latency의 변화🗣️ 초록에 SRD의 탄생 배경이 잘 요약되어 있지만, “EFA 커널 바이패스 인터페이스를 통해~”라는 부분에 대하여 부연 설명을 몇 자 적어보겠습니다.설명하기 앞서, Enhanced Networking에 대하여 언급하겠습니다. 향상된 네트워킹은 더 높은 대역폭, 더 높은 PPS(초당 패킷) 성능 및 지속적으로 더 낮은 지연시간을 제공합니다.이를 지원하기 위해 Elastic Network Adapter(ENA)와 Intel 82599 Virtual Function (VF) interface 메커니즘을 사용하는 방법이 있습니다.사진의 왼쪽 부분은 언급한 2가지 방법 중 ENA software stack입니다. 애플리케이션은 MPI(Message Passing Interface)를 사용하여 시스템의 network transport와 정보를 주고받습니다(interface).이 방법은 운영체제의 TCP/IP 스택과 ENA 드라이버를 사용해 네트워크 통신을 가능하게 합니다.반면 오른쪽의 EFA는 Libfabric API를 통해 인터페이스 하므로 운영체제 커널을 우회하고 EFA 장치와 직접 통신해 오버헤드가 줄어들게 됩니다. ENA와 EFA는 향상된 네트워킹 성능을 제공함으로써, 고성능 컴퓨팅 작업과 기계학습 등에 적합합니다.  Enhanced networking on Linux  EFA basics서론📄 AWS는 상용 이더넷 스위치를 사용해 equal-cost multipath (ECMP) 라우팅으로 high-radix Folded Clos topology를 구축합니다.이 방식은 TCP의 플로우 별 순서를 유지하는데 유용하지만, 네트워크 사용률이나 흐름 속도(rate)를 고려하지 않습니다.해시 충돌은 일부 링크에 “핫스폿”을 발생시켜 경로 전반에 걸쳐 균일하지 않은 부하 분산, 패킷 드롭, 처리량 저하, 높은 대기 시간(high tail latency)을 유발합니다.패킷 지연과 패킷 드롭은 HPC/ML 애플리케이션의 요건인 저 지연을 방해하며, 효율을 떨어뜨립니다. 하나의 이상 값(outlier)이 발생하면 전체 클러스터가 대기 상태로 유지되어 암달의 법칙에 따라 확장성이 제한됩니다.  ECMP : 하나의 목적지로 패킷 라우팅을 수행하면서 여러 개의 경로를 선택하는 라우팅 기법  Amdahl’s law : 다중 프로세서를 사용할 때 이론적 속도 향상을 예측하는 법칙🗣️ 서론에서 제시된 전통적인 TCP의 문제점에 대하여 AWS re:Invent 2022 영상에서 동영상과 함께 굉장히 잘 설명하고 있습니다.꼭! 해당 영상을 시청하여 TCP 혼잡(Congestion)에 대하여 확인하시기 바랍니다.Why Not TCP📄 TCP는 인터넷이 시작된 이래 대부분의 통신에 최적의 프로토콜이지만, 지연 시간에 민감한 처리에는 적합하지 않습니다.데이터 센터에서 TCP의 경우, 최상의 round-trip latency가 25μs 일 수 있지만, 혼잡 시의 latency outlier는 50ms에서 수초 사이가 될 수 있습니다.해당 증상의 주원인은 손실된 TCP 패킷의 재전송입니다.Why Not RoCE📄 이더넷을 통한 InfiniBand라고도 하는 RoCE(RDMA over Converged Ethernet)는 이론적으로는 AWS 데이터 센터에서 TCP의 대안을 제공할 수 있습니다.그러나, InfiniBand 전송은 AWS(대규모 네트워크) 확장성 요구사항에 적합하지 않다는 것을 알게 되었습니다.🗣️ RoCE의 배경지식 이해를 돕기 위해, HUAWEI의 기술 문서를 링크로 첨부합니다.해당 문서에서 설명하는 RDMA(RemoteDirect Memory Access) 네트워크의 유형과 구조와 TCP/IP의 비교 설명이 해당 문단의 이해에 큰 도움이 되었습니다.Our Approach📄 TCP나 다른 전송 프로토콜은 AWS가 필요로 하는 성능 수준을 제공하지 않기에, 하이퍼 스케일 데이터 센터에 최적화된 SRD(네트워크 전송 프로토콜)을 설계하기로 했습니다.SRD는 여러 경로의 로드 밸런싱과 패킷 손실 또는 링크 장애(link failures)로부터 빠른 복구 기능을 제공합니다. SRD는 일반 이더넷 스위치에서 표준 ECMP 기능을 활용하며, 패킷 캡슐화를 조작하여 송신자가 ECMP 경로 선택을 제어합니다.SRD는 특수한 혼잡 제어 알고리즘을 사용하여 패킷 손실 확률을 줄이고 재전송 시간을 최소화하는 등의 성능 향상을 이뤘습니다.SRD를 AWS Nitro 카드에 구현 함으로서, 물리적 네트워크 레이어와 가깝게 두어 호스트 OS 및 하이퍼바이저에서 주입되는 성능 노이즈를 피할 수 있게 했습니다.SRD는 EFA PCIe 디바이스로 호스트에 노출되며, Amazon EC2 인스턴스에서 HPC 응용 프로그램 및 ML 분산 훈련을 실행할 수 있게 합니다.EFA는 운영 체제(OS) 바이패스 하드웨어 인터페이스를 사용하여 인스턴스 간 통신 성능을 향상시키는 “유저 스페이스 드라이버”를 제공합니다.  Nitro Card : 최신 EC2 서버는 메인 시스템 보드와 하나 이상의 Nitro 카드로 구성됩니다. EC2 서비스에서 사용하는 모든 외부 제어 인터페이스를 구현합니다.또한 소프트웨어 정의 네트워킹, Amazon EBS 스토리지 및 인스턴스 스토리지를 제공하는 데 필요한 것과 같은 모든 I/O 인터페이스를 제공합니다.  PCIe : 컴퓨터의 여러 부품들이 서로 통신하는 데 사용되는 인터페이스🗣️ 해당 부분에서는 SRD가 기존 TCP의 제약 사항을 극복하고 어떻게 구성되었는지 설명합니다. 위 설명과 함께 첨부된 Figure 1 그림을 보면, 기 언급된 내용을 확인할 수 있습니다.(SRD는 기존 EFA의 software stack 동일하게, PCIe 디바이스로 호스트에 노출되며 OS를 바이패스(우회) 하는 인터페이스를 제공)이어서 나오는 디자인 부분에서 더 상세한 설명을 알아보겠습니다.SCALABLE RELIABLE DATAGRAM DESIGNMultipath Load Balancing📄 SRD는 다중 경로를 지원하지 않는 레거시 트래픽과 함께 네트워크를 공유하기 때문에, 각 경로의 round-trip time(RTT) 정보를 수집하여 과부하가 발생한 경로를 피합니다.또한 SRD는 네트워크 링크 장애 발생 시, 전체 라우팅 업데이트를 기다리지 않고 패킷 재전송 경로를 변경하여 빠르게 복구합니다.🗣️ 해당 부분에서는 패킷 손실 가능성을 줄이기 위해 트래픽을 사용 가능한 경로에 분산이 필요한 이유와 SRD가 어떻게 여러 경로로 데이터를 분산시키는지에 대해 설명하고 있습니다.AWS re:Invent 2022 영상의 SRD 작동 원리를 설명할 때, 다음과 같이 언급합니다.SRD works by using an ECMP like packet SPRING(Source Packet Routing in Networking) mechanism.즉, 라우팅 기법으로 ECMP를 채택하여 네트워크의 중간 노드에 의존하지 않고 네트워크의 특정 노드 및 링크 세트를 통해 패킷을 조정하는 SPRING 메커니즘과 같이 동작한다고 합니다.(특정 노드에 의존하지 않으므로, TCP의 해시 충돌로부터 발생한 ‘핫스폿’ 문제 회피)Out of Order Delivery📄 여러 경로를 통해 트래픽을 균등하게 분산시키면 대기 시간이 감소하고 패킷 드롭을 방지하는 데 도움이 되지만, large 네트워크에서는 패킷 도착 순서가 잘못될 수 있습니다.패킷 순서를 복원하는 것은 비용이 많이 드는 작업(평균 대기 시간이 증가하거나 큰 버퍼가 필요)이므로, 순서가 맞지 않더라도 패킷을 호스트에 전달하기로 했습니다.애플리케이션이 순서를 벗어난 패킷을 처리하는 것은 전송 계층에 메시지 경계가 불투명한 TCP와 같은 바이트 스트리밍 프로토콜에서는 불가능하지만 메시지 기반 시맨틱을 사용하면 쉽습니다.흐름별 순서 지정 또는 기타 종류의 종속성 추적은 SRD 위의 메시징 계층에서 수행되며, 메시징 계층의 시퀀싱 정보는 패킷과 함께 다른 쪽으로 전송되어 SRD에게는 불투명(opaque) 합니다.🗣️ 데이터를 연속적인 바이트로 스트림을 보내는 TCP 통신의 개념만 있는 제게는 이 부분을 이해하기 상당히 어려웠습니다. (지금도 제대로 이해하고 있지 못하고 있을 수도 있습니다. 😂)패킷의 순서가 있는 프로토콜은 수신 측에서 재조립 과정이 있기에 비용(리소스)과 시간이 필요합니다. 하지만 메시지 기반의 SRD는 순서를 맞출 필요가 없으며 이 작업을 메시징 계층에 위임합니다.때문에 SRD는 메시지 레이어의 작업이 일어나는 방식을 파악하지 않기에 ‘opaque(불투명)’하다고 표현합니다.  An Introduction to Semantic Routing : 해당 문단의 의미를 파악하는데 가장 도움이 된 Paper입니다. SRD가 채택한 ‘메시지 기반 시맨틱’에 대한 정보가 부족해 어려움을 겪고 있을 때, 이 문서에 나오는 시맨틱 라우팅 개념이 도움이 되었습니다.Congestion Control📄 다중 경로 분산은 네트워크 내 중간의 스위치 부하를 줄이지만, incast(다수의 흐름이 스위치의 동일 인터페이스에 집중되어 해당 인터페이스의 버퍼 공간을 고갈시켜 패킷 손실을 초래하는 트래픽 패턴) 혼잡 문제를 줄이는 데 도움이 되지 않습니다.Spraying(경로 분산)은 발신자의 링크 대역폭에 의해 제한되더라도 동일한 발신자의 micro-bursts가 다른 경로에 동시에 도착할 수 있기 때문에 인캐스트 문제를 악화시킬 수 있습니다.따라서 다중 경로 전송에 대한 혼잡 제어는 모든 경로에서 총 큐잉을 최소화하는 것이 핵심입니다.SRD 혼잡 제어의 목표는 최소한의 in-flight bytes로 대역폭을 분배하여 큐가 쌓이는 것과 패킷 드롭을 방지하는 것입니다. 이는 BBR과 다소 유사하지만 데이터 센터 다중 경로를 추가로 고려합니다. 이는 연결 당 동적 전송률 제한과 inflight 제한을 기반으로 합니다.발신자는 전송 속도와 RTT 변경 사항도 고려합니다. 대부분의 경로에서 RTT가 증가하거나 예상 속도가 전송 속도보다 낮아지면 혼잡이 감지됩니다.이런 방법으로 모든 경로에 영향을 미치는 연결 전체의 혼잡을 감지하며, 개별 경로의 혼잡은 경로 재지정을 통해 독립적으로 처리합니다.  in-flight bytes : 전송되었지만, 아직 ACK가 되지 않은 패킷🗣️ 다중 경로 분산은 중간 스위치 부하를 줄이지만, incast 혼잡 문제를 해결하지 못합니다. 대신, 모든 경로에서 총 큐잉을 최소화하여 혼잡 제어를 해야 합니다.SRD 혼잡 제어는 최소한의 in-flight bytes로 대역폭을 분배하고, 큐가 쌓이는 것과 패킷 드롭을 방지하는 것이 목표입니다.아울러 데이터 센터 다중 경로를 추가로 고려하여 연결 전체의 혼잡을 감지하고, 개별 경로의 혼잡은 경로 재지정을 통해 처리합니다.USER INTERFACE: EFAEFA as an Extension of Elastic Network Adapter📄 Nitro 카드에는 클래식 네트워크 장치를 호스트에 제공하는 동시에 AWS VPC 용 데이터 플레인을 구현하는 ENA PCIe 컨트롤러가 포함되어 있습니다.Enhanced Networking은 하이퍼바이저의 개입 없이 고성능 네트워킹 기능을 제공하며, 기존의 반가상화 네트워크 인터페이스 보다 더 높은 성능을 제공합니다. EFA는 HPC/ML에 적합한 Nitro VPC 카드가 제공하는 추가 옵션 서비스입니다.  데이터 플레인 : 서비스의 기본 기능을 제공 예) 실행 중인 EC2 instance 자체, EBS 볼륨 읽기/쓰기, S3 버킷 객체 GET/PUT, Route 53 DNS queries 응답/health checks 수행🗣️ 해당 부분은 초록에서 언급한 EFA 배경지식을 알고 있는 것으로 충분합니다. Nitro 카드가 제공하던 Enhanced Networking의 방법 중 ENA와 EFA가 있습니다.EFA SRD Transport Type📄 모든 EFA 데이터 통신은 queue pairs(QPs)를 통해 이뤄집니다. QP는 전송 큐와 수신 큐를 포함하는 주소 지정이 가능한 엔드포인트 사용자 공간에서 직접 메시지를 비동기적으로 보내고 받는데 사용됩니다.대규모 클러스터에서 모든 프로세스 간의 모든 연결을 설정하려면 많은 QP가 필요하지만, EFA SRD 전송은 QP의 수를 줄일 수 있습니다. SRD는 InfiniBand reliable datagram(RD) 모델과 유사하지만, 메시지 크기를 제한하고 순서에 맞지 않게 전달하여 RD의 한계를 없앴습니다.따라서 head-of-line blocking을 생성하지 않고도 애플리케이션 흐름이 서로 간섭하지 않고 다중화될 수 있습니다.  SRD : 해당 부분에서 참고로 소개된 SRD가 필요한 QP 수를 줄이는 방법을 기재한 문서.  Head-of-line blocking : 패킷 라인에서 첫 번째 패킷에 의해 큐에 보류될 때 발생하는 성능 제한 현상🗣️ SRD의 방식과 유사한 InfiniBand에서는 QP는 비용이 많이 드는 리소스뿐만 아니라, 동일한 목적지 QP로 순서대로 전달해야 하는 복잡성이 있습니다.그러나 SRD는 순서에 맞지 않게 전달하는 특성으로 인해 기존(RD) QP의 복잡성이 줄고 결과적으로 QP의 수를 줄게 합니다.Out of Order Packet Handling Challenges📄 EFA SRD QP 의미론(semantics)은 EFA 상위 레이어 처리에 대해 unfamiliar 순서 지정 요구 사항을 도입했고, 이를 “Messaging Layer”라고 합니다. 메시지 계층은 일반적으로 HPC 애플리케이션에서 네트워크 사항(specifics)을 추상화하는 데 사용됩니다.이 새로운 기능은 신뢰성 레이어가 오프로드되기 때문에 TCP와 같은 전송 구현보다 경량화되어 있습니다.이상적으로는 메시징 레이어가 수행하는 버퍼 관리 및 흐름 제어는 애플리케이션과 긴밀하게 결합되어야 하는데,이는 사용자 버퍼 관리 기능이 있는 user-space 네트워킹을 이미 지원하고 HPC와 같은 애플리케이션에 주로 초점을 맞추고 있기 때문에 실현 가능합니다.메시지 의미론(semantics)을 사용하면 대규모 전송을 위해 메시지 세그먼트가 순서를 벗어난 상태로 도착하면 데이터 복사가 필요할 수 있습니다.이는 커널 버퍼에서 사용자 버퍼로 복사해야 하는 TCP보다 나쁘지 않습니다. EFA에서는 이 복사본를 RDMA 기능(이 글의 범위를 벗어남)을 사용하여 회피합니다.🗣️ EFA SRD QP semantics는 “Messaging Layer”라는 새로운 기능을 도입했는데, 신뢰성 레이어가 offload 되어 TCP 보다 경량화되어 있다고 합니다.상단 Our Approach의 그림에도 나오듯이, SRD는 신뢰성 계층을 하드웨어(EFA device)로 오프로드 시켰습니다. 일반적으로 신뢰성과 관련한 기능은 TCP/IP 스택의 전송 계층에서 수행하지만,EFA에서는 이를 하드웨어에 위임하게 구성했습니다.SRD PERFORMANCE EVALUATION📄 동일한 서버 세트에서 AWS 클라우드의 TCP(기본 구성 사용)와 EFA SRD 성능을 비교했습니다. (제약사항 및 실험 범위는 원문 참고)Incast FCT and Fairness📄 송신자가 barrier를 사용하여 각 전송을 거의 동시에 시작할 때 EFA/SRD 또는 TCP를 통해 MPI bandwidth 벤치마크를 실행했습니다.아래 그림은 각각의 전송 크기에 대한 이상적인 FCT와 최대 FCT를 나타냅니다. SRD FCT는 매우 낮은 지터로 최적에 가까우며, 최대 시간이 이상보다 3~20배 높을 경우 TCP FCT는 노이즈가 발생합니다.  barrier : 일종의 동기화 방법, 스레드/프로세스가 다음 단계를 시작하기 전에 모든 프로세스가 준비될 수 있도록 보장  FCT(Flow Completion Time) : SRD와 TCP에 대한 흐름 완료 시간다음 그림은 2MB 전송에 대한 FCT의 CDF를 보여줍니다. 최소 재전송 시간제한이 50ms이므로 50ms를 초과하는 TCP tail latency는 재전송을 반영합니다.50ms 미만의 샘플만 보더라도(즉, 지연이 타임아웃으로 인한 것이 아닌 경우) 많은 수의 샘플이 이상적인 값보다 3배 이상 높습니다.  Tail Latency : 상위 백분위 응답시간(percentile), 아래 그림에서는 기울기가 완만해지는 우상단 꼬리 모양 부분이 해당🗣️ 해당 지표에 대한 설명을 이해하기 어려웠지만, EFA가 약 12ms에 100% 도달한 반면 TCP는 3배 이상 되는 약 23 ~ 48ms 부근에서 도달한다는 것으로 이해했습니다.Flow Throughput Under Persistent Congestion Incast📄 (타임아웃으로 인한 long tail은 무시하더라도) TCP의 높은 FCT 편차(variance)를 이해하기 위해, 인캐스트 하에서 각각의 flow 처리량을 조사(exam) 했습니다.다음 그림은 데이터를 지속적으로 전송할 때의 각 흐름의 TCP 및 SRD 처리량을 보여입니다.SRD 처리량은 모든 흐름에서 일정하고 이상에 가까운 반면, TCP 처리량은 변동이 심하고 일부 흐름은 예상(2 Gb/s로 설정)보다 평균 처리량이 훨씬 낮습니다.Multipath Load Balancing📄 동일 랙에 위치한 8대의 서버에서 다른 랙의 8대의 서버로 플로우를 실행한, 상호 연관된 비교적 단순한(less demanding) 사례는 다음 그림과 같습니다.TOR 스위치 업링크는 50%로 활용되며, 다운링크는 하나의 발신자만 수신자에게 보내기 때문에 혼잡하지 않습니다.  TOR(Top of Rack) : 랙에 설치된 서버들에 대한 트래픽을 수용하기 위해 배치된 스위이어지는 그림은 8개 수신자 중 한 곳의 모든 흐름에 대한 TCP 및 EFA의 FCT를 보여줍니다.이상적인 로드 밸런싱을 사용하면 혼잡이 전혀 발생하지 않겠지만, inter-switch 링크에 대한 균일하지 않은 ECMP 밸런싱으로 인해 TCP에서 혼잡과 패킷 드롭이 발생했습니다.TCP 중앙값(Median) 지연 시간은 매우 가변적이며 평균은 예상(점선)보다 50% 높은 반면, 꼬리 지연 시간은 예상보다 1~2배 높습니다.SRD FCT 중앙값은 이상적인 수준보다 15% 높으며, 최대 SRD FCT는 평균 TCP FCT보다 낮습니다.🗣️ 지금까지 몇 가지의 실험을 통해 SRD가 TCP보다 더 개선되었다는 것을 확인시켜 줍니다. 이 Paper에서 소개하는 실험 외에도 AWS re:Invent 2022 영상의 벤치마크에 대하여 설명하는 영상을 참고해 보세요.(Throughput과 Tail latencies에서 우위를 가졌습니다. 해당 영상에서 TCP는 ENA를 SRD는 ENA Express를 의미합니다.)CONCLUSION📄 EFA는 HPC/ML 애플리케이션들을 AWS 퍼블릭 클라우드에서 대규모로 실행할 수 있습니다. SRD를 이용하여 지연 시간이 일관되게 낮아지고 tail latency가 TCP보다 더 낮아집니다.Nitro 카드에서 SRD 다중 경로 로드 밸런싱 및 혼잡 제어를 실행하면 패킷이 끊어질 가능성이 줄어들고, 끊어짐으로부터 더 빠르게 복구할 수 있습니다. 이러한 기능은 네트워크 인터페이스 카드와 호스트 소프트웨어의 여러 계층 간의 기능 분할을 통해 달성됩니다.🗣️ 결론 부분은 제가 이 논문을 읽으며 느낀 감정을 몇 자 적어보겠습니다.AWS가 기존 데이터 센터가 사용하는 TCP의 한계를 극복하기 위해, 기존에 존재하던 InfiniBand, RD 등의 기술들을 참고하여 SRD를 탄생시킨 부분이 매우 흥미롭습니다.이 논문에 2020년 11~12월 경에 소개되었는데는, 2년여 뒤 상용화된 제품(ENA Express)까지 내놓게 되는 과정을 확인하니 너무 재미있네요.Outro이 글은 올해 작성한 글 중에서도, 글감을 떠올리고 실제 글로 탄생하기까지 가장 오랜 시간이 걸렸습니다. 처음 시도해 보는 논문 리뷰에 대하여 어떤 식으로 글을 작성할지 굉장히 많은 고민을 했습니다.단순히 한국말로 정보 전달을 하자니 번역기를 옮겨 적은 꼴이고 이미 매우 잘 작성된 AWS Blog 글도 있기에, 어떤 차별점을 주어야 할지 고민했습니다. 그래서 위와 같이 해당 논문을 이해하기 위해 필요한 배경지식들과 제 나름의 이해한 방식을 함께 싣었습니다.이 글을 통해 SRD에 호기심이 생기신다면, 꼭 한번 원문을 보면 스스로 이해하는 시간을 가져보시기 바랍니다.소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해 주세요! 😃",
        "url": "/SRD"
    }
    ,
    
    "migratecodebuild": {
        "title": "Automating git submodules with AWS Code Series (Build, Pipeline)",
            "author": "HeuristicWave",
            "category": "",
            "content": "AWS Code Series (Build, Pipeline)으로 git submodule 자동화Intro여러분이 보고 계신 이 블로그(GitHub Pages 활용)는 2개의 깃헙 레포지토리를 통해 배포되고 있습니다. 첫 번째 레포지토리는 원본 소스코드를 담고 있으며,블로그 글을 작성할 때마다 bundle exec jekyll serve라는 명령어로 localhost에서 퇴고를 진행합니다.해당 명령어는 static page를 생성할 때, url을 제 도메인이 아닌 localhost로 생성해 GitHub Pages에서는 사용할 수 없습니다.그러므로 저는 빌드 결과물이 떨어지는 output 파일을 .ignore로 처리합니다.static page를 제 도메인으로 생성하기 위해서는 bundle exec jekyll build라는 명령어로 빌드 해야 합니다.저는 이 절차를 CI 도구에게 위임했고, CI 도구는 markdown 형식으로 작성한 글들을 html 파일로 생성하여 2번째 레포지토리에 배포합니다.저는 이것을 자동화하기 위해 기존에는 Travis CI를 사용하고 있었습니다.현재 블로그로 CI/CD 파이프라인을 구축하고 약 2년간 88회의 Commit까지 잘 쓰고 있다가,어느새 다음과 같은 알람을 받아 보니 크레딧 소진으로 인하여 Travis CI를 AWS 솔루션으로 대체하기로 했습니다.  Builds have been temporarily disabled for private and public repositories due to a negative credit balance. Please go to the Plan page to replenish your credit balance.📜 Workflow과거 제가 AS-IS 상황에서 Travis CI로 다음과 같은 과정을 통해 블로그에 글을 배포했었습니다.  사용자가 원격 저장소(GitHub)에 git push 명령어로 새로운 코드를 반영  GitHub과 연결해둔 Travis CI가 .travis.yml 파일에 정의한 대로 command 수행          빌드 환경 구축      소스 코드 빌드      빌드 결과물을 배포용 레포지토리에 commit &amp; push      위 과정의 TO-BE로 Travis CI 역할을 CodeBuild와 CodePipeline으로 대체하고 .travis.yml 대신 buildspec.yaml 파일을 정의하겠습니다.💻 Hands-onTravis CI는 Source 연계(GitHub 연결)와 Build가 별도로 분리되어 있지 않습니다.그러나 AWS의 Code Series는 CodePipeline으로 Source와 Build를 연계하고, CodeBuild에서 Build를 정의해야 합니다.1️⃣ Build 정의해당 단계는 CodeBuild 생성 시, Buildspec 단계의 Insert build commands, editor로 구성할 수 있습니다.그러나 본 글에서는 직접 작성하여 Source 레포지토리 루트 위치에 buildspec.yaml 파일을 위치 시켜 진행하겠습니다.빌드 스펙은 공식 문서를 참고하여 필요한 내용들을 정의합니다.  ↪️ Git submodule 기능을 활용하기 위한 buildspec 예시  ruby 2.7, jekyll로 블로그를 git submodule로 운영하는 최소한의 설정입니다. 아래와 같은 commands를 기재한 이유는 troubleshooting 단계에서 설명합니다.  version: 0.2phases:  install:    runtime-versions:      ruby: 2.7    commands:      - echo Installing dependencies...      - gem install bundler      - bundle install --quiet  pre_build:    commands:      - export LC_ALL=\"en_US.utf8\"      - echo Git Setting...      - mkdir buildZone &amp;&amp; cd buildZone      - git init      - git remote add origin https://$GITHUB_TOKEN@github.com/heuristicwave/GitHubPageMaker.git      - git fetch      - git checkout -t origin/master      - git submodule init      - git submodule update --recursive  build:    commands:      - echo Building...      - bundle exec rake site:deploy  2️⃣ CodeBuild  Create build projects를 누르고 Project configuration에서 프로젝트 이름을 정의합니다. 이름 이외의 설정은 비워두었습니다.  Source에서 ‘Github’을 선택하면 OAuth로 연결 혹은 personal access token으로 연결 중 한 가지 방법을 선택합니다. 2가지 방법 모두 가능하므로 편리한 것을 선택합니다.  GitHub이 연동되면 연결하고자 하는 repository를 선택하고 나머지 옵션은 비워두었습니다.  ‘Git submodules’ 기능을 사용한다면, Additional configuration 토글을 눌러 submodules을 체크합니다.  본 글에서는 Source 이벤트로 CodePipeline을 사용하므로, Primary source webhook events는 넘어갑니다.      Environment에서는 다음과 같은 설정값을 주었습니다.          ❗️해당 단계에서 빌드하고 자 하는 런타임 환경을 꼭 공식 문서에서 확인 후, 선택하세요.❗빌드에 환경 변수를 명시했다면, Additional configuration 토글을 눌러 환경 변수를 추가해 주세요.이 단계는 생성 당시 추가하지 않더라도, 생성 이후 ‘Build details’ 탭에서 추가 혹은 변경이 가능합니다.        Buildspec과 Batch configuration은 비워두었습니다.  로깅과 산출물이 필요하면 Artifacts와 Logs를 사용하면 되지만, 저는 CodeBuild 내의 Build history만으로도 충분하기 때문에 사용하지 않았습니다.  마지막으로 Create build projects 버튼을 눌러 빌드 프로젝트를 생성합니다.3️⃣ CodePipeline  Create pipeline을 누르고 Step 1 단계에서 ‘이름’과 ‘Service role’을 지정합니다. 기본 값으로 설정하고 다음(Step 2) 페이지로 넘어갑니다.      Step 2 단계에서는 ‘Source provider’로 GitHub (Version 2)을 선택하고 ‘Connection’에서 깃헙과 연결해 줍니다. 이어서 상황에 맞게 ‘Repository name’과 ‘Branch name’을 선택하고 ‘Output artifact’로 default를 선택합니다.    default를 선택할 경우 고려 사항이 있지만, 자세한 내용은 아래 Troubleshooting - Issue 1단계에서 설명하겠습니다.    Step 3 Build 단계에서는 앞서 생성한 Codebuild를 지정하고 다른 값들을 기본값으로 설정하고 다음 단계로 넘어갑니다.  Step 4 Deploy 단계에서는 CodeDeploy와 같은 CD 도구 대신 Build의 command로 제어하므로 Skip deploy stage 버튼을 눌러 넘어갑니다.  Step 5 Deploy 단계에서는 검토를 마치고 Create pipeline 버튼을 누르면 바로 정의한 파이프라인이 실행됩니다.⛹🏾‍♂️ TroubleshootingIssue 1fatal: not a git repository (or any parent up to mount point /codebuild)  🖍️ CodePipeline의 Output artifact를 default 선택 시, 다음과 같은 안내 문구가 있습니다.Does not include git metadata about the repository. 즉, git metadata 정보가 없으므로 git과 관련된 명령어를 사용할 수 없습니다. 🖋️ metadata 정보만 없을 뿐, ls 명령어를 삽입해 파일 시스템을 확인하면 Source로 지정한 레포지토리의 구조가 담겨 있습니다.  ✏️ git init 명령어를 주입해 초기 세팅 명령어를 작성합니다.Issue 2error: The following untracked working tree files would be overwritten by checkout  🖍️ 위 에러로 구글링을 하면 git clean  -d  -f 명령어로 해결하라 하지만, 근본적인 해결 방법이 아닙니다.  상황에 따라 다르지만, 제 경우 루비의 라이브러리를 설치하는 Gemfile이 삭제되어 후속 빌드 단계에서 문제가 됩니다.  ✏️ 해당 문제의 근본적 원인은 Issue 1과 같이 git metadata 정보는 없지만, source repo의 파일이 담겨 발생하는 문제입니다. 이를 해결하기 위해 다음과 같이 mkdir buildZone &amp;&amp; cd buildZone 새 폴더를 만들어 해당 오류를 우회할 수 있습니다.Issue 3ArgumentError: invalid byte sequence in US-ASCII  🖍️ CodeBuild가 빌드 환경을 구성하는 데 사용하는 도커이미지는 기본 locale이 POSIX로 설정되어 있습니다.  ✏️ 공식 문서에서 가이드 하는 데로 pre_build에 export LC_ALL=\"en_US.utf8\" 환경 변수 주입으로 해결할 수 있습니다.Issue 4각종 인증 이슈, 예) fatal: could not read Password for ~~~  🖍️ 인증 정보가 잘 못 되었거나, 관련 값들을 주입하지 못했을 때 발생합니다.  ✏️ 토큰 값 인증 방법 : git {command} https://$GITHUB_TOKEN@github.com/#{username}/#{reponame}✏️ GitHub 비밀번호 설정 방법 : git config --global credential.helper cache 🖋️ 토큰 값과 같은 기밀성 정보는 parameter store, secrets-manager 등을 활용해 값을 보호하세요.OutroTravis CI의 경우 가이드 하는 대로 간편하게 설정이 가능했습니다. GitHub Actions의 경우 공식 문서에서 타 CI/CD 도구에서 마이그레이션 하는 법이 굉장히 잘 명세되어 비교적 사용이 쉽습니다.CodeBuild의 경우 일일이 다 확인하며 설정해 줘야 하는 점은 어려웠지만, 그만큼 커스텀 하여 사용할 수 있을 것 같습니다. 마지막으로 Travis CI에서 AWS Code Series로 마이그레이션 하기까지 33번의 실패가 있었던 화면을 공유하며 마치겠습니다. 🤪소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해주세요! 😃",
        "url": "/migrateCodebuild"
    }
    ,
    
    "eks-upgrade": {
        "title": "Amazon EKS Multi Cluster Upgrade with ExternalDNS",
            "author": "HeuristicWave",
            "category": "",
            "content": "ExternalDNS로 Amazon EKS 멀티 클러스터 업그레이드하기IntroAmazon EKS(이하 EKS)는 약 3 ~ 5 개월마다 새로운 버전이 출시합니다.운영 측면에서 새로운 버전 출시는 기존 EKS의 버전 업그레이드가 필요하다는 것을 의미합니다. EKS 업그레이드는 EKS 콘솔에서 지금 업데이트 버튼을 눌러 손쉽게 가능합니다.🏠 Single Cluster Upgrade위와 같은 EKS 클러스터 버전 업데이트를 싱글 클러스터 기반의 업그레이드라고 하며, 비교적 손쉽게 k8s 버전 업데이트가 가능합니다.업데이트가 손쉬운 반면 몇 가지 제약 사항도 존재합니다. 대표적으로 아래와 같이 원하는 버전으로 바로 업데이트되는 것이 아니라 순차적 단계를 거쳐야 합니다.🏘️‍ Multi Cluster Upgrade멀티 클러스터 업그레이드는 동일한 환경의 EKS를 멀티로 구성하다 보니,싱글 클러스터와는 달리 원하는 버전으로 바로 생성이 가능하고 만에 하나 롤백이 필요할 경우 기존 환경으로 돌아갈 수도 있습니다.멀티 클러스터 기반의 업그레이드 방법은 여러 가지 방법으로 진행할 수 있습니다. 그중에서도 이번 포스팅에서는 AWS Blog에 소개된 3가지 방법 중,비교적 가장 수월한 방법인 첫 번째 방법으로 한정해서 이야기해 보겠습니다.첫 번째로 소개된 Option 1의 방법은 2개의 동일한 환경에서 Amazon Route 53의 가중치 기능을 활용하여 업그레이드하는 방식입니다.방법은 간단합니다. 싱글 클러스터에서 별다른 작업을 해주지 않았다면, 아마 Route 53의 Routing policy를 Simple로 설정해 두었을 겁니다.멀티 클러스터에서는 Routing policy를 Weighted로 설정해 설정한 비율로 트래픽을 분배하는 원리입니다.위 캡처와 같이 레코드를 생성할 때, 아래 3가지 요소를 주목하여 가중치 정책을 생성합니다.  DNS의 캐시를 최소화하기 위해 TTL은 1m(60 seconds)를 권장  각 다른 환경에서 50:50 가중치를 주고 싶을 경우, 2개의 가중치 레코드에 Weighted 값 1을 부여  Record ID는 레코드의 주석과 같은 역할을 하지만 필수로 작성해야 함 (이어서 이 값의 중요성을~~ 😱)ExternalDNSKubernetes는 KubeDNS를 내부 DNS 서버로 활용합니다. Route 53과 같은 다른 DNS 공급자를 사용하기 위해서는 external-dns를 추가적으로 설치해 사용합니다.물론 external-dns를 사용하지 않고 외부 DNS의 영역과 k8s의 영역을 분리하여 사용할 수도 있지만, external-dns를 적용한다면 외부 DNS 공급자도 코드로 제어할 수 있습니다.Set upExternalDNS를 EKS에 설정하는 방법은 첨부 링크에 자세하게 설명되어 있지만, 놓치기 쉬운 2가지 부분을 언급하고 넘어가겠습니다.ExternalDNSexternal-dns를 최종 배포하기 전, Deployment의 아래 2가지 인자를 수정해야 합니다.args:    - --domain-filter=&lt;Your_R53_Domain_Name&gt;    - --txt-owner-id=&lt;Your_R53_HostedZone_Id&gt;  🐞 이번 포스팅을 준비하며 external-dns가 간헐적으로 동작하는 경우를 목격했습니다. 원인은 해당 인자를 오기재했기 때문인데, 원래대로라면 동작하지 않아야 하는데 버그인 것 같습니다.IngressExternalDNS를 제대로 설정했다면, 이어서 외부로 노출할 Ingress의 annotations를 수정합니다.external-dns: publicexternal-dns.alpha.kubernetes.io/hostname: myDomain.comexternal-dns.alpha.kubernetes.io/set-identifier: recordIDexternal-dns.alpha.kubernetes.io/aws-weight: '1'Simple 라우팅 정책에서는 set-identifier가 없어도 되지만, 이외 라우팅 정책에서는 필수적으로 들어가야 합니다. 직전 콘솔에서는 Record ID를 기재하지 않았을 경우 화면이 넘어가지 않지만, external-dns에서 해당 값이 빠지면 로그와 파드 상태 모두 특이점이 발견되지 않아 원인을 찾기 어려워집니다.Issueyaml 파일에 의도를 기재하여 배포했지만, 정작 Route 53에서 레코드가 업데이트되지 않는 경우구축 초기부터 가중치 정책을 적용한 멀티 클러스터를 생성한다면 해당 이슈를 만나지 않겠지만,싱글 클러스터로 external-dns를 운영하고 있는 환경에서 멀티 클러스터를 적용하면 해당 이슈를 만날 수 있습니다.  UPSERT is not possible, doing UPSERT will actually do a CREATE that will fail이를 해결하기 위해 노출하고자 하는 Ingress를 delete &amp; apply 한다면 우회가 가능하겠지만, 운영하고 있는 서비스라면 다운타임이 발생할 것입니다.지금으로서는 Route 53 콘솔 화면에서 수동으로 기존의 정책을 수정해 주고 후속으로 생성하는 클러스터는 코드로 제어하는 방법이 있습니다. 그러나 코드와 콘솔 2가지 채널에서 인프라를 다루는 방법은 바람직하지 않습니다.해당 이슈를 해결한 직후 external-dns의 logs│ time=\"2023-02-25T16:28:56Z\" level=info msg=\"Applying provider record filter for domains: [myDomain.com. .myDomain.com.]\"│ time=\"2023-02-25T16:28:57Z\" level=info msg=\"Desired change: CREATE ex.myDomain.com A [Id: /hostedzone/Z0HOSTEDZONEID]\"│ time=\"2023-02-25T16:28:57Z\" level=info msg=\"Desired change: CREATE ex.myDomain.com TXT [Id: /hostedzone/Z0HOSTEDZONEID]\"│ time=\"2023-02-25T16:28:57Z\" level=error msg=\"Failure in zone myDomain.com. [Id: /hostedzone/Z0HOSTEDZONEID]\"│ time=\"2023-02-25T16:28:57Z\" level=error msg=\"InvalidChangeBatch: [RRSet with DNS name ex.myDomain.com.,│ type TXT, SetIdentifier recordID cannot be created as a non-weighted set exists with the same name and type.]\\n\\tstatus code: 400, request│ time=\"2023-02-25T16:28:57Z\" level=error msg=\"failed to submit all changes for the following zones: [/hostedzone/Z0HOSTEDZONEID]\"-- After changing Simple Routing policy --│ time=\"2023-02-25T16:29:58Z\" level=info msg=\"2 record(s) in zone myDomain.com. [Id: /hostedzone/Z0HOSTEDZONEID] were successfully updated\"│ time=\"2023-02-25T16:30:58Z\" level=info msg=\"Applying provider record filter for domains: [myDomain.com. .myDomain.com.]\"│ time=\"2023-02-25T16:30:58Z\" level=info msg=\"All records are already up to date\"검증실제 1:1로 라우팅이 일어나고 있나 확인하고 싶다면, 웹 혹은 dig 명령어로 확인이 가능하지만 가장 정확한 방법은 Route 53 내 Test record를 사용하는 것입니다.Record 테스트를 위해 Record name을 기재하고 Get response 버튼을 누르면 Response returned by Route 53 화면에서 실시간으로 바뀌는 IP를 확인할 수 있습니다.멀티 클러스터 교체 작업❗️ 기존 클러스터를 A, 업그레이드를 진행할 클러스터를 B라 가정하겠습니다.B 클러스터가 문제없다 판단되면, 다시 한번 가중치를 조절해 A 클러스터를 대체합니다.  가중치 변화, 1:0 🔜 1:1 🔜 0:1B 클러스터에서 문제가 있다 판단되면 B의 가중치를 0으로 바꾸면 롤백의 효과를 볼 수 있습니다.Outroexternal-dns는 Route 53 리소스를 제어할 수 있어 편리하면서도 운영이 복잡합니다. 위에서 언급한 이슈 외에도 GitOps를 구축한 상태에서 멀티 클러스터를 운용하려면, 각 클러스터마다 다른 Repository가 필요합니다.terraform으로 external-dns를 대체할 수도 있지만, eksctl를 사용한다면 external-dns가 도움이 되니 사용 환경에 따라 적절한 도구를 사용해야 할 것 같습니다.모든 것을 코드로 관리하는 것은 쉽지 않네요. 🤣소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해 주세요! 😃",
        "url": "/EKS_Upgrade"
    }
    ,
    
    "openai-lambda": {
        "title": "Using OpenAI API with AWS Lambda",
            "author": "HeuristicWave",
            "category": "",
            "content": "AWS Lambda, OpenAI API를 활용한 개인 AI 봇 만들기Intro2월 2일 AWSKRUG의 Slack 채널에서 ChatGPT Slack App 테스트를 시작했다는 글을 보고,저도 메신저와 연동하여 ChatGPT를 사용해 보고 싶은 욕심이 생겼습니다. (아직, ChatGPT를 개인 봇에 적용하지는 않았습니다.)  🖍 해당 포스팅에서는 ChatGPT가 아닌 GPT-3 모델 중, text-davinci-003 모델을 사용했습니다.      배경 지식 : Difference between ChatGPT and the new davinci 3 model?  이미 인터넷에 OpenAI API를 Slack과 연동하여 사용하고 있는 사례들은 많아,제가 근무하는 회사에서 사용하고 있는 NHN의 협업 툴 Dooray와 연동하기로 했습니다.  💡 해당 포스팅에서는 Dooray와 AWS Lambda의 통합 방법을 다루지만,두레이 외의 다른 메신저 도구와도 연동하는 방법이 유사하므로 해당 방법을 응용하여 활용할 수 있습니다!🧭 Workflow위크플로우는 위와 같습니다. 사용자가 메신저에서 제공하는 / 커맨드로 질의를 하면,해당 요청이 AWS Lambda를 통해 OpenAI의 API를 활용해 질의에 대한 대답을 받아 메신저로 전달합니다.해당 기능을 구현하기 위해서는 OpenAI API keys와 AWS Lambda가 필요합니다.작업 순서  Lambda Layer 추가  Lambda Function 배포  Lambda Function URL 생성  Messenger 서비스와 Lambda 통합  Messenger 서비스의 POST에 맞춰 Lambda Function 수정🛠️ AWS Lambda로 OpenAI API 활용하기OpenAI의 API REFERENCE를 확인하면 Python, Node.js를 활용한 예시가 상세하게 나옵니다.예시에 나오는 대로 해당 코드를 Amazon EC2를 대여하여 상시 운영 서버에서 활용해도 되지만, 메신저에 연동하여 잠깐만 활용할 예정이므로 Serverless 컴퓨팅 서비스인 Lambda를 사용하겠습니다.AWS Lambda Layer 추가하기Python으로 OpenAI를 사용하기 위해서는 openai 파이썬 바인딩이 필요합니다.이를 위해 람다에서 여러 함수가 공유하는 코드 및 데이터를 중앙에서 관리하는 방식인 Lambda Layers 기능을 활용합니다.저는 OpenAI 패키지에 대한 종속성을 해결하기 위해서, OpenAI-AWS-Lambda-Layer를 사용했습니다.  ⚠️ 해당 레포의 README.md에 기재된 대로 진행하면 curl로 OpenAI를 사용할 수 있습니다.우선, AWS Lambda 콘솔의 Addtional resources의 Layers를 클릭하여 빌드 한 zip 파일을 업로드하고 호환성(python3.8, x86_64)을 체크해 준 다음 Layer를 생성합니다.AWS Lambda Functions 배포이어서 람다 콘솔 화면에서 Create function으로 함수를 생성하고 Add layer 버튼을 눌러,사전에 생성한 layer를 추가해 줍니다. 아래 사진과 같이 Layers 아이콘에 (1)이 추가되었습니다.⬆️ 앞서 언급한 오픈소스 파이썬 코드를 복사하고 27라인에 OpenAI로부터 발급받은 Key로 바꿔 적고 Deploy 버튼을 눌러 배포합니다.⬇️ 이어서 Configuration에서 Memory와 Timeout 값을 수정합니다.통상 129MB 정도의 메모리를 사용해 256MB와 OpenAPI로부터 응답이 늦어질 수 있으므로 1분이라는 넉넉한 시간을 주었습니다.마지막으로 Configuration 탭의 Function URL에서 URL을 생성합니다.이때, Auth type은 별도 인증 로직이 없는 NONE으로 설정해 주었습니다.🔄 Messenger(Dooray)와 Lambda 통합하기저는 통합할 메신저로 두레이를 사용했습니다. 두레이에서 /command 기능을 구현하는 방법은 다음 링크를 참고합니다.두레이 커맨드 추가하기 가이드Slack을 비롯한 대부분의 메신저가 외부 서버와 통합하기 위해 RequestUrl을 요구합니다.사전에 생성한 람다의 Function URL을 RequestUrl에 기재하면 통합이 완료됩니다.Messenger(Dooray) Request 형식 파악하기이전 단계에서 테스트 없이 코드를 배포했지만, 사실 람다를 코드를 개발하고 나면 Test event를 주입하여 작성한 람다가 의도한 대로 동작하는지 확인해 봐야 합니다.그러나 외부 서비스와 연동하여 어떠한 형식으로 Event(json)가 날라오는지 모르는 상황에서는 모니터링을 통해 파악해야 합니다.이벤트를 1회 발생시키고 CloudWatch Log groups에서 Event를 확인합니다.두레이 유저가 생성한 command는 text라는 필드에 담기고 이는 body로 감싸져 전달됩니다.Event가 어떤 형식으로 전달되는지 알게 되었으니, 오픈소스를 해당 형식에 맞게 수정합니다.body = json.loads(event['body'])prompt = body['text']기존 작성된 코드를 두레이 형식에 맞춰 text로 바꿔주니 아래와 같이 구현된 모습을 확인할 수 있었습니다.Outro이번 포스팅에서 OpenAI API를 사용함에 있어 두레이라는 메신저와 람다를 통합하는 부분을 다뤘습니다.하지만 Lambda와 외부 서비스가 어떻게 연동되는지 원리를 알면 어떤 서비스던지 연동이 가능합니다.이 밖에 현재 코드는 PoC 수준의 코드라 부족한 점이 많습니다.API Key를 그대로 기재하면 보안 이슈가 있으므로, AWS Secrets Manager를 활용한 별도의 환경 변수 처리가 필요합니다.또한 외부 서비스에서 Function URL을 호출할 때도, 인증 작업을 추가해 줘야 합니다.추후 ChatGPT 유료 버전이 나오면 결제하여 사용한 후기를 적기 약속하며 글을 마치겠습니다.소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해 주세요! 😃",
        "url": "/OpenAI_Lambda"
    }
    ,
    
    "eksctl-ppt": {
        "title": "DIY Amazon EKS with eksctl",
            "author": "HeuristicWave",
            "category": "",
            "content": "AWSKRUG 컨테이너 소모임🐳 - 1월 19일(목) 발표 자료Introeksctl을 현업에서 적용하기 위해 필수적으로 필요한 최소한의 요소들을 다룹니다.💡 SlideShare 링크를 통해 다운로드 받을 수 있습니다.OutroAWSKRUG에서는 첫 발표였는데, 부족한 발표임에도 19일 당일 소중한 시간을 내주셔서 감사합니다. 추후, 해당 장표들을 설명하는 블로그 포스팅을 함께 개제할 예정입니다.EKSCTL Tips    DIY Amazon EKS with eksctl (Deck)",
        "url": "/eksctl_ppt"
    }
    ,
    
    "goroutines": {
        "title": "Goroutines",
            "author": "HeuristicWave",
            "category": "",
            "content": "Goroutines, Concurrent Programming in GoIntro해당 포스팅은 Tucker의 Go 언어 프로그래밍 24장 고루틴과 동시성 프로그래밍 읽고 정리한 내용임을 알립니다.미루고 미루던 Go 언어 학습을, 글또 덕분에 올해 Go 언어 학습을 끝마칠 수 있을 것 같습니다. 😵‍💫Goroutines고루틴은 Go 언어에서 관리하는 경량 스레드입니다. 함수나 명령을 동시에 수행할 때 사용하며, 여러 고루틴을 갖는 프로그램 코딩을 동시성 프로그래밍이라고 합니다.고루틴을 이해하기 위해, 선수 지식들을 알아보겠습니다.Thread메모리 공간에 로딩되어 동작하는 프로그램을 프로세스라고 합니다. 프로세스는 1개 이상의 작업 단위를 가지고 있으며, 이 작업 단위를 스레드라고 합니다.스레드가 하나면 싱글 스레드 프로세스, 여럿이면 멀티 스레드 프로세스라 합니다.원래 CPU 코어는 한 번에 한 명령밖에 수행할 수 없습니다. 그러나 스레드가 CPU 코어를 빠르게 교대로 점유하면 동시에 모든 스레드가 실행되는 것처럼 보입니다.Context switchingCPU 코어가 여러 스레드를 전환하는 것을 컨텍스트 스위칭이라고 합니다. 스레드를 전환하려면 현재 상태를 보관해야 다시 스레드가 전환되어 돌아올 때 마지막 실행 상태부터 이어서 실행이 가능합니다.이를 위해 스레드의 명령 포인터(instruction pointer), 스택 메모리 등의 정보를 저장하는 데 이것을 스레드 컨텍스트라고 합니다.스레드가 전환될 때마다 스레드 컨텍스트를 저장하고 복원하기 때문에 전환 비용이 발생하고 적정 개수를 넘어 너무 많은 스레드를 수행하면 성능이 저하됩니다.하지만 Go 언어에서는 CPU 코어마다 OS 스레드를 하나만 할당해 사용하므로 컨텍스트 스위칭 비용이 발생하지 않습니다.Goroutines Example모든 프로그램은 최소 하나의 고루틴을 가지고 있습니다. 이는 메인 루틴으로 main() 함수와 함께 고루틴이 시작되고 종료됩니다.이미 하나의 고루틴이 있으며, 추가로 고루틴을 생성하는 방법은 다음과 같이, go functionName() go 키워드와 함께 함수를 호출하는 것입니다.아래 코드는 2개의 서브 고루틴을 사용한 예시입니다. 어떤 결과가 나올지 예상해 보고, 하단의 결과를 열어 확인해 보세요 😎package mainimport (\t\"fmt\"\t\"time\")func PrintHangul() {\thanguls := []rune{'가', '나', '다', '라', '마', '바', '사'}\tfor _, v := range hanguls {\t\ttime.Sleep(300 * time.Millisecond)\t\tfmt.Printf(\"%c\", v)\t}}func PrintNumbers() {\tfor i := 1; i &lt;= 5; i++ {\t\ttime.Sleep(400 * time.Millisecond)\t\tfmt.Printf(\"%d \", i)\t}}func main() {\tgo PrintNumbers()\tgo PrintHangul()}  👀 실행 결과 보기  해당 코드는 고루틴이 생성되어 있지만, 메인 함수가 먼저 종료되어 아무런 결과도 출력되지 않습니다.결과를 출력하기 위해서는 서브 고루틴이 모두 실행되고 완료되는 2000ms 보다 많은 시간을 main() 함수에 넣으면 됩니다.  이렇게 time.Sleep(3 * time.Second) 3000ms를 보장하는 코드를 삽입하면 모든 실행을 보장합니다.실행 시간 보장하기생성한 서브 고루틴들의 실행을 보장하기 위해서는 WaitGroup 객체를 사용하면 됩니다.var wg sync.WaitGroupwg.Add(3)   // 작업 개수 설정wg.Done()   // 작업이 완료될 때마다 호출wg.Wait()   // 모든 작업이 완료될 때까지 대기해당 방법을 통해 위에 예시로 소개한 고루틴을 다음과 같이 수정하면 모든 실행을 보장할 수 있습니다.  👀 서브 고루틴 기다리기  package mainimport (\t\"fmt\"\t\"sync\"\t\"time\")var wg sync.WaitGroupfunc PrintHangul() {\thanguls := []rune{'가', '나', '다', '라', '마', '바', '사'}\tfor _, v := range hanguls {\t\ttime.Sleep(300 * time.Millisecond)\t\tfmt.Printf(\"%c\", v)\t}\twg.Done()}func PrintNumbers() {\tfor i := 1; i &lt;= 5; i++ {\t\ttime.Sleep(400 * time.Millisecond)\t\tfmt.Printf(\"%d \", i)\t}\twg.Done()}func main() {\twg.Add(2)\tgo PrintNumbers()\tgo PrintHangul()\twg.Wait()\t// time.Sleep(3 * time.Second)}  Mechanism고루틴은 명령을 수행하는 단일 흐름으로 OS 스레드를 이용하는 경량 스레드입니다. 해당 정의를 이해하기 위해 OS 스레드와 고루틴이 어떻게 다른지 알아보겠습니다.2개의 코어에서 2개의 고루틴이 존재한다 가정하면, 아래 그림과 같이 각 코어 별, OS 스레드에 하나의 고루틴이 실행됩니다. ________           ______________         .''''''''.|        |         /             /        /    Go    \\| CORE 1 |--------/ OS Thread 1 /---------\\ routine1 /|________|       /_____________/           '........' ________           ______________         .''''''''.|        |         /             /        /    Go    \\| CORE 2 |--------/ OS Thread 2 /---------\\ routine2 /|________|       /_____________/           '........'위 상황에서 고루틴을 하나 더 생성하면, 남는 코어가 없으므로 3번째 고루틴은 다른 고루틴이 실행 완료될 때까지 대기 상태로 멈춰 있습니다.만약 고루틴 2가 실행 완료되면, 그제야 대기하던 고루틴 3이 실행됩니다. ________           ______________         .''''''''.|        |         /             /        /    Go    \\| CORE 1 |--------/ OS Thread 1 /---------\\ routine1 /|________|       /_____________/           '........' ________           ______________         .''''''''.|        |         /             /        /    Go    \\| CORE 2 |--------/ OS Thread 2 /---------\\ routine2 /|________|       /_____________/           '........'                                                ^ .'!Wait!'.                                     |/    Go    \\______After Goroutin 2 is removed___|\\ routine3 / '.!Wait!.'System Call커널 서비스를 사용하기 위해 시스템 콜을 호출하면, 해당 서비스가 완료될 때까지 대기 상태가 됩니다.앞선 예시에서는 실행 중인 고루틴이 완료되기까지 대기 상태를 유지했다면, 시스템 콜이 발생한 상황(고루틴 3)에서는 해당 고루틴을 대기열로 보내고대기하던 다른 고루틴(고루틴 4)을 실행하며 코어와 스레드 변경 없이 고루틴만을 이동시킵니다. ________           ______________         .''''''''.|        |         /             /        /    Go    \\| CORE 1 |--------/ OS Thread 1 /---------\\ routine1 /|________|       /_____________/           '........' ________           ______________         .''''''''.|        |         /             /        /    Go    \\| CORE 2 |--------/ OS Thread 2 /---------\\ routine3 /|________|       /_____________/           '........'                                                ^ .'!Wait!'.                                     |/    Go    \\&lt;------ Switch only Goroutin -------|\\ routine4 /  without changing cores and threads '.!Wait!.'이와 같이 고루틴을 이용하면 컨텍스트 스위칭과 없이 오직 고루틴만 옮겨 다니므로, 컨텍스트 스위칭 비용이 증가하면서 발생하는 프로그램 성능 저하로부터 자유로워지게 됩니다.동시성 프로그래밍 주의점여러 고루틴이 동일한 메모리 자원에 접근하면 값을 변경시키면 동시성 문제를 일으킵니다. 이런 문제를 해결하기 위해 한 고루틴이 접근할 때,뮤텍스(mutex, 상호 배제)를 이용하면 다른 고루틴이 자원에 접근하지 못하게 권한을 통제할 수 있습니다.Mutex뮤텍스는 Lock() 메서드를 호출해 뮤텍스를 회득하면, 이후에 Lock() 메서드를 호출한 고루틴은 앞서 획득한 뮤텍스가 반납될 때까지 대기하게 됩니다.var mutex sync.Mutex        // 패키지 전역 변수 뮤텍스func mutexExample() {    mutex.Lock()            // 뮤텍스를 확보할 때까지 대기    defer mutex.Unlock()    // 이하 로직은 뮤텍스를 확보한 단 하나의 고루틴만 실행    ...}위 예시의 3줄만 작성한다면 프로그램에 뮤텍스를 이용해 동시성 문제를 해결할 수 있습니다. 그러나 또 다른 문제가 발생할 수 있습니다.  오직 하나의 고루틴만 공유 자원에 접근하므로, 동시성 프로그래밍으로 얻는 성능 향상을 얻을 수 없음  뮤텍스를 잘못 사용하면, 데드락(Deadlock, 교착 상태)에 빠져 무한정 대기하게 됨Deadlock하나의 프로세스가 2개 이상의 자원을 얻어야 하는 상황에서, 서로 원하는 자원이 상대방에 할당되어 무한히 다음 자원을 기다리는 데드락을 예시를 통해 발생시켜 보겠습니다.package mainimport (\t\"fmt\"\t\"math/rand\"\t\"sync\"\t\"time\")var wg sync.WaitGroupfunc diningProblem(name string, first, second *sync.Mutex, firstName, secondName string) {\tfor i := 0; i &lt; 100; i++ {\t\tfmt.Printf(\"%s 밥을 먹으려 합니다.\\n\", name)\t\tfirst.Lock()\t\tfmt.Printf(\"%s %s 획득\\n\", name, firstName)\t\tsecond.Lock()\t\tfmt.Printf(\"%s %s 획득\\n\", name, secondName)\t\tfmt.Printf(\"%s 밥을 먹습니다.\\n\", name)\t\ttime.Sleep(time.Duration(rand.Intn(1000)) * time.Millisecond)\t\tsecond.Unlock()\t\tfirst.Unlock()\t}\twg.Done()}func main() {\trand.Seed(time.Now().UnixNano())\twg.Add(2)\tfork := &amp;sync.Mutex{}\tspoon := &amp;sync.Mutex{}\tgo diningProblem(\"A\", fork, spoon, \"포크\", \"수저\")\tgo diningProblem(\"B\", spoon, fork, \"수저\", \"포크\")\twg.Wait()}위 예제는 실행시키면 아래와 같이 어떤 고루틴도 원하는 만큼의 뮤텍스를 확보하지 못해 무한히 대기하게 됩니다.B 수저 획득A 포크 획득fatal error: all goroutines are asleep - deadlock!서로 다른 자원에 접근하기애초에 같은 자원을 여러 고루틴이 접근하지 않는다면, 멀티코어의 이점을 얻으면서 뮤텍스로 인해 발생하는 문제도 피할 수 있습니다.각 고루틴에게 서로 다른 자원에 접근하도록 만들기 위해 아래 2가지 방법이 있습니다.  영역 나누기 : 고루틴 간 간섭이 발생하지 않게 각각의 고루틴으로 할당된 작업만 실행  역할 나누기 : 채널을 활용해 고루틴 간의 간섭을 없애기Go 언어에서 동시성 프로그래밍을 도와주는 채널과 컨텍스트에 대해서는 다음 포스팅에서 다루도록 하겠습니다.Outro요약을 덧붙이며 이번 포스팅을 마무리 짓도록 하겠습니다.  고루틴은 경량 스레드로 컨텍스트 스위칭 비용이 발생하지 않습니다.  멀티 코어 머신에서 여러 고루틴을 사용해 성능을 증가시킬 수 있으나, 같은 메모리 영역을 조정하면 문제가 발생합니다.  뮤텍스는 동시에 고루틴 하나만 자원에 접근하도록 조정합니다.  뮤텍스를 잘못 사용하면 데드락 문제가 발생합니다.  작업 분할 방식과 역할 분할 방식으로 뮤텍스 없이 동시 프로그래밍을 가능하게 할 수 있습니다.소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해 주세요! 😃",
        "url": "/Goroutines"
    }
    ,
    
    "geultto2": {
        "title": "글또 7기 회고",
            "author": "HeuristicWave",
            "category": "",
            "content": "회고 글을 쓰며 드는 생각글또 7기를 마무리해가며… 🏃🏻이번 글을 포함하여, 총 2회 제출만이 남았습니다. 본래 회고 글은 맨 마지막 제출에 작성하려 했으나, 마감 8시간을 남기고 이미 패스권은 다 소진해 계획을 수정했습니다.해당 포스팅은 22년 5월 5일에 작성한 글또 7기 다짐글 템플릿을 수정해 작성한 글로,해당 글을 같이 띄워두고 보면 조금 더 재미?있지 않을까 싶습니다.🧩 계획 점검지금까지 2번의 패스권 사용과 8번의 제출이 있었습니다. 다짐 글에서 아래와 같이 콘텐츠를 계획했는데, 얼마나 달성했는지 점검해 보겠습니다.      기술 도서 리뷰 밀린 기술 서적 부채(?)를 청산한다 했는데, 여전히 쌓여 있습니다. 한 권도 제대로 읽은 책이 없지만,“이펙티브 엔지니어“와 “Tucker의 Go 언어 프로그래밍“거을 거의 다? 읽어가니, 자체 평가로 B를 부여하겠습니다.         기존 시리즈물 마감 2편에 머물러 있던, ‘테라폼 더 익숙하게’ 라는 시리즈물에 추가 3편을 연재하며, 총 5편의 시리즈물을 만들었습니다.추가로 앞서 작성했던 기존 2편도 수정을 통해 완성도를 조금 더 높였습니다. 연재가 끝난 건 아니지만, 해당 5편을 통해 Terraform Associate 자격증을취득하는 데 도움이 되었으므로 S를 부여하겠습니다.        관심 기술 스터디 “Kubernetes, Terraform, Istio, AWS 서비스들에 대한 글을 작성하며, 성장의 기록들을 남기겠습니다.”라고 다짐했는데,Kubernetes 1편, Terraform 3편, AWS 서비스 1편을 작성했습니다. Istio에 대해서 작성하지 못했지만, 그래도 비교적 다짐을 지킨 것 같아 A를 제 스스로에게 주겠습니다.        주제를 추천받아 작성 최근 OIDC에 대한 글이 4번 다짐에 부합하는 것 같습니다. OIDC에 대한 글을 작성하는데, 가장 많은 시간을 쏟았으므로 A 등급으로 평가를 마무리하겠습니다.  종합 평가4가지 항목에 대하여, 각각 B, S, A, A 등급으로 평균 A 판정을 받았습니다. 저는 저 스스로에게 굉장히 관대한 사람인 것 같습니다.✋ 잠깐!정말 제 스스로에 대한 평가가 맞는지, 다짐 글에 다짐을 다시 보며 검토를 해보겠습니다.자발적 번아웃 🔥자발적 번아웃이 올 정도로 열심히 글을 작성한다 했는데, 결국 오지 않은 것 같습니다.  글또에는 여러 기수를 걸쳐 지속적으로 활동하고 계신 분들이 꽤나 많습니다.저도 이번 7기가 끝이 아니라 지속적인 활동으로 글또를 이어가고 싶습니다.그렇지만, 7기 활동이 끝날 무렵 회고를 하는 시점에서 7기 활동 간 생산된 12편의 글로 인하여 후회 없는 활동을 하고 싶습니다.다시 바꾸어 말하면, “너무 힘들어서 8기는 쉬어야겠다.” 싶을 정도의 감정을 느끼도록 열심히 활동하고 싶습니다.2주마다 겪은 창작의 고통만 본다면, 후회 없는 활동에 가까운 노력을 한 것 같습니다. 그러나, 다짐 글 당시에 작성한 “너무 힘들어서 8기는 쉬어야겠다.” 싶을 정도의 감정은아직 느끼지 못했습니다. 패스권을 2회나 사용했기 때문에, 제 스스로 열심히 했다고 말하기가 부끄럽습니다.부끄러움을 늦추는 글의 유효기간 🙈과거 저는 아래 그래프와 함께 글의 완성도와 부끄러움의 발현 시기는 양의 상관관계를 갖고 있다고 말했습니다.더불어 이런 목표를 가지고 이번 7기에 임했습니다.  작년에 제가 작성한 글 들의 경우, 아무리 길어도 대략 한 계절정도 지나면 부끄러움이 스멀스멀 올라오는것 같더군요.그래서, 이번 활동 기간 동안에는 과거 제가 썻던 글보다 더 부끄러움이 오는 시기가 늦는 글을 작성해 보려합니다. 활동기간이 약 6개월 정도되니 아마 5월 말에 쓰는 글에 대한 부끄러움의 정도를 7기 활동이 끝날 무렵인 회고 때 다뤄보면 좋을 것 같습니다.5월부터 글을 작성하다 보니, 글또 초창기에 작성한 글들은 벌써 한 계절을 넘었습니다. 해당 글들을 지금 다시 보니 앞서 언급한 목표는 달성한 것 같습니다.앞선 종합 평가에서 스스로에게 A를 부여했으나, 검토를 진행하다 보니 부끄러움이 몰려와 B 정도로 수정해야겠습니다.P.S.사실 이번 포스팅에는 “이펙티브 엔지니어” 독후감을 계획하고 있었지만, 썩 초안이 마음에 들지 않아 없던 일로 돌아갔습니다.독후감 계획은 무산되었지만, 책 본문에 나온 내용을 소개하며 마치겠습니다.“우리가 집중하기 어려운 것은 앞서 말한 것처럼 연속 시간이 부족하거나 맥락이 너무 자주 전환되어서도 있지만, 가끔은 어려운 일을 시작하는 데 필요한 활성화 에너지를 모을 동기가 부족해서일 수도 있다.”심리학 교수 피터 골비처(Peter Gollwitzer)는 연구에 참여하는 학생들에게 크리스마스이가 지나고 이틀 내에 휴가를 어떻게 보냈는지 에세이를 우편으로 보내라고 했다.학생 중 절반에게는 에세이를 언제 어디서 어떻게 구체적으로 명시하게 하니, ‘실행 의사’를 구체적으로 표현한 학생 중 71%는 에세이를 우편으로 보냈다.표현하지 않은 학생 중 에세이를 보낸 학생은 32% 뿐이었다. 행동에 아주 작은 변화를 주었을 뿐인데 완료율이 2배 이상 증가한 것이다.결과론이기는 하지만, 이번 글또 활동도 나름의 계획을 ‘다짐 글’이라는 ‘실행 의사’로 표현하니 작년보다 더 많은 글을 작성하게 된 것 같습니다. 🥲",
        "url": "/geultto2"
    }
    ,
    
    "oidc": {
        "title": "The Journey to Know OIDC",
            "author": "HeuristicWave",
            "category": "",
            "content": "Journey to OIDC through authentication, authorization, SAML, and OAuth.Intro요즘 제 일상에서, OIDC(OpenID Connect)에 대해 많은 이야기가 오고 갔습니다. 그동안 동작 원리도 제대로 알지 못한 채 사용하고 있던 제 모습을 반성하며,OIDC를 알아가기 위해 조사한 내용들을 다뤄보겠습니다.🥾 Authentication &amp; AuthorizationOIDC를 향한 여정의 첫걸음은 인증과 인가입니다. 국어로도 비슷한 두 단어는 영어로도 비슷하며 다음과 같이 줄여서 표현하기도 합니다.AuthN(인증) &amp; AuthZ(인가, 권한 부여)인증과 인가에 관한 설명은 온라인에 굉장히 많지만, 저는 그중에서도 Auth0(오스제로) 사의 문서의 비교표를 인용하였습니다.            Authentication      Authorization                  사용자가 자신이 주장하는 사람이 맞는지 결정합니다.      사용자가 접근할 수 있는지 없는지 결정합니다.              사용자의 자격 증명이 유효한지 확인합니다.      정책과 룰을 통해 접근 여부를 확인합니다.              통상 인가 전 단계에 진행합니다.      통상 인증 이후 단계에 진행합니다.              일반적으로 ID Token을 통해 정보를 전송합니다.      일반적으로 Access Token을 통해 정보를 전달합니다.              일반적으로 OIDC 프로토콜에 의해 관리됩니다.      일반적으로 OAuth 2.0 프레임워크에 의해 관리됩니다.        💡 위 비교표에 Bold처리된 키워드가 오늘 포스팅을 이해하기 위한 핵심 용어이므로 주목해 주세요!⛰ OAuth 2.0OAuth 2.0 Authorization Framework의 RFC 문서의 Abstract 부분을 확인하면 다음과 같이 정의합니다.“OAuth 2.0 인가 프레임워크는 서드파티 앱들이 제한된 권한을 얻는 것을 가능하게 해줍니다.”여러분들은 특정 서비스 회사의 애플리케이션을 이용할 때, 구글이나 페이스북으로부터 특정 권한을 받아 서비스를 이용한 경험이 있을 것입니다.구글 계정 정보의 내 계정에 액세스할 수 있는 앱에 들어가 확인해 보시면,내 계정이 어떤 앱들이 액세스할 수 있게 권한을 주었는지 확인 가능합니다.위 사진을 통해, 구글이 OAuth 프레임워크를 사용해 Meetup, Notion, Slack과 같은 서드파티 앱들에게계정의 이메일 주소 확인, 개인정보 보기 등과 같은 제한된 권한에 대하여 인가를 해줬다는 사실을 알 수 있습니다.Slack으로 알아보는 OAuth 2.0 FlowOAuth에 워크플로를 알아보기 전에, 자주 언급되는 핵심 용어들을 언급하고 넘어가겠습니다.  Client : 인가를 받는 애플리케이션 (배달의 민족, 야놀자, 오늘의 집 등)  Resource Server : 제어하고자 하는 자원(Protected Resource)을 가진 서버 (구글, 페이스북, 깃헙)  Resource Owner : 자원의 소유자 (end-user)  Authorization Server : AuthZ 처리를 하는 서버다시 RFC 문서로 돌아와서 해당 프로토콜의 Flow를 앞서 보여드린 예시처럼 우리의 경험에 빗대어 설명해 보겠습니다.# Abstract Protocol Flow+--------+                               +---------------+|        |--(A)- Authorization Request -&gt;|   Resource    ||        |                               |     Owner     ||        |&lt;-(B)-- Authorization Grant ---|               ||        |                               +---------------+|        ||        |                               +---------------+|        |--(C)-- Authorization Grant --&gt;| Authorization || Client |                               |     Server    ||        |&lt;-(D)----- Access Token -------|               ||        |                               +---------------+|        ||        |                               +---------------+|        |--(E)----- Access Token ------&gt;|    Resource   ||        |                               |     Server    ||        |&lt;-(F)--- Protected Resource ---|               |+--------+                               +---------------+우선 A &amp; B에 해당하는 Client가 Resource Owner에게 AuthZ를 요청하고 승인 절차를 Slack 로그인을 통해서 알아보겠습니다.위 이미지에서 Client인 slack은 Google과 Apple 2가지 방식의 OAuth를 제공합니다.‘Google로 계속’이라는 버튼(A 단계, AuthZ 요청)을 누르면, slack이 계정의 이름, 이메일, 프로필 사진 등의 정보 공유를 요청했다는 것을 구글의 Authorization Server로부터 받습니다.해당 시점의 웹브라우저 URI를 확인하면 다음과 같습니다. (편의상 URI을 쿼리 스트링 단위로 나누고, Decode 하여 표기했습니다.)https://accounts.google.com/o/oauth2/v2/auth/oauthchooseaccount? \\   client_id=6XXXXXXXX.apps.googleusercontent.com&amp; \\  redirect_uri=https://oauth2.slack.com/get-started/oauth/google/end&amp; \\  scope=openid email profile&amp; \\  response_type=code&amp; \\  access_type=offline&amp; \\  state=&amp; \\  prompt=consent&amp; \\  flowName=GeneralOAuthFlow슬랙의 권한 부여의 경우, https://accounts.google.com/{이하 생략}이라는 Authorization Server의 주소와 client_id를 포함한 8개의 querystring으로 구성되어 있습니다.여기서 상위 4개의 querystring은 인가를 위한 필수 파라미터입니다.  client_id : client를 식별하기 위해 Register 단계에서 생성된 애플리케이션의 식별자  redirect_uri : authorization code를 전달받을 client 서버의 URI  scope : 인가할 권한을 명시  response_type : code, token 등 OAuth에 적용할 방식의 타입Authorization Code Grant캡처화면에서는 우리의 웹 브라우저를 통해 A와 B의 단계가 클릭 2번으로 끝나지만, 사실 A와 B 사이에는 더 복잡한 단계들이 숨겨져 있습니다.아래 RFC 문서에 기재된 Authorization Code Grant를 살펴보며 A 이후의 단계들을 살짝 보고 넘어가겠습니다. 🫣+----------+| Resource ||   Owner  ||          |+----------+     ^     |    (B)+----|-----+          Client Identifier      +---------------+|         -+----(A)-- &amp; Redirection URI ----&gt;|               ||  User-   |                                 | Authorization ||  Agent  -+----(B)-- User authenticates ---&gt;|     Server    ||          |                                 |               ||         -+----(C)-- Authorization Code ---&lt;|               |+-|----|---+                                 +---------------+  |    |                                         ^      v (A)  (C)                                        |      |  |    |                                         |      |  ^    v                                         |      |+---------+                                      |      ||         |&gt;---(D)-- Authorization Code ---------'      ||  Client |          &amp; Redirection URI                  ||         |                                             ||         |&lt;---(E)----- Access Token -------------------'+---------+       (w/ Optional Refresh Token)위 승인 Flow의 User-Agent는 우리의 웹브라우저입니다. 브라우저는 인가 서버의 URI에 querystring으로 client_id와 redirect_uri를 붙여 인가 서버에 전달하면(A),인가 서버는 검증 작업을 거쳐 Authorization Code을 발급(C) 합니다.Authorization Code를 받은 브라우저는 Client에게 전달(C) 하고, Client는 다시 인가 서버에게 Code를 보내고(D)Access Token을 돌려받습니다(E).이후 Client는 생성된 Access Token을 활용해 scope에 명시된 기능을 Resource Server로부터 제공받습니다.해당 절차가 Abstract Protocol Flow 그림의 B ~ F에 해당합니다.지금까지 생소한 정보들을 쏟아내다 보니, 독자 여려분들이 굉장히 혼란스러울 것 같습니다. (처음 OAuth를 공부했던 제가 그랬습니다 😵‍💫)처음부터 모든 과정을 이해하려 하면 어려우니, 대략적인 흐름만 파악하고 넘어가셔도 좋습니다. 이후 OIDC에서 예시와 함께 또 다룰 예정이거든요 🙃  🛎 위 2가지 Flow가 OAuth 2.0의 전체 Flow는 아닙니다. Client Register, Refresh Token 등 중요 개념들이 상당히 생략 되어 있습니다.OpenID Connect(OIDC)얼렁뚱땅 OAuth를 넘기고 드디어 OpenID Connect 순서에 다 다르었습니다. OIDC를 다루기 전, OpenID가 무엇인지 OpenID 재단의 설명에서 몇 가지 살펴보겠습니다.OpenID  OpenID는 새 비밀번호를 만들 필요 없이, 기존 계정을 사용해 여러 웹사이트에 로그인할 수 있습니다.  OpenID를 사용하면 웹사이트와 공유되는 정보의 양을 제어할 수 있습니다.  당신의 비밀번호은 Identity provider(Idp)에게만 주어지며, Idp가 신원을 확인하므로 안전하지 않은 웹사이트가 당신의 신원을 훼손하는 것에 대해 걱정할 필요가 없습니다.  ⚡️ OpenID 2.0은 OpenID Connect로 대체되었습니다. Ref. Libraries for Obsolete SpecificationsUsing OAuth 2.0 to Access APIs이어서, OpenID 재단의 설명의 OIDC는 다음과 같이 정의되어 있습니다.“OpenID Connect 1.0 is a simple identity layer on top of the OAuth 2.0 protocol.” OIDC가 OAuth 프로토콜의 상위 간단한 신원 계층이라는 말이 무엇을 의미할까요?여기서 앞서 slack 예시에서 만난 OAuth의 scope 부분으로 거슬러 올라가 보겠습니다. (scope=openid email profile) 해당 파라미터에는 인가할 리소스가 openid로 기재되어 있습니다.즉, slack은 제 프로필 정보를 얻기 위해 OAuth 2.0을 사용해 Google OAuth2 API v2를 호출했고, 액세스하려는 API의 scope을 openid로 지정했습니다.이해를 돕기 위해, Google Developers의 OAuth 2.0 Playground(Client를 구축할 필요 없이, 데모 환경을 제공)를 사용해 보겠습니다.Step1에서 Google OAuth2 API v2의 scope으로 email, profile, openid를 지정하고 Authorize APIs 버튼을 누르니,Client만 Google OAuth 2.0 Playground로 바뀌고 slack 예시와 동일한 결과를 받는 것을 확인할 수 있습니다.  🙈 해당 URI도 주의 깊게 살펴보면 paramter의 순서만 다를 뿐, 필수 파라미터를 포함하여 slack 예시와 비슷한 URI를 볼 수 있습니다.Step2에서는 Idp(여기서는 구글의 인증 서버)로부터 발급된 Authorization code를 토큰으로 바꾸는 Exchange authorization code for tokens 버튼을 누르면,Refresh token과 Access token Idp로부터 받아 Playground 웹 페이지에 나타나게 됩니다.이어서 전달된 Response 값을 확인해 보면, 타 OAuth 2.0 API를 scope으로 선택했을 때와는 다르게 id_token 값으로 JWT 형태의 값을 받습니다.id_tokenOIDC는 OAuth 2.0의 상위 계층이므로 동작 원리가 OAuth 2.0과 동일합니다. 한 가지 다른 점은, id_token 정보가 포함되어 있다는 것입니다.Step2에서 받은 id_token 값을 jwt.io에 복사하여 Decode 한 정보를 살펴보겠습니다.jwt는 .으로 구분되어 decode 하면 아래 3가지 필드로 나뉩니다.  header : id 토큰의 형식, 해시 알고리즘의 종류, 암호화에 사용된 공개키 ID  payload : 사용자 인증 정보 - (발급 기관, 토큰이 발급된 앱, id 토큰 유저의 번호, 발급 시간, 만료 시간, 이메일, 이름, 프로필 사진 등)  signature : 인증 서버에 header의 공개키로 서명한 값client는 payload에 담긴 사용자 정보를 활용해 유저의 인증을 할 수 있습니다.Recap여기까지 포스팅을 읽으셨다면, 앞부분에 언급했던 Auth0(오스제로)의 인증, 인가 비교표를 떠올려주세요.OAuth는 AuthZ(인가)에 초점이 맞춰 저 Access Token으로 특정 권한을 허가해 줄 뿐 사용자의 정보는 담고 있지 않습니다.반면, OIDC는 ID Token을 통해 정보를 전송하며 AuthN(인증)을 가능하게 하였습니다.Outro이번 포스팅은 기존에 올린 타 글들보다 많은 레퍼런스 문서들을 읽었습니다. 이미 온라인에 굉장히 잘 정리된 콘텐츠들이 많지만,제 나름대로 재가공 하다 보니 부족한 부분들이 많습니다. 제게 많은 도움이 되었던 레퍼런스들을 아래 첨부하니, 함께 보시면 도움이 많이 될 것 같습니다.마지막으로, Samsung SDS의 인사이트 리포트에실린 요약이 오늘의 정보들을 깔끔하게 정리해 주는 것 같아 함께 첨부하며 마치겠습니다. (요약된 문장의 단어 하나하나가 해당 기술의 특징을 잘 나태낸다는 생각이 드네요 😊)  SAML 2.0 : 2001년 OASIS에서 정의한 개방형 Authentication(인증) 및 Authorization(인가) 표준이며, 엔터프라이즈 애플리케이션의 SSO(Single Sign On)를 목적으로 XML(Extensible Markup Language) 형식으로 개발  OAuth 2.0 : 2006년 Twitter와 Google이 정의한 개방형 Authorization 표준이며, API 허가를 목적으로 JSON(Javascript Object Notation) 형식으로 개발  OIDC 2.0 : 2014년 OpenID Foundation에서 정의한 개방형 Authentication 표준이며, 컨슈머 어플리케이션의 SSO를 목적으로 JSON 형식으로 개발소중한 시간을 내어 읽어주셔서 감사합니다! 특히나 이번 포스팅은 잘못된 내용 혹은 부적절한 설명이 있을 수도 있으니 자유롭게 지적해 주세요!벌써 포스팅 이후 3번이나 다시 배포하게 되었네요. 😃📚 ReferencesSSO(SAML &amp; OAuth 2.0)  SAML vs. OAuth: Comparison and Differences : SAML, OAuth 2.0 동작 원리 설명  NAVER WORKS Developers, SSO 개요 : 네이버 웍스에서 SAML, OAuth 2.0 구현 방법 소개  OASIS, SAML V2.0 Technical OverviewOAuth 2.0  OAuth 2.0 RFC  생활코딩, WEB2 - OAuth 2.0OIDC  Samsung SDS, 편의성을 높인 ID 인증 관리 - OIDC(OpenID Connect)가 주목 받는 이유  kakao developers, OIDC  OpenID Connect Spec  DaleSeo, 구글 OpenID Connect 사용법 : 실습 코드 제공  k8s 인증 완벽이해 #3 - OpenID Connect : 쿠버네티스 인증을 설명하는 글이지만, OIDC를 이해하기 위해 아주 좋은 글Hands-On  OAuth 2.0 Playground  jwt.io : jwt 토큰 Decode  Github Docs, Building OAuth App  passportjs기타 도움이 되는 콘텐츠  얄팍한 코딩사전, 세션 VS 토큰! JWT가 뭔가요? : 배경 지식이 없다면, 이 영상 먼저 보는 것을 추천  okta, What’s the Difference Between OAuth, OpenID Connect, and SAML?",
        "url": "/OIDC"
    }
    ,
    
    "golanginterface": {
        "title": "GO Lang Interface",
            "author": "HeuristicWave",
            "category": "",
            "content": "Explain the “interface” and “abstraction” of the go language.Intro해당 포스팅은 Tucker의 Go 언어 프로그래밍 20장 인터페이스를 읽고 정리한 내용임을 알립니다.8월은 31일이고 해당 도서도 31개의 Chapter로 구성되어 있어, 하루에 1장씩 공부하면 Go 언어를 익힐 수 있을 것 같다는 호기로운 생각이 인터페이스를 만나고 나서 사라졌습니다.이렇게라도 하지 않으면 올해도 Go 언어 공부를 미룰 것 같아 작성하게 되었습니다. 😵‍💫Interface인터페이스란 구현을 포함하지 않은 메서드 집합입니다. 구현을 포함하지 않았으므로 인터페이스는 구체화된 타입이 아닙니다. 즉, 추상화된 객체로 상호작용하기 위해 인터페이스를 사용합니다.선언 방법type(타입 선언) DuckInterface(인터페이스 명) interface(인터페이스 키워드) {    // 메서드 집합    Fly()    Walk(distance int) int}내부에 선언된 메서드는 반드시 메서드명(_(x int) 형태 불가)이 있어야 하며, 이름이 같은 메서드는 함께 있을 수 없습니다.왜 사용할까?예제를 통해 구체화된 객체가 아닌 인터페이스를 사용함으로써, 프로그램의 변경 요청에 유연하게 대응할 수 있는 방법에 대하여 알아보겠습니다.Fedex에서 아래와 같은 패키지 코드를 제공한다고 가정하겠습니다.package fedeximport \"fmt\"type FedexSender struct {}func (f *FedexSender) Send(parcel string) {    fmt.Printf(\"Fedex sends\" %v, parcel\\n\", parcel)}Fedex가 제공한 패키지를 이용해 상품 배송 기능을 만든다면 다음과 같습니다.package mainimport \"github.com/tuckersGo/musthaveGo/ch20/fedex\"func SendBook(name string, sender *fedex.FedexSender) {    sender.Send(name)}func main() {    sender := &amp;fedex.FedexSender{}    SendBook(\"Mastering Go\", sender)    SendBook(\"Mastering Rust\", sender)}여기서 한국의 우체국이 Fedex의 패키지를 활용해 아래 코드를 작성했다고 가정하겠습니다.package mainimport \"github.com/tuckersGo/musthaveGo/ch20/koreaPost\"import \"github.com/tuckersGo/musthaveGo/ch20/fedex\"func SendBook(name string, sender *fedex.FedexSender) {    sender.Send(name)}func main() {    sender := &amp;koreaPost.PostSender{}    SendBook(\"Mastering Go\", sender)    SendBook(\"Mastering Rust\", sender)}해당 코드를 빌드 하면, 우체국과 Fedex의 타입이 달라 다음과 같은 에러를 발생시킵니다. cannot use sender (variable of type *koreaPost.PostSender) as type *fedex.FedexSender in argument to SendBook인터페이스로 추상화 계층 만들기Fedex 패키지를 오류 없이 사용하기 위해서는 &amp;fedex.FedexSender{}와 같이 fedex 패키지의 타입과 동일하게 코드를 작성해야 합니다.그러나 이런 방법은 Fedex 패키지에 의존성이 존재할뿐더러 관리 측면에서도 유연하지 못한 방법이므로, 인터페이스를 사용해 해당 문제를 해결해 보겠습니다.type Sender interface {    Send(parcel string)}우선, Send() 메서드만 포함하는 인터페이스를 작성해 한국의 우체국 코드에 포함합니다.이어서 SendBook() 함수의 인수 *fedex.FedexSender를 Sender 인터페이스로 입력받을 수 있도록 코드를 수정하면,기존의 SendBook() 함수는 Sender의 인수가 Fedex 인지, UPS 인지 어떤 타입이든지 상관없이 받아들이는 유연한 코드가 됩니다.  👀 Interface를 적용한 코드 보기  package mainimport (  \"github.com/tuckersGo/musthaveGo/ch20/koreaPost\")type Sender interface {  Send(parcel string)}func SendBook(name string, sender Sender) {  sender.Send(name)}func main() {  sender := &amp;koreaPost.PostSender{}  SendBook(\"Mastering Go\", sender)  SendBook(\"Mastering Rust\", sender)}  이처럼 Sender 인터페이스 정의 시 인터페이스 구현 여부를 명시적으로 드러내지 않고 메서드 포함 여부로만 결정하는 방식을 duck typing이라고 합니다.덕 타이핑을 통해 내부 동작을 감춰 서비스 제공자(Fedex)와 사용자(우체국) 모두 자유도가 높아졌는데, 이런 방식을 추상화(abstraction)라고 합니다.즉, 인터페이스는 추상화를 제공하는 추상화 계층(abstraction layer)이며, 기존의 의존 관계를 끊는 디커플링(decoupling)을 가능하게 해줍니다.인터페이스 기능지금까지 인터페이스의 기본 기능을 알아보았다면, 이제부터는 아래 3가지 기능에 대해 알아보겠습니다.  인터페이스를 포함하는 인터페이스  비어있는 인터페이스  인터페이스 기본값 nilEmbedding Interface구조체에서 다른 구조체를 포함된 필드로 가질 수 있듯이 인터페이스도 다른 인터페이스를 포함할 수 있습니다.type Reader interface {    Read() (n int, err error)    Close() error}type Writer interface {    Write() (n int, err error)    Close() error}// 2개의 인터페이스의 합쳐지면서, 같은 메서드 형식의 Close() error가 하나 메서드만 포합됩니다.type ReadWriter interface {    Reader    Writer}위 인터페이스는 아래 각각의 타입에 따라, 사용할 수 있는 인터페이스가 다음과 같이 달라집니다.  Read(), Write(), Close() 메서드를 포함한 타입 : Reader/Writer/ReadWriter 모두 사용 가능  Read(), Close() 메서드를 포함한 타입 : Reader 만 사용 가능  Write(), Close() 메서드를 포함한 타입 : Writer 만 사용 가능  Read(), Write() 메서드를 포함한 타입 : Close() 메소드가 없으므로, Reader/Writer/ReadWriter 모두 사용 불가능Empty Interface어떤 값이든 받을 수 있는 함수, 메서드, 변숫값을 만들 때 빈 인터페이스를 사용합니다.func Sample(s interface{}) {    x := s.(type)}Sample() 함수는 빈 인터페이스를 인수로 받으므로, 모든 타입을 인수로 사용할 수 있습니다.이런 특징을 활용하여 switch 구문에서 타입별로 다른 로직을 수행하도록 할 수 있습니다.func Sample(s interface{}) {    switch x := s.(type) {    case int:        fmt.PrintF(\"s is int %d\\n\", int(x))    case string:        fmt.PrintF(\"s is string %s\\n\", string(x))    default:        fmt.PrintF(\"Not supported type: %T:%s\\n\", x, x)    }}nil Interface인터페이스 변수의 기본값은 유효하지 않은 메모리 주소를 나타내는 nil입니다.Attacker라는 인터페이스가 존재할 때, 아래와 같이 변수 att의 초깃값이 없으므로 해당 값은 nil이 됩니다.func main() {    var att Attacker    att.Attack()}  att의 메모리 주소는 nil이므로 런타임 에러가 발생하므로, 인터페이스를 사용할 때는 항상 인터페이스 값이 nil이 아닌지 확인해야 합니다.인터페이스 변환하기인터페이스 변수는 타입 변환을 통해서 구체화된 다른 타입이나 다른 인터페이스로 타입 변환이 가능합니다.구체화된 다른 타입으로 타입 변환하기인터페이스 변수 a를 ConcreteType으로 변환하 법var a Interfacet := a.(ConcreteType)👀 구체화된 다른 타입으로 변환하는 예시package mainimport \"fmt\"type Stringer interface {\tString() string}type Student struct {\tAge int}func (s *Student) String() string {\treturn fmt.Sprintf(\"Student Age:%d\", s.Age)}func PrintAge(stringer Stringer) {\ts := stringer.(*Student)                // 3. 인터페이스 변수를 *Student 타입으로 변환\tfmt.Printf(\"Age: %d\", s.Age)}func main() {\ts := &amp;Student{15}                       // 1. *Student 타입 변수 s 선언\tPrintAge(s)                             // 2. 변수 s를 인터페이스 인수로 제공}main() 내부에 선언된 구조체 포인터 *Student 타입 변수 s를 선언하고(주석 1번), 주석 2번에서 Stringer 인터페이스 변수로 PrintAge() 함수를 호출했습니다.이어서 Stringer 인터페이스 변수는 Age값에 접근할 수 없으므로 주석 3번에서 *Student로 타입이 변환되었습니다.이어서 이러한 구조체 변환 시, 자주 만나는 컴파일 에러를 알아보겠습니다.❗️ 타입 변환 실패 (컴파일 타임)인터페이스 변수를 구체화된 타입으로 변환하려면 해당 타입이 인터페이스 메서드 집합을 포함해야 합니다.예를 들어 방금 예시에서 아래와 같이 가 String() 메서드를 포함하지 않는다면, 컴파일 타임 에러가 발생합니다.func (s *Student) String() string {\treturn fmt.Sprintf(\"Student Age:%d\", s.Age)}즉, 위 메서드가 없다면 주석 3번과 같은 Stringer 인터페이스에서 *Student로 타입 변환이 불가합니다.다른 인터페이스로 타입 변환하기ConcreteType이 AInterface와 BInterface 인터페이스 모두를 포함하고 있을 경우에는 아래와 같이 다른 인터페이스로 타입 변환이 가능합니다.var a AInterface = ConcreteType{}b := a.(BInterface)❗️ 타입 변환 실패 (런 타임)서로 다른 인터페이스로 타입 변환 시, 서로 다른 메서드 집합을 가지고 있어도 문법적으로 문제가 발생하지는 않습니다.그러나 경우에 따라, 타입 변환에 실패하여 런 타임 에러가 발생할 수 있습니다.  👀 다른 인터페이스로 타입 변환이 실패하는 예시  package maintype Reader interface {\tRead()}type Closer interface {\tClose()}type File struct {}func (f *File) Read() {}func ReadFile(reader Reader) {\tc := reader.(Closer)\tc.Close()}func main() {\tfile := &amp;File{}\tReadFile(file)}        Reader 인터페이스 변수를 Closer 인터페이스 타입으로 변환하려 하나, reader 인터페이스 변수가 가리키는 *File 타입이 Close() 메서드를 포함하지 않으므로 타입 변환에 실패  런 타임 에러가 발생하는 문제를 방지하기 위해, 아래와 같이 타입 변환 성공 여부를 반환하는 코드를 작성할 수 있습니다.var a Interfacet, ok := a.(ConcreteType)   // t: 타입 변환 결과, ok: 변환 성공 여부  👀 타입 변환 성공 여부를 반영한 예시  func ReadFile(reader Reader) {\tc, ok := reader.(Closer)\tif ok {\t\tc.Close()\t}\t\t/**\t* 한 줄로 표현\t* if c, ok := reader.(Closer); ok {}\t*/}  Outro처음으로 책을 읽고 정리한 내용을 작성했는데, 이해한 내용을 바탕으로 재구성하는 것도 쉽지 않은 것 같습니다.제가 레퍼런스로 차용한 도서의 저자가 유튜브에 공개한 강의(Tucker Programming)와요약을 덧붙이며 이번 포스팅을 마무리 짓도록 하겠습니다.  인터페이스는 구현을 포함하지 않은 메서드 집합니다.  인터페이스에서 정의 시, 메서드 포함 여부로만 결정하는 덕 타이핑을 통해 자유도 높은 프로그래밍이 가능하다.  인터페이스로 추상화 계층을 만들고 상호작용을 정의한다.  인터페이스는 인터페이스 자체를 포함하거나 빈 상태로 사용할 수 있으며, 기본값은 nil이다.  인터페이스는 구체화된 다른 타입이나 다른 인터페이스로 변환이 가능하며 타입 변환 시 에러를 고려해야 한다.소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해주세요! 😃",
        "url": "/GoLangInterface"
    }
    ,
    
    "declarativevsimperativeink8s": {
        "title": "Declarative vs Imperative in Kubernetes",
            "author": "HeuristicWave",
            "category": "",
            "content": "Kubernetes Object Management에서 발견한 명령형/선언형 방식IntroKubernetes의 리소스들을 제어하기 위해 kubectl 명령어를 다루다 보니,replace, create, apply 등과 같이 비슷하게 동작하는 명령어들에 대해서 어떤 차이가 있는지 궁금증이 생겼습니다.help 명령어를 사용하면 각각 다음과 같이 동작한다는 정보를 얻을 수 있습니다.  apply : Apply a configuration to a resource by file name or stdin create : Create a resource from a file or from stdin해당 설명으로는 궁금증이 말끔히 해소되지 않아 공식 문서를 읽다, Kubernetes Object Management를 통해 머리를 스치는 깨달음을 얻었습니다.Kubernetes Object Management문서의 첫 부분은 쿠버네티스 객체를 관리하는 기법으로 다음 3가지 기법을 소개합니다.  Imperative commands  Imperative object configuration  Declarative object configurationImperative(명령형, 절차형), Declarative(선언형)의 개념은 단어 뜻에서도 알 수 있지만, 제가 기존에 숙지하고 있던 개념을 먼저 기술하고 쿠버네티스에서는 어떻게 다른지 적어보겠습니다.Declarative vs ImperativeDeclarative vs Imperative 개념은 프로그래밍에서도 종종 보이지만, AWS 인프라 구축에 빗대 표현해 보겠습니다.왼쪽 현재(Current) 상태를 원하는(Desired) 상태로 만들어야 하는 상황을 가정하겠습니다.여기서 Imperative하게 구성한다면 AWS CLI와 같은 도구를 통해서 다음 순서로 Desired State를 만듭니다.  웹 서버 2개 추가  Rule 추가  권한 부여반면 Declarative하게 구성한다면 Terraform과 같은 선언적 성격을 가진 도구 다음과 같이 구성합니다.웹 서버 3대, Rule, 권한 구성Terraform에서는 Desired State를 코드로 작성한 *.tf 형식의 파일을 apply하여 Declarative하게 인프라를 다룹니다.이제 기존에 제가 제대로 알고 있지 않았던 “Kubernetes에서는 Desired State를 YAML 형식의 파일을 활용해 Declarative하게 인프라를 다룬다.”라는 반쪽짜리 정답에 대하여 알아보겠습니다.Imperative commands공식 문서의 명령형 명령의 사용법은 다음과 같은 예시와, 해당 방법은 이전 history를 제공하지 않으므로 일회성 작업에만 추천한다고 기재되어 있습니다.kubectl create deployment nginx --image nginx해당 방법은 명령어 한 줄로 리소스를 생성할 수 있어 비교적 간편한 방법이지만, history를 제공하지 않는 점이 단점이라는 이유는 다른 방법들을 소개한 다음 설명하겠습니다. 😒Imperative object configuration명령형 오브젝트 구성 방법 kubectl 명령어와 create, replace 등과 같은 명령과 옵션 플래그 및 파일 이름이 필요합니다.파일 이름 YAML 혹은 JSON 형식의 오브젝트에 대한 정의를 포함하고 있어야 합니다.명령어와 파일을 활용해 리소스를 Create/Delete/Update 하는 방법kubectl create -f nginx.yamlkubectl delete -f nginx.yamlkubectl replace -f nginx.yaml해당 방법은 Imperative Command 방식과 비교하여, YAML 파일을 활용하기 때문에 형상 관리가 가능한 이점이 생겼습니다.그렇지만 직접 YAML 파일을 작성해야 하는 추가적인 단계가 발생했습니다.여기서 저는 의문이 들었습니다. Terraform의 *.tf과 같이 YAML 파일로 Desired 상태를 만드는 방법이 왜 명령형 오브젝트 구성 방법인지.이 떡밥도 마지막 방법을 소개한 이후 회수하도록 하겠습니다. 🤫Declarative object configuration공식문서에서는 해당 기법을 설명하기 위해 아래와 같은 쉽게 이해되지 않는 설명이 기재되어 있습니다.  When using declarative object configuration, a user operates on object configuration files stored locally,however the user does not define the operations to be taken on the files.Create, update, and delete operations are automatically detected per-object by kubectl.  declarative object configuration 방식을 사용할 때, 사용자는 로컬에 저장된 개체 구성 파일에 대해 작업하지만,사용자는 파일에 대한 작업들을 정의하지는 않는다. Create, update, and delete 작업들은 객체별로 자동으로 감지된다.즉, Imperative object configuration 방식에서는 사용자가 Create, Update, Delete를 명령어로 결정했습니다. (사용자의 판단)Declarative object configuration 방식에서는 오로지 구성 파일에서 정의한 대로 삭제 혹은 생성 등이 작동합니다. (정의한 대로 작동)Declarative object configuration 방식은 configuration 파일이 위치한 /config 디렉토리에 diff, apply 명령어로 작동시킵니다.구성 파일이 여러 개라면 -R 옵션을 함께 넣어줍니다.kubectl diff -f configs/    // 정의한 config 적용에 대한 결과 예상kubectl apply -f configs/   // 정의한 config 적용diff는 마치 테라폼의 plan과 같은 역할을, apply 테라폼의 apply처럼 동작합니다 :) 여담으로 저는 과거 테라폼관련 포스팅 당시 apply 하기 전, plan 명령어의 중요성을 여러 번 강조했었는데,정작 저는 kubernetes manifest 파일들을 적용할 때, --dry-run 옵션만으로 리소스가 현재 상태에 미치는 영향을 판단하고 diff를 적극적으로 사용하지 않았던 모습이 부끄럽습니다. 😣추가적으로 Imperative object configuration 방식과 Declarative object configuration 방식을 혼용하여 사용하는 예시를 통해,apply와 반대되는 명령어가 delete가 아닌 이유를 생각해 보며 다음 Annotations 차례로 넘어가겠습니다.kubectl apply -f nginx.yaml   // Declarative object configuration 방식으로 리소스 생성kubectl delete -f nginx.yaml  // Imperative object configuration 방식으로 리소스 삭제Annotations위 3가지 방식을 설명하면서 공식 문서에 기재된 각 방식의 Trade-offs에 대한 내용들을 대부분 생략했습니다.상태에 대한 기록을 설명하지 않았기 때문이죠. kubectl apply는 이전의 호출 이후 구성의 변경 사항을 판별하기 위해 리소스에 어노테이션을 첨부합니다.이를 통해, Declarative object configuration 방식은 History를 기록하며 Audit도 가능하게 되었습니다.해당 어노테이션은 쿠버네티스 오브젝트 metadata 하위 annotations에 kubectl.kubernetes.io/last-applied-configuration 이름으로 현재 리소스에 적용된 config 값들이 저장되어 있습니다.  apply 명령어로 생성한 리소스를 kubectl get {Type} {Name} -o yaml 명령어로 조회하면 어떤 값이 적용되었는지 조회 가능합니다.🧑‍🔬 Lab앞서 설명한 object configuration 방식들을 직접 체험할 수 있도록 예제 코드를 작성해 두었습니다.아래 실험용 Manifest 파일들을 생성하고 다음 명령어로 어노테이션을 조회해보세요. kubectl get deploy nginx-declarative -o yaml | grep \"annotations\"  🪜 Imperative object configuration    cat &lt;&lt;EOF &gt; nginx_imperative.yamlapiVersion: apps/v1kind: Deploymentmetadata:  name: nginx-imperativespec:  replicas: 1  selector:    matchLabels:      app: nginx-imperative  template:    metadata:      labels:        app: nginx-imperative    spec:      containers:      - image: nginx        name: nginx-imperativeEOFkubectl create -f nginx_imperative.yaml    📣 Declarative object configuration    cat &lt;&lt;EOF &gt; nginx_declarative.yamlapiVersion: apps/v1kind: Deploymentmetadata:  name: nginx-declarativespec:  replicas: 1  selector:    matchLabels:      app: nginx-declarative  template:    metadata:      labels:        app: nginx-declarative    spec:      containers:      - image: nginx        name: nginx-declarativeEOFkubectl apply -f nginx_declarative.yaml  🖍 오답 노트이제 떡밥 회수 겸, 기존에 제가 제대로 알고 있지 않았던 “Kubernetes에서는 Desired State를 YAML 형식의 파일을 활용해 Declarative하게 인프라를 다룬다.”라는 반쪽짜리 정답에 대하여 오답노트를 적어보겠습니다.Declarative 도구인 Terraform은 *.tf에 상태를 정의함과 동시에 apply 이후 자동으로 생성되는 *.tfstate 파일에 현재 상태가 함께 기록됩니다.Kubernetes에서는 YAML로 정의하지만, 적용된 config 정보가 포함되지 않은 방법은 Imperative object configuration 방식입니다.이제서야 apply command를 사용해 last-applied-configuration 가 함께 기록되는 Declarative object configuration 방식을 제대로 이해한 것 같습니다.Outro글을 마치며 주저리주저리 떠오른 생각들을 적어보겠습니다.저는 매번 블로그 포스팅 소재를 고민합니다. 실제로 10개의 포스팅 아이디어가 떠오르지만, 블로그 글로 탄생하는것은 1~2개 뿐입니다.실제로 뭔가 트러블 슈팅에 대한 글을 작성하려고 해도, 나중에 공식문서를 보니 더 잘 작성된 것 같은 느낌을 종종 받습니다.또한 애초에 문서를 제대로 읽었다면, 해당 이슈를 만나지 않았을 것만 같아 업로드를 포기하는 글이 있었습니다.이런 고민이 너무 길어져 주기적인 업로딩이 늦어지는 점은 명확한 단점인 것 같습니다. (아 물론! 매일 현실과 타협하여 미루다보니 늦어지는게 가장 큰 이유겠지만요)그래서 이번에는 기술블로그를 2주에 1편을 쓰자는 약속을 지키기 위해서… Kubernetes Object Management를 해설과 동시에,이번 포스팅은 제가 공식문서를 어떻게 읽는지 읽으면서 어떤 생각을 하는지에 대한 사고의 흐름을 담으며 이해하는 과정을 담았습니다.소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해주세요! 😃",
        "url": "/DeclarativeVSImperativeinK8s"
    }
    ,
    
    "terraformtips5": {
        "title": "Terraform Tips 5 - Import",
            "author": "HeuristicWave",
            "category": "",
            "content": "Terraform 더 익숙하게 5 - ImportIntroIaC를 도입하기 위해 구축 단계부터 코드로 인프라를 작성할 수도 있지만, 기 구축된 인프라를 코드화할 수도 있습니다.이때 사용하는 Terraform의 기능이 바로 Import입니다.하지만 저는 구축 단계에도 종종 Import 기능을 활용합니다. Terraform으로 코드를 작성하기 위해 registry.terraform.io에서가이드 하는 대로 코드를 작성하는 것이 생각보다 어려운 작업이기 때문이죠. 🥲그래서 저는 먼저 구축하고자 하는 인프라를 콘솔상에서 구성한 다음, 구축에 필요한 Attribute 들을 파악합니다.그다음 Import를 사용해 동작하는 IaC 코드를 얻고 수정합니다.즉, 저는 Import 기능을 Cheat Sheet처럼 사용하고 있습니다. 😅이번 포스팅에서는 실제 제가 Cheat Sheet으로 활용하는 ‘Import 시나리오’를 통해 Import를 학습해 보겠습니다.⏮ 조립은 분해의 역순AWS Systems Manager의 인스턴스 운영 자동화를 위한 State Manager 기능을 사용하기 위해 문서를확인해 보았지만, 다음과 같은 사용법 만이 기재되어 있습니다.resource \"aws_ssm_association\" \"example\" {  name = aws_ssm_document.example.name  targets {    key    = \"InstanceIds\"    values = [aws_instance.example.id]  }}해당 코드를 apply 해도 무수한 Error만 만날 뿐 빠르게 진도가 나가지 않기에, 우선 AWS 웹 콘솔을 활용해 인스턴스 운영 자동화를 위한 State Manager 기능을 구현해 두었습니다.0️⃣ 준비 작업이번 포스팅의 작업 공간(~/terraform)을 생성하고 해당 위치에서 아래 코드 블록을 터미널에 복사합니다. (리소스가 위치한 리전 명에 맞게 세팅해 주세요)cat &lt;&lt;EOF &gt; provider.tfprovider \"aws\" {  region  = \"ap-northeast-2\"}EOF이후, terraform init 명령어를 실행시켜주세요.1️⃣ Skeleton Code 작성웹 콘솔로 작업한 State Manager를 코드화하기 위해 아래와 같이 Skeleton Code를 작성합니다.(import 후, 생성되는 *.tfstate를 담는 일종의 빵틀을 제작하는 단계입니다.)cat &lt;&lt;EOF &gt; main.tfresource \"aws_ssm_association\" \"copycat\" {}EOF2️⃣ Import Configuration사용법(terraform import [options] ADDRESS ID)에 따라 아래 명령어를 실행시키면 root directory에 미리 생성된 인프라가 *.tfstate 파일에 담깁니다.  ⚠️ 본 예시는 SSM을 기준으로 작성되었습니다. SSM인 경우 아래와 같이 Association ID를 기재합니다.다른 리소스일 경우, 인스턴스 넘버, 파이프라인 이름 등과 같은 고유한 이름을 기재해야 합니다.$ terraform import aws_ssm_association.copycat &lt;Association ID&gt;생성된 *.tfstate 파일을 확인하면 json 형태로 aws_ssm_association resource block에 작성해야 하는 각종 Config 값들을 알 수 있습니다.그러나, show 명령어를 사용해 HCL Syntax에 맞춰 human-readable한 형태로 출력합니다.$ terraform show -no-color &gt; main.tf본래 -no-color 옵션은 coloring 작업을 비활성화하지만, Editor에 format 맞추기 위해 필수적으로 해당 옵션을 사용합니다.3️⃣ Modify Arguments이제서야 얼추 모양을 갖춘 main.tf의 resource block에는 리소스가 인프라에 반영된 이후 단계에 생성되는 각종 result 값(arn, association_id, id)이 포함되어 있습니다. 해당 값들은 terraform plan 명령어를 수행해 Error 메시지에 명시되므로 지워야 하는 Arguments들을 찾아 코드를 수정합니다.이때, Instancd Id, IAM Role ARN 등과 같이 절대적인 값도 재사용 가능한 변수로 처리하는 것이 좋습니다.해당 작업을 마치고 나면, plan, apply 명령어를 수행하여 다음 메시지를 얻으면 정상적으로 Import 작업이 완료된 것입니다.  Apply complete! Resources: 0 added, 0 changed, 0 destroyed.🥵 Import into Module지금까지 학습한 절차는 단순 Resource에 Import 시키므로 비교적 수월한 과정이었습니다.  Resource configured with count ➡️ terraform import 'aws_instance.baz[0]' i-abcd1234 Resource configured with for_each ➡️ terraform import 'aws_instance.baz[\"example\"]' i-abcd1234 Module ➡️ terraform import module.foo.aws_instance.bar i-abcd1234Module을 Import 하는 절차도 Resource와 동일하지만, 명백한 한계점이 있습니다.흔하게 사용되는 ec2-instance 모듈을 사용한다고 가정하겠습니다.Resource 때와 동일하게 아래와 같은 Skeleton Code를 작성하고 Import 명령어를 수행하는 부분은 동일합니다. (1️⃣ &amp; 2️⃣ 과정 동일)module \"ec2_instance\" {  source  = \"terraform-aws-modules/ec2-instance/aws\"  version = \"~&gt; 3.0\"  name = \"single-instance\"}이때, foo는 ec2_instance가 되고 bar는 해당 모듈의 aws_instance에서 정의한 this가 됩니다.문제는 3️⃣ Modify Arguments 단계에서 발생합니다. Single Resource에서는 plan을 통해 수정해야 하는 Arguments들을 알 수 있지만, module에서 skeleton code 이외에 더 기재해야 하는 variable 값들을 알 수 없습니다.즉, module import 이후 생성되는 *.tfstate 파일의 json 값을 일일이 확인하여 module의 input parameter에 해당하는 값들을 알아내 하나씩 다 기재하는 방법 외에는 온전하게 import 명령어를 사용할 수 없습니다.Reverse Engineering으로 원래의 코드를 완벽하게 재현하기 어려운 것처럼, 구성이 복잡한 모듈은 Reverse Terraforming이 매우 어렵습니다.🌏 Reverse Terraform Open Source2️⃣ Import Configuration 단계에서 수행한 작업은 Terraformer(작성 시점 기준 ★ 8.1k),Terraforming(업데이트 종료) 등과 같은 오픈소스 도구를 활용할 수도 있습니다.⚠️ 고려 사항Terraformer는 AWS configuration Profiles Select와 Attribute filters과 같은 편리한 기능들을 제공합니다. 그러나 terraformer AWS 리소스 지원 범위에서도 확인할 수 있다시피,위에서 작업한 aws_ssm_association과 같이 지원하지 않는 리소스들도 존재합니다.Outro마지막으로 Import Workflow를 다시 한번 정리하면서 마치겠습니다.  Import 대상(이미 프로비저닝 된 인프라)이 되는 Skeleton Code 작성  Write Config : Import &amp; Show 명령어 수행  Modify Arguments  Plan &amp; Apply지금까지 테라폼 더 익숙하게 Import 편을 읽어주셔서 감사합니다! 잘못된 내용은 지적해 주세요! 😃📚 References  Terraform Documentation Import Command  Hashicorp Tutorial 문서 Import Terraform ConfigurationTerraform Tips    Module &amp; Output    Data &amp; Index    Refresh &amp; Replace    Move (Refactoring)    Import",
        "url": "/TerraformTips5"
    }
    ,
    
    "ecs-walkthrough": {
        "title": "ECS on EC2 Walkthrough",
            "author": "HeuristicWave",
            "category": "",
            "content": "EC2 기반의 ECS를 다루기 위한 사소한 지식들 톱아보기!Intro당장 Amazon Elastic Container Service(이하 ECS)를 운영해야 하지만, 공식 문서를 다 읽기에는 벅차고 중요한 운영 포인트들을 빠르게 학습하기 위해 아래와 같은 요소들을 다룹니다.ECS 마스터가 될 수 있는 모든 것을 다루는 것은 아니지만, 최소한의 고민해 볼 만한 지점들을 다뤄보았습니다.  Amazon ECS Container Agent  ECS 리소스 할당  ECS Scaling  ECS 서비스 구성🕋 ECS Container AgentEC2 기반의 ECS를 운용하기 위해서는 ECS 최적화 AMI를 사용해야 합니다.일반 EC2 인스턴스로도 ECS를 운용할 수 있지만, ECS 최적화 AMI를 사용하는 것이 관리와 운용 측면에서 유리합니다.  예를 들어 Docker를 운영하다 보면, 미사용 상태인 컨테이너 이미지가 쌓여나가 prune 명령어를 통해 이미지를 정리해 주어야 합니다.이런 상황에서 컨테이너 에이전트는 다양한 자동화된 이미지 정리 옵션을 제공합니다.ECS-optimized AMI에는 Amazon ECS Container Agent가 기본적으로 포함되어 있습니다.ECS 인스턴스를 부트스트랩 하는 단계에서 EC2의 user data를 사용해 /etc/ecs/ecs.config에 configuration parameters을 전달합니다.아래와 같이 환경 변수를 지정하지 않아도 default 값이 지정되어 있어 운용상의 큰 문제는 없지만,공식 문서를 확인해 필요한 configuration 들을 파악해야 합니다.#!/bin/bashcat &lt;&lt;'EOF' &gt;&gt; /etc/ecs/ecs.configECS_CLUSTER=MyClusterECS_LOGLEVEL=debugEOF다양한 configuration 중, 운영 환경(GPU, SPOT, 서비스 등)에 따라 필요한 Configuration 값들이 다르겠지만 일반적으로 ECS_RESERVED_MEMORY는 고려하여 지정하는 것이 좋습니다.  💡 인스턴스의 모든 메모리를 테스크에 배정한다면, 테스크와 중요한 시스템 프로세스 사이에서 메모리 경합이 발생할 수 있습니다.이를 예방하기 위해 ECS_RESERVED_MEMORY configuration을 사용해 메모리를 예약함으로써 풀에서 할당 가능한 메모리를 제외할 수 있습니다.필요로 하는 최소한의 요구 메모리가 정의되어 있지는 않지만, 저는 문서의 예시처럼 256MiB로 사용하고 있습니다.🎛 ECS ResourceECS는 3가지 범주로 리소스를 설정할 수 있습니다.🥇 Container instance : ECS 클러스터를 이루고 있는 EC2 인스턴스의 유형 🥈 Task Size : Task Definitions을 통해 정의하는 Size 🥉 Container Size : Task Definitions의 Container 정의 부분에서 정의하는 Size   💡 ECS on EC2에서는 Task Size와 Container Size 방식 중 선택 가능하지만, Fargate 방식에서 Task Size는 필수입니다.Container definitionsContainer Size의 리소스 할당 파라미터는 각각 다음과 같은 docker run 명령어 옵션에 매핑됩니다. cpu : --cpu-shares, memory : --memory(hard) / --memory-reservation(soft)CPU : Unit 단위 (1,024개의 CPU 유닛은 vCPU 1개), Memory : Hard/Soft Limits Hard 🆚 Soft  Hard : 명시한 값만큼 리소스 제약이 생깁니다. 때문에 클러스터 인스턴스에서 docker stats 명령어를 조회하면 LIMIT 값이 Hard로 명시한 값으로 표시됩니다.  Soft : 명시한 값이 클러스터 인스턴스에 예약됩니다. Soft limit이므로 컨테이너 메모리 사용량이 명시한 값의 제한을 넘겨 사용 가능합니다.Hard와 달리, 클러스터 인스턴스에서 docker stats 명령어를 조회하면 LIMIT 값이 클러스터 인스턴스의 총 자원으로 표시됩니다.🥇로 갈수록 더 상위 개념이며, Task Definitions에서는 🥈, 🥉을 활용해 리소스를 제어합니다.세밀한 제어를 하고 싶다면 🥈, 🥉을 모두 사용하여 제어할 수 있으나,🥉 사용해야 하는 특별한 이유가 없다면 🥈만을 사용해 리소스를 제어하는 것이 용이합니다.Actual available memory16GiB의 인스턴스를 프로비저닝해도 실제 사용 가능한 메모리(15318 MiB)는 더 적습니다.            Amazon EC2      인스턴스      free -m      Registered                  c5.2xlarge      16384 MiB      15574 MiB      15318 MiB      EC2 인스턴스와 free -m 명령어로 확인한 차이ECS 플랫폼 메모리 오버헤드와 OS 커널이 차지하는 메모리로 인해 차이가 발생합니다.free -m 명령어로 확인한 메모리와 컨테이너 인스턴스(registered)의 차이컨테이너 에이전트를 설정할 때, ECS_RESERVED_MEMORY=256으로 설정한 만큼 차이가 발생합니다.🏘 ECS ScalingECS의 Scaling 방법은 2가지로 정의할 수 있습니다. 해당 스케일링 기법은 선택 사항이 아니라, 2가지 모두를 고려해 적용해야 합니다.  Service에 의하여 Task가 Scaling 되는 Horizontal Autoscaling  CapacityProvider에 의하여 컨테이너 인스턴스가 Scaling 되는 Cluster AutoscalingService auto scaling서비스 스케일링은 또다시 Target tracking과 Step scaling으로 나뉩니다.Target tracking은 CPU/Memory 사용률 및 ALB 요청 횟수를 기반의 정책이 있으며, Step 방식은 Alarm을 활용한 Custom 정책들을 작성할 수 있습니다.  💡 Step scaling policies의 문서 첫 문장은 다음과 같습니다. Although Amazon ECS service auto scaling supports using Application Auto Scaling step scaling policies, we recommend using target tracking scaling policies instead. Step scaling은 조금 더 공격적인 정책이 필요할 때 대안으로 사용하고, Service auto scaling은 복수의 조정 정책을 동시에 활용할 수 있으므로 하나의 정책에 의존하기보다는 복수의 정책을 사용해 보다 세밀한 Scaling 정책을 만드는 게 어떨까요? 🧐Cluster auto scalingService auto scaling에 컨테이너 인스턴스 내 자원을 다 할당했을 경우, CapacityProvider EC2 Auto Scaling을 활용해 클러스터 자원을 확보합니다.AWS 블로그에 자세한 작동원리가 나오므로 꼭 한번 읽어보시기 바랍니다.해당 내용을 요약하자면 Capacity Provider를 Cluster 인프라를 관리합니다.이를 위해 CapacityProviderReservation 지표가 존재하고 사전에 설정한 Target capacity %(1~100사이의 값)에 맞춰 EC2 AutoScalingGroup(ASG)에 Trigger가 작동합니다.CapacityProviderReservation(%) = M(현재 인스턴스+추가 요청 인스턴스)/N(현재 인스턴스) * 100(%)만약 Target capacity을 100으로 지정했을 때, 현재 Cluster의 Node 개수가 3이고 CapacityProviderReservation가 200이라면CapacityProviderReservation를 목표치(Target capacity)인 100에 맞추기 위해 3개의 EC2를 Scale-out 시킵니다.  ⚠️ To create an empty Auto Scaling group, set the desired count to zero.  Capacity Provider는 Desired 값이 0인 ASG와 연결되어야 합니다. 또한 CapacityProvider에 의하여 관리형 조정을 enable 한 상태에서,ASG를 수동으로 수정한다면 CapacityProviderReservation 계산에 영향을 미칠 수 있으므로 지양해야 합니다. 🚫️ DO NOT EDIT OR DELETE 해당 메시지는 Service &amp; Cluster Scaling이 CloudWatch에 자동으로 생성한 TargetTracking의 주석 내용입니다.관리형 정책의 기능을 활용 시, 조금 더 세밀한 Scaling 필요하다면 알람 Trigger의 빈도가 아닌 다른 방안을 고민하도록 합니다!🧮 ECS Service ConfigurationDeployment ConfigurationECS 서비스에서 배포와 관련된 설정을 보면, maximumPercent와 minimumHealthyPercent가 있습니다.비슷하면서도 헷갈리기 쉬운 2개의 설정값 개념은 손에 잡힐듯하면서도 쉽게 잡히지 않는 것 같아, 예시 상황을 적어보았습니다.😂해당 개념이 조금 더 와닿을 수 있도록, 공식 문서를 읽어보시고 아래 상황이 어떨지 예상해 보세요!🧑🏻‍💻는 ECS maximumPercent가 200%, minimumHealthyPercent가 100%로 설정되어 있으며, Rolling update 방식을 사용하고 있다.현재 ver01을 운영하는 🧑🏻‍💻는 실수로 오류를 포함한 ver02를 배포했다. 기존 Task 4개일 때, 어떤 상황이 벌어지는가?  🖍 정답 보기  minimumHealthyPercent가 100%이기 때문에 ver02에 프로비저닝되고 정상 상태로 확인될 때까지 ver01은 중단되지 않습니다.ver02는 running 상태로 진입하지 못해 provisioning-pending-stopped 단계를 반복합니다.이때 maximumPercent가 200%임에 따라 ver02 Task는 4개와 ver01 Task 4개(합, 최대 8개)의 Task가 동시에 올라올 수 있습니다.Task PlacementECS의 서비스가 컨테이너 인스턴스에 Task를 배치하는 전략은 아래와 같이 3가지 분류됩니다.  binpack: CPU 또는 메모리를 최소화하기 위해 유휴 자원을 고려한 배치  random : 무작위 배치  spread : ami-id, availability-zone, instance-type, os-type 등을 고려한 배치위 3가지 전략은 단독 혹은 복수로 선택되어 사용될 수 있으며, 가용성을 확보하기 위해 AZ를 고려하여 AZ + binpack, AZ + spread와 같이 사용되기도 합니다.쿠버네티스를 공부해 보신 분이라면, 해당 전략은 마치 k8s의 nodeSelector와 비슷하게 동작합니다.  🔊 CapacityProviderReservation에 영향을 미치는 배포 설정과 배치 전략 ECS 서비스 설정에서 언급한 배포 설정과 배치 전략은 가용성 문제와 직결되고 이는 비용 문제로도 이어집니다.앞서 언급 한 CapacityProviderReservation 계산에 활용되는 M 값에 배포 설정과 배치 전략이 영향을 미친다는 점을 유의하세요!Outro이번 포스팅을 통해 ECS와 관련된 내용을 정리하다 보니, AWS 내의 다른 오케스트레이션 서비스인 EKS의 운용 전략과 무척이나 비슷하다는 느낌을 지울 수 없었습니다.쿠버네티스의 HPA 최적화, Pod 배치 및 리소스 할당 전략과 같은 포인트들은 운영을 하며 지속적으로 관리하는 관리 대상인 만큼,위에 언급된 ECS의 운영 포인트들도 서비스를 배포한 이후에도 지속적으로 관심을 가져야 하는 포인트임을 강조하며 마칩니다.소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해 주세요! 😃",
        "url": "/ECS_Walkthrough"
    }
    ,
    
    "terraformtips4": {
        "title": "Terraform Tips 4 - Move (Refactoring)",
            "author": "HeuristicWave",
            "category": "",
            "content": "Terraform 더 익숙하게 4 - Move (Refactoring)Intro지난 3편에서는 코드의 변경 없이 형상을 유지하는 기법 중 하나인 Refresh와 Replace에 대하여 알아보았습니다.4편에서는 코드를 변경(Refactoring) 할 때 형상을 유지하는 방법인 Move와 관련된 기능들을 소개합니다.🤹 Moving ResourcesTerraform으로 정의한 인프라는 terraform.tfstate에 기록되며, real-world의 객체는 특정 리소스 주소와 연결되어 있습니다.그래서 정의한 인프라 코드를 변경 후, 적용하면 real-world의 객체와 상태가 변경됩니다.코드로 정의된 실제 인프라를 운영하고 있다면, 코드 리팩토링 시 발생하는 리소스 변경 지점을 State 명령어를 활용해 해소해야 합니다.Commands  state mv : real-world 객체와 연결된 리소스 주소를 변경  state rm : real-world 객체를 파괴하지 않고 코드로 정의한 리소스 관리 대상 제거  state replace-provider : 재생성 없이, 새로운 provider에 기존 리소스 전🛠 state mv위 3가지 명령어 중에서도 가장 활용도가 높은 state mv 명령어의 예시들을 확인해 보겠습니다.  리소스 이름 재정의 : 리팩토링 과정에서 정의된 리소스 명을 변경하는 경우  terraform state mv {ResourceType}.{ExistingName} {ResourceType}.{ChangedName}  리소스를 모듈로 이동 : 루트에 위치한 리소스를 child 모듈에 포함시켜 리팩토링 하는 경우 terraform state mv {Type}.{Name} module.{ModuleName}.{Type}.{Name}  모듈을 다른 모듈로 이동 terraform state mv module.{ModuleName} module.{ParentModuleName}.module.{ModuleName}  meta-argument for_each, count로 정의된 특정 리소스 교체 terraform state mv {ResourceType}.{Before}[0] {ResourceType}.{After}[0]🏃️ Moved statements지난 3편에서 refresh와 taint가 가진 한계점으로 인하여--refresh-only와 replace가 나왔듯이, state mv도 한계점이 있습니다.  The terraform taint command informs Terraform that a particular object has become degraded or damaged. Problems with terraform state mv위 링크로 첨부한 Terraform 1.1 버전이 출시하면서 발표한 Move 기능에 대한 발표 자료를 보면 아래 3가지 이유로 한계점을 다룹니다.  Risky and error prone  Terraform Cloud users couldn’t refactor within core workflows  Module authors couldn’t coordinate changes themselves3가지의 한계점이 언급되었지만, 결국 refresh 때와 마찬가지로 plan을 통한 예측 과정이 없는 동일한 이유로 위와 같은 문제가 발생한다는 사실을 알 수 있습니다.Refactoring앞서 언급한 한계점들은 plan 기능이 없는 mv 명령어 대신 기존의 테라폼 문법에 moved block이 추가되며 해소되었습니다.아래 예시는 공식 문서에 기재된 예제입니다.locals {  instances = tomap({    big = {      instance_type = \"m3.large\"    }    small = {      instance_type = \"t2.medium\"    }  })}resource \"aws_instance\" \"a\" {  for_each = local.instances  instance_type = each.value.instance_type  # (other resource-type-specific configuration)}moved {  from = aws_instance.a  to   = aws_instance.a[\"small\"]}moved block에 변경 전후 상태를 선언하여, 기존 상태를 바꾸는 명령조차도 코드로 선언하여 상태를 바꾸는 행위를 코드화했습니다.⁉️ 고려사항클라우드 환경에서 운영을 하다 보면 최적화 과정 중 리소스 스펙(인스턴스 종류, 타입)이 자주 변경됩니다.그럼 아래와 같은 moved 블록이 체인과 같이 길어지게 됩니다.# Block 1moved {  from = aws_instance.a  to   = aws_instance.b}# Block 2moved {  from = aws_instance.b  to   = aws_instance.c}이는 오히려 사용하지 않는 혹은 중복된 코드를 지우고 로직을 이해하기 쉽게 디자인해야 하는 리팩토링과는 멀어지게 됩니다.그러므로 AWS 리소스의 경우 단순 스펙 변경과 같은 작업은 Launch templates을 사용하는 게 좋습니다.이처럼 AWS 서비스 내에서 형상 관리 기능을 제공하는 서비스를 활용해, moved 블록을 생성하는 작업을 최소화해야 합니다.Outro지금까지 리팩토링 작업을 위해 필수적으로 사용되는 state mv와 moved syntax 사용법을 알아보았습니다.Move의 탄생 과정에서 선언형 도구인 테라폼의 목적에 맞게 진화해나가는 모습과 초기 설계의 중요성을 고민해 볼 수 있었던 좋은 기회였습니다.여담으로 저는 테라폼 버전이 0.12일 때 다루기 시작했는데, 2년 만에 버전 1.2.2에 이르며 정착해나가는 과정을 보니사용자도 IaC 도구의 철학을 이해하며 함께 성장하는 기분에 감격스럽습니다.지금까지 테라폼 더 익숙하게 Move(Refactoring) 편을 읽어주셔서 감사합니다! 잘못된 내용은 지적해 주세요! 😃Terraform Tips    Module &amp; Output    Data &amp; Index    Refresh &amp; Replace    Move (Refactoring)    Import",
        "url": "/TerraformTips4"
    }
    ,
    
    "terraformtips3": {
        "title": "Terraform Tips 3 - Refresh &amp; Replace",
            "author": "HeuristicWave",
            "category": "",
            "content": "Terraform 더 익숙하게 3 - Refresh &amp; ReplaceIntroIaC(Infrastructure as Code)를 운용하며 중요하게 생각하는 포인트 중 하나는, 코드로 정의한 형상과 실제 인프라의 형상을 동일하게 유지하는 것입니다.Terraform에서는 Configuration Drift(정의한 형상과 달라지는 경우)를 방지하기 위해 다양한 명령어를 제공합니다.이번 포스팅에서는 형상을 유지하는 다양한 기법 중 하나인 Refresh와 비슷하면서도 헷갈리는 개념인 Replace에 대하여 알아보겠습니다.♻️ Refreshrefresh 명령어는 Configuration Drift가 발생했을 때, 현재 상태를 terraform.tfstate에 반영해 현재 인프라의 상태로 최신화시키는 명령어입니다.문서에는 다음과 같이 기재되어 있지만, 처음 접한다면 무엇을 말하는지 쉽게 와닿지 않습니다.  The terraform refresh command reads the current settings from all managed remote objects and updates the Terraform state to match. terraform refresh 명령어는 원격 객체의 현재 상태를 읽어 Terraform state와 일치시킵니다.Refresh 명령어는 다음과 같은 상황에서 사용합니다. 클라우드 환경에서 클러스터를 운용하면 인스턴스의 Scale이 변화함에 따라 인스턴스 ID 값도 변합니다.이 경우 코드로 정의한 상태는 프로비저닝 당시 시점을 기억하지만, 실제 인프라의 현상은 최신 인스턴스의 상태를 가지고 있으므로 Drift가 발생합니다.위 상황에서 Refresh 명령어로 코드의 상태와 인프라의 현재 상태를 일치시킬 수 있지만, 해당 명령어는 deprecate 되었습니다. 왜냐하면 관리자가 무엇이 변경되는지 알지 못하고 tfstate가 최신화되기 때문입니다.그래서 테라폼 v0.15.4.에서부터는 plan과 apply에 -refresh-only 옵션을 제공하기 시작했습니다.–refresh-only왼쪽은 프로비저닝 당시 *.tfstate입니다. 오른쪽은 해당 환경을 콘솔에서 health check를 하는 간격을 120초로 변경한 화면입니다.의도적으로 Drfit 상황을 만들었기 때문에 terraform plan --refresh-only 명령어로 점검하면 다음과 같이 어떤 리소스 객체가 변경될 것인지 확인 가능합니다.위 사진에서는 elb의 AutoScalingGroup 내의 인스턴스가 바뀌어 최신화됨과, 제가 의도적으로 콘솔에서 변경한 health_check 값을 확인할 수 있습니다.이처럼 항상 IaC의 형상을 변경하기 전, plan을 통한 사전 검토 기능을 제공하기 위해 --refresh-only 옵션이 제공되었습니다.plan으로 문제없음을 확인했다면, 이어서 terraform apply --refresh-only 명령어로 *.tfstate를 최신화 시킵니다.⚠️ 주의*.tfstate는 Refresh로 최신화되었지만, 여전히 코드로 정의한 interval 값은 180입니다.그러므로 코드로 정의된 부분을 변경하지 않은 상황에서 terraform apply 명렁어를 치면, 현재 120의 interval 값이 180으로 원복 됩니다.☠️ TaintReplace를 설명하기 앞서, 기존 테라폼에는 taint라는 명령어가 있습니다. 문서에는 다음과 같이 기재되어 있습니다.  The terraform taint command informs Terraform that a particular object has become degraded or damaged. terraform taint 명령어는 특정 객체가 저하되거나 손상되었음을 Terraform에 알립니다.Taint 명령어는 인프라를 정의한 코드는 그대로인 상태에서 리소스만 교체할 경우에 사용합니다.저의 경우 스파이크성 트래픽을 갑자기 받아 로드밸런서의 성능이 저하되었을 때, ELB를 교체한 경험이 있습니다.이런 상황에서 성능이 저하된 혹은 교체가 필요한 리소스 객체만을 on/off 방식으로 주석 처리 및 해제하며 apply 명령어로 교체할 수 있지만,taint(교체 리소스를 마킹) 명령어로 교체하고 untaint(교체 리소스 마킹 해제) 명령어로 교체할 필요가 없다고 명령할 수 있습니다.그러나 위와 같은 워크플로우는 테라폼 v0.15.2.에서 deprecate 되었고, -replace 옵션을 제공하며 더 직관적인 사용자 경험을 제공하게 됩니다.🌗 -replace-replace 옵션은 taint 명령어와 동일하게 작용하며, untaint 명령어를 칠 필요가 없습니다.코드는 그대로지만 리소스 객체가 변경되는 Replace 명령어 사용법은 다음과 같습니다.$ terraform apply -replace=\"aws_instance.example[0]\"교체할 리소스 인자값를 찾기 위해, terraform state list 명령어로 target을 확인할 수 있습니다.💡 -replace 옵션 역시, --refresh-only 옵션과 동일하게 plan 명령어와 함께 적용하여 변경 지점을 미리 파악하고변경되는 리소스에 대한 검토를 하는 습관을 들입시다!OutroRefresh와 Replace 비슷하면서도 전혀 다른 두 명령어의 변천사를 확인하며 인프라의 동일한 형상을 유지하기 위한 방법을 알아보았습니다.이번 포스팅에서 언급한 방법 외에도 형상을 일치시키는 방법들이 존재하지만, 위 2가지 명령어만 제대로 활용하면 대부분의 인프라의 형상이 달라지는 사태를 예방할 수 있습니다. IaC에서는 항상 형상을 변경하기 전, 변경되는 리소스에 대한 검토를 하는 습관이 매우 중요하다는 것을 강조하며 이번 3편을 마칩니다.지금까지 테라폼 더 익숙하게 Refresh &amp; Replace 편을 읽어주셔서 감사합니다! 잘못된 내용은 지적해 주세요! 😃Terraform Tips    Module &amp; Output    Data &amp; Index    Refresh &amp; Replace    Move (Refactoring)    Import",
        "url": "/TerraformTips3"
    }
    ,
    
    "geultto": {
        "title": "글또 7기 다짐글",
            "author": "HeuristicWave",
            "category": "",
            "content": "글또 7기 다짐글 입니다.글또를 시작하며… 🏃🏻저는 최근 ‘글 쓰는 개발자 모임, 글또’ 7기로 활동할 수 있는 기회를 얻었습니다. 글또가 어떤 활동인지는 글또 페이지에서 확인할 수 있습니다.글또의 존재는 2기 때부터 알고 있었지만, 스스로 활동을 하기에는 아직 부족하다 생각되어지난 1년간 한 달에 글 1편씩을 작성하는 습관을 만들고 나서 7기에 가입하게 되었습니다.활동은 올해 5월부터 10월까지 약 6개월간 활동하게 되는데요, 첫 미션으로 다짐글 작성이라는 주제를 받았습니다.그 동안 이 기술 블로그에 개인적인 얘기는 담지 않을뿐더러,평상시에도 일기와 같은 자신의 마음(다짐 포함)을 적어 본 적이 없어 글을 써 내려가기가 부끄럽네요.🧩 계획계획하고 있는 콘텐츠는 다음과 같습니다.      기술 도서 리뷰 현재 제 책상 위에는 구입해두고 읽지 못한 책들이 꽤나 쌓였습니다.‘컨테이너 보안’, ‘Go 언어를 활용한 네트워크 프로그래밍’ 등 밀린 기술 서적 부채(?)를 청산하도록 노력하겠습니다.        기존 시리즈물 마감 작년 제 블로그에 ‘테라폼 더 익숙하게’ 라는 시리즈물을 연재하기로 하고 게으른 탓 2편 밖에 작성하지 못했습니다.머릿속에만 남아있는 후속 글 들을 작성해 시리즈물 다운 글을 작성해 보도록 하겠습니다.     아직 2편 밖에 없는 작고 초라한 나의 시리즈물        관심 기술 스터디 Kubernetes, Terraform, Istio, AWS 서비스들에 대한 글을 작성하며, 성장의 기록들을 남기겠습니다.        주제를 추천받아 작성 저는 어떻게 보면 글을 작성하는 것보다, 주제를 선정하는 게 더욱 어려운 것 같습니다.글또를 통해 비슷한 직군의 엔지니어 분들이 고민하는 혹은 알고 싶은 주제들이 무엇인지 파악해 관련 글을 작성하고 싶습니다.  ✋ 다짐자발적 번아웃 🔥글또에는 여러 기수를 걸쳐 지속적으로 활동하고 계신 분들이 꽤나 많습니다.저도 이번 7기가 끝이 아니라 지속적인 활동으로 글또를 이어가고 싶습니다.그렇지만, 7기 활동이 끝날 무렵 회고를 하는 시점에서 7기 활동 간 생산된 12편의 글로 인하여 후회 없는 활동을 하고 싶습니다.다시 바꾸어 말하면, “너무 힘들어서 8기는 쉬어야겠다.” 싶을 정도의 감정을 느끼도록 열심히 활동하고 싶습니다.부끄러움을 늦추는 글의 유효기간 🙈이 페이지를 보면 제가 한 해 동안, 작성한 글들이 나옵니다. 당시에도 한편 한편 공들여 작성하며 “이정도면 꽤나 괜찮은 글 아닐까?” 라는 생각을 종종했었는데,시간이 지난 시점에 다시 읽어보면 부끄럽다는 생각이 들때가 있습니다.온라인 어디선가 본 내용인데, 좋은 글을 작성한 것으로 평가 받는 작가들도 과거 자신이 작성한 글을 보면 부끄럽다고 합니다.이런 것들을 보면 자신의 글을 부끄러워 하는게 당연한 것일 수도 있는데, 다음과 같은 생각이 들었습니다.위 그래프에서 보이다시피, 글의 완성도와 부끄러움의 발현 시기는 양의 상관관계를 갖고 있다고 생각합니다.작년에 제가 작성한 글 들의 경우, 아무리 길어도 대략 한 계절정도 지나면 부끄러움이 스멀스멀 올라오는것 같더군요.그래서, 이번 활동 기간 동안에는 과거 제가 썻던 글보다 더 부끄러움이 오는 시기가 늦는 글을 작성해 보려합니다.활동기간이 약 6개월 정도되니 아마 5월 말에 쓰는 글에 대한 부끄러움의 정도를 7기 활동이 끝날 무렵인 회고 때 다뤄보면 좋을 것 같습니다.",
        "url": "/geultto"
    }
    ,
    
    "export-data-to-s3-lambda": {
        "title": "Export cloudwatch log data to Amazon S3 using lambda",
            "author": "HeuristicWave",
            "category": "",
            "content": "본 글은 Cloudwatch Logs groups의 데이터를 Lambda를 사용해 주기적으로 S3로 export 하는 방법을 다룹니다.Intro글을 시작하기 앞서, Shraddha Paghdar - Export AWS CloudWatch logs to S3 using lambda functions in Node.js를 참고하여 해당 글을 작성했음을 알립니다. 본격적으로 방법을 소개하기 앞서, Cloudwatch Logs groups의 데이터를 Lambda를 사용해 주기적으로 S3로 export 하기 위한 플로우는 다음과 같습니다.  Amazon EventBridge에 정의한 Rule에 의해 Lambda를 호출합니다.  이후 람다가 지정한 CloudWatch의 Logs에 적재된 데이터를 찾습니다.  2번에 연속되어 이관될 대상의 로그들이 S3로 이관됩니다.📜 Workshop1️⃣ Log를 담을 Bucket 및 정책 생성하기Amazon S3 콘솔 화면에 접속해 Log들이 담길 Bucket을 생성하고 이어서 Permissions에서 S3 버킷에 대한 권한을 설정합니다.해당 방법은 공식 문서, 1단계와 3단계에 자세히 설명되어 있습니다.  💡 공식 문서 3단계에서 기재된 random-string은 필요에 의한 경우 사용하세요. 해당 글에서는 편의상 생략하였습니다.2️⃣ Lambda 생성하기Step 1 : Lambda가 사용하는 IAM Role &amp; Policy 생성Lambda가 Log를 Export 할 수 있도록 다음 정책을 생성합니다.IAM에서 Create policy를 선택하고 아래 JSON을 복사하여 붙여 넣고, cloudwatch_export_task라는 이름으로 정책을 생성합니다.{    \"Version\": \"2012-10-17\",    \"Statement\": [        {            \"Effect\": \"Allow\",            \"Action\": \"logs:CreateExportTask\",            \"Resource\": \"arn:aws:logs:{Region}:{AccountNumber}:*\"        }    ]}Lambda에 권한을 부여해 주기 위해서 IAM &gt; Roles &gt; Create Role을 선택합니다.Use case로 Lambda를 선택하고 앞서 생성한 정책을 부여한 뒤, export_S3_lambda라는 이름으로 Role을 생성합니다.Step 2 : 코드 작성Lambda 콘솔 화면에서 아래와 같이 빈칸을 채우고, Step 1에서 만들어둔 role을 부여해 람다 함수를 생성합니다.  💡 만약, Export 역할을 수행하는 람다 함수가 생성하는 로그를 수집하고 싶을 경우에는 Create a new role with basic Lambda permissions을선택하고 Console에 의해 자동적으로 생성되는 Role에 Step 1에서 만든 정책을 부여하면 됩니다.이어서 아래 코드를 복사하여 상황에 맞는 인자 값을 넣어주고 Deploy 합니다.Parameter  region : 로그 그룹과 대상 버킷은 동일 리전에 위치  destination : 로그가 이관되는 대상 버킷  logGroupName : Cloudwatch Log group 이름  destinationPrefix : 1️⃣ 에서 언급한 random-string 값  from/to : Lambda 함수가 호출 되는 시점을 기준으로, from/to 기간의 로그 그룹들을 exportconst AWS = require('aws-sdk')const cloudconfig = {  apiVersion: '2014-03-28',  region: 'region',}const cloudwatchlogs = new AWS.CloudWatchLogs(cloudconfig)exports.handler =  async (event, context) =&gt; {   const params = {      destination: 'bucket-name',      logGroupName: 'log-groups-name',      destinationPrefix: '',      from: new Date().getTime() - 86400,      to: new Date().getTime(),  };await cloudwatchlogs.createExportTask(params).promise().then((data) =&gt; {    console.log(data)    return ({      statusCode: 200,        body: data,    });  }).catch((err) =&gt; {    console.error(err)    return ({      statusCode: 501,        body: err,    });  });}이후 테스트 버튼을 눌러 결과값을 보면 taskId 값이 생성되고 S3에 로그가 이관된 것을 확인할 수 있습니다.3️⃣ EventBridge 트리거 생성하기작성한 람다 함수 콘솔 화면 상단에서 Add trigger 버튼을 눌러 다음과 같이 Rule을 생성합니다. 저는 주기를 점검하기 위해 아래와 같이 5분을 주었습니다.  💡 스케쥴 표현식 작성법Outro지금까지 Lambda 함수와 EventBrdige를 사용하여 자동으로 로그를 S3으로 백업하는 방법을 알아보았습니다.해당 방법 외에도 로그를 이관하는 다양한 방법들이 있으므로, 더 쉽고 좋은 방법이 있다면 알려주세요!소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해 주세요! 😃",
        "url": "/Export_data_to_S3_Lambda"
    }
    ,
    
    "ecs-cw-logs": {
        "title": "Using the awslogs log driver in ECS(Fargate)",
            "author": "HeuristicWave",
            "category": "",
            "content": "ECS Task의 컨테이너가 생산하는 로그들은 CloudWatch를 활용하여 수집할 수 있습니다.Cloudwatch Logs를 운영하며 로그 적재가 제대로 되지 않거나, Timestamp가 일치하지 않거나, 지나친 지연 시간이 발생하거나, 알아보기 어려운 형태로 로그가 쌓인다면 아래 요소들을 고민해 볼 수 있습니다.📚 References  Using the awslogs log driver  Regular expression WikipediaFargate에서 필요한 awslogs 로그 드라이버공식 문서에서는 awslogs-region, awslogs-group 만이 필요하다고 하지만, Fargate를 사용하는 경우 awslogs-stream-prefix이 추가적으로 필요합니다.또한, 가시성을 확보하기 위해 CloudWatch Logs에 수집된 여러 줄의 로그를 하나의 메시지로 보기 위해서는 awslogs-multiline-pattern이 필수적으로 필요합니다.awslogs-multiline-pattern공식 문서의 Note 부분을 보면 다음과 같은 메모를 확인할 수 있습니다. (정말 공식 문서는 한 줄도 그냥 지나칠 수 없는 것 같습니다!)  Multiline logging performs regular expression parsing and matching of all log messages.This may have a negative impact on logging performance.실제로 저는 정규 표현식을 간과하고 검증되지 않은 정규식들을 적용했다가 다음과 같은 Negative Impact를 만났습니다.  로그가 수집되기까지의 지나친 지연 시간 발생 (10분 이상)  지연시간으로 인한 Timestamp 불일치 (Ingestion time과 Event Timestamp의 과도한 오차)  1, 2번 이유로 인한 로그 미수집Regular Expression Lab지금부터 예시들을 통해, CW Logs를 운영하며 만날 수 있는 상황들을 체험해 보겠습니다.  샘플 로그를 복사하여 RegExr에서 match 여부를 테스트해 볼 수 있습니다.Case 1️⃣awslogs-multiline-pattern의 value로 ^INFO를 설정할 경우 3개의 Line 중 match 되는 라인은 몇 라인일까요?INFO | (pkg/trace/info/stats.go:104 in LogStats)                # Line 1INFO | (pkg/trace/info/stats.go:104 in LogStats)                # Line 212:15:10 UTC | INFO | (pkg/trace/info/stats.go:104 in LogStats) # Line 3  🖍 정답 보기      INFO | (pkg/trace/info/stats.go:104 in LogStats)                # Line 1    ^(caret) 은 전체 문자열의 시작 위치에만 일치하므로, Line 1 만이 match 됩니다.Case 2️⃣다음은 시:분:초를 표현하는 정규 표현식 입니다. 해당 정규 표현식은 아래 3줄을 모두 Match 시킬 수 있을까요? (0[1-9]|1[0-9]|2[0-4]):(0[1-9]|[1-5][0-9]):(0[1-9]|[1-5][0-9])07:36:35 | Which line will match? Line 108:00:01 | I am Line 2!08:01:00 | I am Line 3!  🖍 정답 보기      07:36:35 | I was matched 08:00:01 | I was not matched! 08:01:00 | I was not matched!     그렇다면 왜? 첫 번째 라인만이 매칭되었을까요? 분, 초에 해당하는 표현식을 유심히 살펴보면 00분 00시는 매칭되지 않습니다.때문에 각각 (분 : (0[0-9]|[1-5][0-9]), 초 : (0[0-9]|[1-5][0-9]))로 수정해야 위 3줄을 매칭 시킬 수 있습니다.Other Case위 2가지 케이스만 준비된다면 모든 로그들을 제대로 분리하여 수집할 수 있을까요?  🤔 생각해보기      Flag가 INFO 형식이 아닌 WARN이 발생할 경우    Timestamp로 매칭 작업을 하는데 한 줄에 1회 이상 Timestamp가 포함된 경우              ex) 08:00:01 | It’s 08:02:03 right now.              Application Crash로 인한 예상치 못한 메시지가 포함될 경우    awslogs 로그 드라이버 내의 우선순위  아마도 위에 기재한 것들 외에도 더 고려 할 것들이 많을 것 같습니다.개발이나 알고리즘 문제를 풀 때와 마찬가지로 항상 예상치 못한 실패 지점을 예상하는 습관이 필요한 것 같습니다.ECS Error Handling and Troubleshooting    Using Amazon ECS Exec for debugging    Using the awslogs log driver in ECS(Fargate)",
        "url": "/ECS_CW_Logs"
    }
    ,
    
    "ecs-exec": {
        "title": "Using Amazon ECS Exec for debugging",
            "author": "HeuristicWave",
            "category": "",
            "content": "Docker에서는 exec 명령어를 통해 실행중인 컨테이너에 접속하여 디버깅이 가능하다.21년 3월 부터 해당 기능이 AWS의 ECS에서도 사용가능하게 되었는데, 해당 기능을 사용하며 만났던 문제들을 기록.📚 References  Using Amazon ECS Exec for debugging  AWS CLI Cmd Ref : excute-commnadExec 활성화  SSM 에이전트와 SSM 서비스 간의 통신에 필요한 권한 부여  task-definition config 추가    \"linuxParameters\": {    \"initProcessEnabled\": true}        CLI로 execute-command enable 후, 점검    aws ecs create-service \\ --cluster cluster-name \\ --task-definition task-definition-name \\ --enable-execute-command \\ --service-name service-name \\ --desired-count 1        아래 명령어로 활성화 여부 확인. (grep option 활용, grep -F4 \"managedAgents\", grep \"enableExecuteCommand\")    aws ecs describe-tasks \\ --cluster cluster-name \\ --tasks task-id         활성화 상태일 때의 Snippet    {    \"tasks\": [        {            ...            \"containers\": [                {                    ...                    \"managedAgents\": [                        {                            \"lastStartedAt\": \"2021-03-01T14:49:44.574000-06:00\",                            \"name\": \"ExecuteCommandAgent\",                            \"lastStatus\": \"RUNNING\"                        }                    ]                }            ],            ...            \"enableExecuteCommand\": true,            ...        }    ]}        Running ECS Exec    aws ecs execute-command --cluster cluster-name \\ --task task-id \\ --container container-name \\ --interactive \\ --command \"/bin/sh\"      문제 해결Exec 명령어 이후 에러 핸들 (공식 문서들에 답이 다 있엇다 😂)An error occurred (ClusterNotFoundException) when calling the ExecuteCommand operation: Cluster not found.  Cluster ARN 기입 (From AWS CLI Ref : The Amazon Resource Name (ARN) or short name of the cluster from AWS CLI Ref)SessionManagerPlugin is not found. Please refer to SessionManager Documentation here: http://docs.aws.amazon.com/console/systems-manager/session-manager-plugin-not-found  클라이언트 PC에 SSM Plugin 설치 (From AWS Docs : Prerequisites for using ECS Exec)ECS Error Handling and Troubleshooting    Using Amazon ECS Exec for debugging    Using the awslogs log driver in ECS(Fargate)",
        "url": "/ECS_Exec"
    }
    ,
    
    "imagebuilder": {
        "title": "Create Immutable Server using AWS Image Builder &amp; Auto Scaling Group",
            "author": "HeuristicWave",
            "category": "",
            "content": "본 글은 2021년 4월 30일 EC2 Image Builder supports Auto Scaling launch template에소개된 기능을 활용해 Image Builder로 Golden AMI를 만들고,해당 이미지로 Auto Scaling Group의 launch template을 업데이트하는 방법을 소개합니다.Intro클라우드를 다루다 보면 Netflix 가 여러 분야에서 앞장서 있는 것을 확인할 수 있습니다.그중에서 Immutable Server Pattern을 알리는데 기여한 2013년에 발간된 AMI Creation with Aminator를꼭 한번 읽어보시기 바랍니다. 과거 넷플릭스는 AWS의 이미지 파일을 생성하기 위해 Aminator라는 도구를 사용했지만,본 포스팅에서는 이미지 빌더로 AMI를 만들고 Auto Scaling Group에 적용시키는 방법을 배워 Immutable Server Pattern을 학습해 보겠습니다.Immutable ServerImmutable Server 대한 내용은 martin fowler 블로그에 소개된 ImmutableServer글을 보시면 굉장히 잘 설명되어 있습니다. 요약 + 그동안의 제 뇌피셜을 함께 말씀드리면 다음과 같습니다.서버를 운영하다 보면 업데이트를 비롯한 다양한 이슈들로 인해 구성요소가 자주 변하게 됩니다.그렇게 운영하다 보면 무언가 꼬여 서버를 재 생성하는 일이 발생하는데 아래와 같은 라이프 사이클을 만나게 됩니다.이런 상황에서 구성이 다른 여러 대의 서버를 가지게 되면 Configuration drift가 발생했다고 표현합니다.또한 위 그림처럼 자꾸 새로운 이미지로 회귀하니까, Shift Left라고도 표현하고 이를 한국어로 표현하는 말을 찾아보니 원점 회귀라는 표현도 쓰입니다.구성 변경이 잦은 서버는 깨지기(fragile) 쉽고, 또 장애가 발생할 경우 재현이 어려울뿐더러 변경 시 연쇄적인 장애를 유발할 가능성이 있습니다.이를 두고 깨지기 쉬운 눈송이 같다 하여 Snowflake Server라 부릅니다.이 상황을 피하기 위해, Configuration drift를 피하는 Base Image(Configuration이 발생하면 새롭게 생성)를 활용한 방법을 Phoenix Server라 부릅니다. (Phoenix Server 용어는 마틴 파울러의 동료 Kornelis Sietsma가 제안했다 합니다.)피닉스 서버의 개념은 기본 이미지(Base AMI, Golden AMI)를 통해 언제나 일관성 있는 환경을 제공했고,이러한 개념들이 자연스럽게 Immutable Server의 개념으로 이어졌습니다.(최근에는 Immutable Server를 넘어 Immutable Infrastructure의 개념도 있습니다!)이미지 빌더와 오토 스케일 그룹으로 이뮤터블 서버 구축하기클라우드에서의 Immutable Server Pattern 검색하면 아래와 같은 좋은 예시들이 나옵니다.  Create immutable servers using EC2 Image Builder and AWS CodePipeline  Tutorial: Immutable infrastructure for Azure, using VSTS, Terraform, Packer and Ansible  Provision Infrastructure with Packer  하시코프 튜토리얼을 보고 만든 필자의 블로그 🥲그러나 이제부터 다룰, 이미지 빌더와 오토 스케일 그룹으로 이뮤터블 서버 패턴을 만족시킬 수 있습니다.AWS에서 제공하는 AutoScale은 탄력적인 확장과 축소를 제공하지만 Desired Capacity 기능을 활용해 항상 동일한 서버의 수를 유지할 수 있습니다.  Image Builder의 Image pipelines를 활용해 원본 AMI로부터 원하는 형태의 Output(Custom/Golden/Base AMI)을 제작합니다.  Distribution settings을 통해 lt(Launch Template)에 1번에서 생성한 AMI로 교체하여 새로운 버전을 만듭니다.  이제, 새롭게 생성되는 ASG(Auto Scaling Group)은 새롭게 버전 업된 lt를 통해 인스턴스를 생성합니다.3번의 ASG의 경우, Refresh를 하기 전까지는 이전 상태의 lt를 기준으로 인스턴스가 운영되고 있습니다.이미지 생성과 동시에 새로운 AMI로 EC2 Refresh를 할 수 있는 방법이 있지만,새롭게 생성된 인스턴스가 운영환경에 바로 적용되는 것은 바람직하지 않으므로 해당 부분에 대한 자동화는 제외하였습니다.만약 검증된 AMI를 생성한다면, Lambda 혹은 기타 방법 등을 통해 Refresh 하여 이미지 생성부터 배포까지 자동화할 수 있습니다.추후 다른 포스팅에서 자세히 다룰 예정이지만, 이러한 패턴은 Immutable Infrastructure의 한 요소를 이루기도 합니다.📜 Workshop0️⃣ Launch Template &amp; Auto Scaling groups 생성기존 환경에 0️⃣이 준비되어 있다면 다음 단계인 Distribution settings으로 넘어가도 좋습니다.해당 단계는 제가 굉장히 게으른 관계로 AWS Documentation : Creating an Auto Scaling group using a launch template로 대체하겠습니다.  💡 만약 기존 환경이 Launch Template이 아닌 Launch Configurations으로 구성되어 있을 경우,콘솔 화면에서 Copy to launch template 버튼을 누르시면 손쉽게 lt로 변경 가능합니다. 💡 Launch Template과 Launch Configurations는 굉장히 유사하지만, Launch Template의 경우 더 다양한 기능들을 제공합니다.특히 버전관리 기능을 통해 Rollback을 하거나, 업데이트시 ASG를 활용한 Rolling Update가 가능해 Launch Template 사용을 권장합니다.1️⃣ Distribution settingsEC2 Image Builder 콘솔 화면에 접속해 Distribution settings에서 새로운 세팅을 생성합니다.필수 항목인 이름을 작성하고 Region settings에서 배포할 리전을 확인한 후,하단의 Launch template configuration에서 Step 0️⃣에서 작성한 lt를 지정하고 create settings로 생성합니다.2️⃣ Image pipelinesStep 1Distribution settings 작성이 완료되었다면, 콘솔에서 Image pipelines에 접속하여 아래 정보들을 기재합니다.아래 사진의 좌측 Step 5까지의 과정을 거치면 목표로 했던 환경이 완성됩니다.Build schedule에서는 주기적으로 파이프라인을 실행할 수 있는 방법들을 제공하는데 운영자가 원하는 방식으로 설정할 수 있습니다.Step 2Choose recipe 단계에서는 기존에 만들어둔 recipe가 없으므로 Create new recipe를 선택하고, Image type으로 AMI를 선택합니다.다음 Base image를 고르는 단계에서는 아래 화면과 같이 관리형 이미지를 사용하거나 기존에 작성한 Custom AMI ID를 사용해도 됩니다.그다음 Instance configuration와 Working directory에서는 기본 값으로 둬도 상관없지만SSM, User data, Working directory path의 필요 여부에 따라 활용하시면 됩니다.이어서 Components에서는 Golden AMI를 구축하기 위해 선행되어야 하는 각종 Agent나 소프트웨어(Apache, dotnet etc)를 선택할 수 있습니다.저는 편의상 CloudWatch Agent를 선택했습니다.그다음 이어지는 Test components, Storage, Tags 역시 필요 여부에 따라 활용하시면 됩니다.Step 33단계에 진입하면 아래 사진과 같이 이미지 빌더의 인프라 Config 값들을 정의할 수 있는 공간이 나옵니다.3번째 버튼인 Create New infrastructure configuration으로 직접 인스턴스 유형, 네트워크, SNS topic을 설정이 가능하지만,저는 1번 Create infrastructure configuration using service defaults로 기본 구성 값들을 잡아주었습니다.(IAM Role과 SNS Topic도 자동으로 생성해 주고 굉장히 편리하네요.)Step 4드디어, 이전 1️⃣ Distribution settings 과정에서 만들어둔 배포 설정 해당 단계에서 선택합니다.Step 5Review 단계까지 구성 요소들을 검토해 보고 Create pipeline을 누르고 Image pipelines 콘솔로 돌아오면 아래와 같은 화면을 만나게 됩니다.아래 캡처화면의 경우 이미 빌드가 끝난 상태이지만, 테스트를 위해서 Actions - Run pipeline 단계를 거치면,ec2 콘솔에서 image build를 위한 builder 인스턴스가 생성되고 종료되는 것을 과정을 확인할 수 있습니다.3️⃣ Review  EC2 - Images - AMIs에서 새롭게 생성된 AMI 확인  EC2 - Launch templates에서 새롭게 버전이 올라간 lt 확인, 여기서 Versions 정보를 누르면 Image Builder가 생성한 Description을 확인할 수 있습니다.  제대로 작동하는지 확인하기 위해 기존의 EC2 하나를 종료시키면, 저의 경우 ASG의 Desired가 2로 설정했으므로 하나의 인스턴스가 새로운 버전으로 변경됩니다.이는 EC2 - Auto Scaling groups에서 확인할 수 있습니다.4️⃣ Clean Up분해는 조립의 역순으로?! 2️⃣ -&gt; 1️⃣ -&gt; 0️⃣ 역순으로 리소스를 정리하고 3️⃣ Review 항목을 점검하여 모든 리소스가 회수되었는지 확인합니다.Outro지금까지 각종 이야기 거리들과 EC2 Image Builder를 사용하며 Immutable Server Pattern을 학습해 보았습니다.과거 제가 Packer로 관련 환경을 구축한 적이 있는데, 역시 AWS 환경에서는 AWS의 서비스를 사용하는 게 연계도 용이하고 구축도 쉽네요.소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해 주세요! 😃📚 References글 중간중간 하이퍼 링크로 Reference 들을 달아두었습니다.특히, 이번 포스팅 레퍼런스들은 좋은 내용들이 많으니 원본들을 읽어보시고 당시 제가 느낀 기쁨들을 함께 느낄수 있으면 좋겠습니다.",
        "url": "/ImageBuilder"
    }
    ,
    
    "terraformtips2": {
        "title": "Terraform Tips 2 - Data &amp; Index",
            "author": "HeuristicWave",
            "category": "",
            "content": "Terraform 더 익숙하게 2 - Data &amp; Index AWS 리소스로 알아보는 terraform Data 활용 팁Intro이번 포스팅은 Tip이라 하기에는 부끄러운 사소한 지식이지만, 제가 자주 잊어버리는 내용이라 글로 남기게 되었습니다. Terraform Data를 잘 활용하면 디스크 이미지, 코드로 정의한 다양한 리소스 및 클라우드 공급자 API에서 가져온 정보들을 알 수 있습니다.모든 Data Sources가 동일한 방법으로 간편하게 조회할 수 있으면 좋겠지만, 막상 사용하려고 하면 이런 저런 문제들을 만나게 됩니다.공식문서(Tutorial : Query Data Sources)에서도 Data 활용방법을 배울 수 있지만,이번 포스팅에서는 3가지 예제와 함께 리소스를 Query 하는 방법을 배워 보겠습니다.Query AMIAWS의 최신 AMI는 주기적으로 갱신됩니다. 따라서 재사용 가능한 코드를 작성하기 위해, 항상 최신 AMI를 참조하는 코드를 작성하는데 다음과 같은 방법을 사용할 수 있습니다.data \"aws_ami\" \"amazon_linux\" {  most_recent = true  owners = [\"amazon\"]  filter {    name   = \"name\"    values = [\"amzn2-ami-kernel-*-hvm-*-x86_64-gp2\"]  }}output \"name\" {  value = data.aws_ami.amazon_linux.id}위와 같은 방법으로 filter와 owners 값을 조정하며 어떤 이미지든지 id 값(output)을 얻어 낼 수 있습니다.예를 들어 ECS의 Optimized AMI를 사용하는 경우, 다음과 같은 filter 값을 줄 수 있습니다.  values = [“amzn2-ami-ecs-hvm-*-x86_64-ebs”]ECS와 달리 EKS는 AMI 명명 규칙이 약간 달라 filter 기능을 활용해야, 조건에 맞는 Optimized 이미지를 얻을 수 있습니다.EKS 이미지의 경우 모든 이미지의 첫 문자열이 amazon-eks-으로 시작하기 때문에, *를 함께 넣어 Optimized AMI를 얻을 수 있습니다.data \"aws_ami\" \"amazon_linux_eks\" {  most_recent = true  owners = [\"amazon\"]  filter {    name   = \"name\"    values = [\"amazon-eks-*\"]  }  filter {    name   = \"architecture\"    values = [\"x86_64\"]  }}그러나 위 Query의 결과 값을 공식 문서에 기재된 AMI ID와 비교해 보면,x86 ID가 아닌, x86 가속 ID 값과 일치한 다는 것을 알 수 있을 겁니다.문서에 기재된 x86 가속은 gpu가 사용 가능한 Optimized AMI입니다. gpu를 사용하는 노드의 명명 규칙이 amazon-eks-gpu로 시작하기 때문에 위 filter 조건으로는 gpu 노드가 조회됩니다.그렇다면 일반 x86 노드는 어떻게 조회해야 할까요? 🧐 어느 문서에도 기재되어 있지 않지만,대략적인 명명 규칙을 유추하여 보니 일반 EKS 노드는 다음과 같은 필터를 사용해야 한다는 것을 알게 되었습니다.  values = [“amazon-eks-node-1.22-*”]  value 값의 prefix 규칙을 보니 EKS의 버전도 prefix 안에 포함되어, 이 값을 응용하면 다양한 버전의 EKS Optimized AMI를 얻을 수 있다는 것을 추측 할 수 있습니다.이외에도 리전마다 다른 Optimized AMI는 data에는 명시하지 않았지만, Provider에 명시한 리전에 종속성을 갖게 됩니다.또 하나의 팁을 드리자면, 만약 Filter에서 지원하지 않는 명명 규칙을 가진 AMI라면 정규식으로도 조회가 가능합니다!여기까지 제가 자주 사용하는 다양한 AMI ID를 조회하는 방법을 알아보았습니다. 이제 어떤 AMI라도 조회가 가능하겠죠? 😎Query AZAWS의 리전마다 사용가능한 az가 다르기 때문에, 조금 더 유연한 코드를 작성하기 위해 다음 코드를 사용해 사용가능한 az를 검색합니다.이후, data.&lt;NAME&gt;.&lt;ATTRIBUTE&gt;.names 로 사용가능한 az 값들을 확인할 수 있습니다.$ data \"aws_availability_zones\" \"available\" {  state = \"available\"}$ output \"azs\" { value = data.aws_availability_zones.available.names}names에는 사용가능한 az가 배열 형태로 들어가 있어, names[0], names[1]과 같이 Index 값으로 특정 값을 지정할 수 있습니다.그러나, 모든 data가 Index 값을 가지고 있는 것은 아닙니다.Query vpc_id다른 리소스와 AWS 솔루션들을 연계하기 위해서는 vpc_id 값이 필수적으로 들어갑니다.vpc의 id를 구하기 위해서는 다음과 같은 방법으로 id를 조회할 수 있습니다.(tags 값을 활용해 일종의 필터링을 사용할 수도 있습니다.)data \"aws_vpcs\" \"vpcs\" {    tags = {        Name = var.vpc_name    }}output \"vpc_id\" {    value = data.aws_vpcs.vpcs.ids}위 코드로 다음과 같이 Output 값을 얻을 수 있지만, 하나의 az 값을 얻을때와 동일한 방식으로 ids[0] 형식으로 값을 조회하려 하면,“This value does not have any indices.” 라는 에러 메시지와 함께 출력을 지원하지 않습니다.Changes to Outputs:  + vpc_id = [      + \"vpc-0x1234567890\",    ]도대체 무엇이 잘못된 것일까요? az와 동일한 방법으로 접근했지만, 왜 지원하지 않는지는 아직까지도 모르겠습니다…누구 아시는 분이 있다면 알려주세요.count로 index 부여하기위 문제를 해결하기 위해서는 az를 검색할 때보다는 불편하지만, count를 사용해 해결할 수 있습니다.data \"aws_vpc\" \"target\" {  count = length(data.aws_vpcs.vpcs.ids)  id    = tolist(data.aws_vpcs.vpcs.ids)[count.index]}# sample code using vpc_idresource \"aws_lb_target_group\" \"sample_resource\" {  count = length(data.aws_vpcs.vpcs.ids)  # Skip Config  vpc_id      = data.aws_vpc.target[count.index].id}aws_vpcs가 아닌 aws_vpc를 추가하고 index 를 부여하기 위한 내장 함수들을 사용해 index를 부여합니다.이후, 리소스에서 data 값들을 식별하기 위한 count를 기입하고 위와 같이 index 값으로 조회가 가능합니다.Outro지금까지 Data를 활용해 각종 리소스들을 검색하는 방법을 알아 보았습니다.vpc_id도 az와 같이 별도의 index 과정 없이,간편한 조회가 가능하면 좋겠습니다. (제가 아직 방법을 모르는 것일 수도 있어요!)지금까지 테라폼 더 익숙하게 Data &amp; Index 편을 읽어주셔서 감사합니다! 잘못된 내용은 지적해 주세요! 😃Terraform Tips    Module &amp; Output    Data &amp; Index    Refresh &amp; Replace    Move (Refactoring)    Import",
        "url": "/TerraformTips2"
    }
    ,
    
    "terraformtips1": {
        "title": "Terraform Tips 1 - Module &amp; Output",
            "author": "HeuristicWave",
            "category": "",
            "content": "Terraform 더 익숙하게 1 - Module &amp; Output AWS 리소스로 알아보는 terraform Module 연계 팁IntroTerraform Module을 잘 활용하면 본인이 원하는 대로 인프라를 레고처럼 조립할 수 있습니다.레고처럼 인프라를 조립하기 위해서는 모듈 간의 연계가 중요한데요,이번 포스팅에서는 모듈 내에 작성된 Output value들을 활용하는 방법을 알아보겠습니다.해당 포스팅은 Output에 대한 이해가 있다는 가정하에 기술하였으므로, Output에 대한 설명이 필요하시면 아래 링크들을 참고해 주세요!🥲 사실, 아래 문서보다 더 잘 설명할 자신이 없어서… 언제나 가장 좋은 교재는 공식 문서입니다!  Terraform Docs  Tutorial : Output Data from TerraformOutput Command OptionOutput은 terraform apply 명령어를 수행하고 난 후, 맨 마지막에 Value 들이 렌더링 되어 나옵니다.그러나 테라폼 코드를 작성하는 중간중간 인프라의 value 값들이 필요할 때가 있습니다.그럴 때, output이 정의된 상황에서 terraform output {label} 명령어로 Ouput 값을 확인합니다.-raw 옵션을 함께 주면 문자열 형태가 아닌 raw한 텍스트 값만을 얻을 수 있습니다.$ terraform output -raw security_group_idAccessing Child Module Outputs하위 모듈의 아웃풋에 접근할 경우도 종종 있는데요, 이때는 module.&lt;모듈 명&gt;.&lt;Output 명&gt; 이런 형식으로 조회가 가능합니다.하위 모듈 출력값에 접근하는 것이, 모듈과 모듈은 연계하는 방법이기에 아래 예시에서 알아보겠습니다.module \"vpc\" {  source = \"terraform-aws-modules/vpc/aws\"  name   = \"sample_vpc\"  cidr = \"10.10.0.0/16\"  azs            = [\"us-west-2a\"]  public_subnets = [\"10.10.1.0/24\"]  tags = {    Owner       = \"me\"    Environment = \"stage\"  }}위와 같은 vpc 모듈은 security group 모듈과 거의 단짝 수준으로 함께 움직이는데요,security group 모듈을 활용하려면 다음과 같이 vpc_id 값이 필요합니다.이 경우, vpc를 먼저 생성하고 vpc_id 값을 알아내어 사용할 수 있지만, 다음과 같은 방법으로 모듈을 연계합니다.module \"security_group\" {  source      = \"terraform-aws-modules/security-group/aws\"  name        = \"ssh\"  description = \"ssh from workstation\"  vpc_id      = module.vpc.vpc_id  ingress_cidr_blocks = [\"0.0.0.0/0\"]  ingress_rules       = [\"ssh-tcp\"]}Find Module Output Label방금 전, 모듈을 연계하는 방법을 배워 보았습니다. 그런데, 모듈을 연계하기 위해서는 미리 사전에 작성된 모듈의 Output Label을 알아야 합니다.우선 에디터의 Explorer 탭에서 .terraform 폴더를 열어봅시다.apply를 적용한 security-group, vpc 모듈이 내 로컬 머신에 숨어 있습니다.해당 모듈 폴더 안에 들어가면 outputs.tf 가 정의되어 있으므로 해당 파일을 참고하여 Label 값을 얻어오면 됩니다!Example상황 : VPC 모듈을 사용해 VPC와 서브넷을 구축한 상황에서, EFS를 각 서브넷에서 사용하려고 합니다.EFS에서 모듈로 생성한 Subnets을 어떻게 참조할까요?  🖍 정답 보기  위에서 언급한대로 modules/vpc/output.tf 에서 프라이빗 서브넷의 Label을 확인해보면,private_subnets 이라 기재된 것을 확인할 수 있습니다. 이를 활용해 아래와 같이 모듈을 참조 할 수 있습니다.  resource \"aws_efs_mount_target\" \"mount\" {  count = length(module.vpc.private_subnets)  file_system_id = aws_efs_file_system.foo.id  subnet_id      = module.vpc.private_subnets[count.index]}  Outro이렇게 Module의 Output 값을 활용하는 방법을 알게 되니, 테라폼 모듈 조립에 대한 자신감이 생겼습니다. 앞으로도 테라폼에 더 익숙하지기 위한 방법들을 시리즈로 연재할 계획인데, 언제 끝날지 모르겠습니다. 😑지금까지 테라폼 더 익숙하게 Module &amp; Output 편을 읽어주셔서 감사합니다! 잘못된 내용은 지적해 주세요! 😃Terraform Tips    Module &amp; Output    Data &amp; Index    Refresh &amp; Replace    Move (Refactoring)    Import",
        "url": "/TerraformTips1"
    }
    ,
    
    "constructhub": {
        "title": "AWS CDK Library, Construct Hub",
            "author": "HeuristicWave",
            "category": "",
            "content": "본 글은 지난 AWS re:Invent 2021에서 발표된 Construct Hub를 둘러보며 느낀 첫인상에 대한 글입니다.IntroAWS re:Invent 2021, 아마존 CTO인 Dr. Werner Vogels 키노트에서AWS CDK 라이브러리들을 활용할 수 있는 Construct Hub가 발표되었습니다.보통 AWS 리인벤트에서는 AWS 고유의 서비스들이 소개되는데요,이번에 말씀드릴 Construct Hub는 AWS의 솔루션이 아닌 오픈 소스 커뮤니티이자,CDK 라이브러리를 검색하고 공유할 수 있는 장이라고 할 수 있습니다.Construct Hub를 통해 AWS가 그리는 IaC(Infrastructure as Code)의 미래를 함께 만나보겠습니다.우선, Construct Hub를 소개하기 앞서 AWS의 IaC 도구들에 대하여 간략하게 알아보겠습니다.️📂 AWS CloudFormation2011년 AWS의 리소스를 JSON 또는 YAML 형식으로 인프라를 템플릿화 할 수 있는 CloudFormation을 발표했습니다.  예) AWS S3 버킷 리소스를 제어하는 CloudFormation 코드  Resources:  HelloBucket:    Type: AWS::S3::Bucket    Properties:      AccessControl: PublicRead      WebsiteConfiguration:        IndexDocument: index.html        ErrorDocument: error.html그러나, 인프라가 복잡해질수록 CloudFormation 코드의 길이도 길어지고 리소스 간의 관계도 파악하기 어려워 관리의 피로도가 증가하게 되었습니다.AWS는 이러한 CloudFormation의 약점을 보완하고 개발자들이 YAML 형식으로 인프라를 정의하는 게 아니라,선호하는 프로그래밍 언어로 클라우드 인프라를 정의할 수 있도록 Cloud Development Kit(CDK) 를 2019년에 정식으로 출시했습니다.⌨️ AWS Cloud Development KitCDK는 익숙한 프로그래밍 언어를 사용해 클라우드 애플리케이션 리소스를 정의할 수 있는 오픈 소스 소프트웨어 개발 프레임워크입니다.CDK는 어떤 방식으로, 앞서 언급된 CloudFormation의 약점들을 보완하는지 그림과 함께 알아보겠습니다.CDK는 Construct의 집합체라 말할 수 있습니다.Construct는 클라우드 서비스를 이루는 컴포넌트라 생각하셔도 좋습니다.CDK 프레임워크를 통해 개발자 혹은 클라우드 운영자는 타입스크립트, 파이썬, 닷넷, 자바 등의 익숙한 프로그래밍 언어 중하나를 선택하여, construct 라이브러리를 사용해 프로그래밍 합니다.하나의 Stack으로 엮어진 소스코드를 cdk synth 라는 명령어로 CloudFormation에서 사용되는 템플릿으로 변환하고 cdk deploy 명령어로 인프라를 배포합니다.CDK로 작성된 인프라 코드는 프로그래밍 언어의 이점을 그대로 채택하여,해당 언어에 익숙한 사람이라면 YAML로 작성된 CloudFormation 보다 인프라의 환경을 파악하기 쉽습니다.실제로 올해 키노트에 언급된 Liberty Mutual INSURANCE사의 CDK 도입 사례에서1500라인의 CloudFormation 코드를 CDK에서 단 14줄로 구현하며 CDK의 뛰어난 가시성을 알렸습니다.🗂 Construct Hub대망의 Construct Hub를 소개하기 위해 먼 길을 돌아왔습니다.😓맨 처음에 Construct Hub를 오픈 소스 커뮤니티이자, CDK 라이브러리를 검색하고 공유할 수 있는 장이라고 소개한 말이 맞는지 그림과 함께 확인해 보겠습니다.Construct Hub의 메인 홈페이지를 확인해 보면 CDK 라이브러리를 검색할 수 있는‘서치 바’와 현재 ‘지원하는 언어’와 ‘서비스 프로바이더’(AWS, Datadog, Mongo DB, Aqua Security 등)가 보입니다.이 중에서도 화면 왼쪽에 위치한 다양한 CDK들의 종류에 대해 궁금증이 생기실 것 같습니다.앞서 소개한 AWS CDK는 프로그래밍 언어로 작성한 인프라 코드를 Cloudformation으로 템플릿을 생성했습니다.이처럼 CDK가 생성하는 템플릿이 AWS Cloudformation으로 활용 가능하도록 하는 것을 AWS CDK,쿠버네티스로 활용 가능하도록 하는 것을 CDK8s, 테라폼으로 활용 가능하도록 하는 CDKtf라고 합니다.위와 같이 현재 3가지 Type을 지원하고 있으며, 향후 다른 도구들도 지원할 가능성이 있다고 합니다.다음으로는 Construct Hub에 등록된 다양한 Construct 검색 결과입니다. 현재는 대부분 Construct는 Hahicorp, Datadog과 같은 클라우드 서비스 Publisher 들이 참여했지만,개인도 JSII (CDK가 다중 언어 라이브러리를 제공할 수 있도록 하는 기술)기반의 construct를 만들고 aws-cdk, cdk8s, cdktf 등의 키워드와 함께 npm 레지스트리에 공개되어 있다면 약 30분 내에 Construct Hub에도 개시된다고 합니다.화면에 보이는 특이점으로는 HashiCorp가 제공하는 CDKtf를 통해,다른 클라우드 서비스들과 통합하여 AWS 서비스 만이 아닌 모든 클라우드를 CDK로 제어 가능하게 하려는 큰 그림을 그려나가고 있다는 것을 알 수 있습니다.Outro지금까지 Construct Hub를 간단하게 살펴보며, AWS IaC 도구들의 변천사와 AWS가 그리는 IaC의 미래를 엿볼 수 있었습니다.그중에서도 흥미로웠던 요소들은 다음 2가지로 말씀드릴 수 있습니다.  IaC를 도입한 조직의 경우, Terraform, Ansible 등 여러 IaC 도구를 각각의 IaC 도구들의 특성에 맞게 복합적으로 운영합니다. CDK가 다방면으로 IaC 도구를 지원(AWS CDK, CDK8s, CDKtf) 하게 만들어 관리 복잡도를 줄이려는 노력이 흥미롭습니다.  CDKtf를 통해, 타 클라우드 서비스를 CDK로 제어하게 된다는 점이 흥미롭습니다.사실 CDK8s, CDKtf 모두 Construct Hub가 나오기 이전부터 존재했지만, Construct Hub에 개시된 문서들을 보니 더 흥미롭게 다가옵니다.아직 세상에 알려진지 얼마 되지 않은 Construct Hub를 활용하기에는 어려움이 있지만, 누구나 Construct Hub에 기여할 수 있는 오픈소스 생태계를 구축한 만큼 빠른 성장이 기대됩니다. Construct Hub가 기여할 IaC 미래에 긍정적인 기대를 걸어봅니다.소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해주세요! 😃📚 References  AWS Construct Hub availability  Construct Hub",
        "url": "/ConstructHub"
    }
    ,
    
    "karpenter": {
        "title": "My first impression of AWS Karpenter",
            "author": "HeuristicWave",
            "category": "",
            "content": "본 글은 지난 11월 29일에 GA된 Karpenter를 가볍게 사용해 보며 느낀 첫인상에 대한 글입니다.IntroEKS 환경에서 더 빠르고 효율적인 Kubernetes Cluster Autoscaler Karpenter가 GA 되었습니다.오픈소스인 Karpenter는 모든 클라우드 벤더와의 통합도 목표하지만, 아직 타 클라우드와의 통합은 진행 중이라 v0.5으로 공개되었다고 합니다.즉 GA 된 v0.5만큼은 AWS 리소스와 완벽하게 통합되었기에 이번 업데이트에 공개되었다고 할 수 있습니다.사실 기존 EKS 환경에서도 EC2의 Autoscaling을 활용한 CA를 지원하였는데, Karpenter 출시가 어떤 영향을 미치게 될 것인지 알아보겠습니다.Kubernetes Autoscaling쿠버네티스에서는 다음과 같은 3가지 Autoscaling 방법이 있습니다.  HPA(Horizontal Pod Autoscaler)  VPA(Vertical Pod Autoscaler)  CA(Cluster Autoscaler)Pod Scaling의 경우 CPU 사용량, 메트릭 등을 관찰하여 스케일링하지만, EKS 클러스터 자체의 자원이 모자라는 경우 CA를 고려해야 합니다.⚙️ Cluster Autoscaler with EC2 Auto Scaling기존 EKS는 다음과 같이 EC2의 Autoscaler 기능을 활용해 탄력적인 환경을 구성했습니다.hpa와 vpa가 eks에서 내에서 scaling을 진행하는 것과는 달리,worker node를 확보하기 위해 EC2의 Auto Scaling Group을 사용하며 k8s와 ec2 별도의 Layer를 관리해야 하는 운영 복잡도가 발생하게 되었습니다.🪓 Cluster Autoscaler with Karpenter반면 Karpenter의 경우 k8s의 native method를 확장한 개념이기 때문에, 기존의 CA 방법과는 달리 효율적인 환경을 제공할 수 있습니다.(실제로 카펜터를 운영해 보면 기존의 Auto Scaling Group을 사용하지 않는 것은 아니지만,사용자 입장에서는 고려하지 않아도 되니 k8s layer에서만 관리된다고 해도 틀린 말은 아닌 것 같습니다.)karpenter.sh의 홈 화면을 보면 간단한 동작 원리를 설명하는 그림이 있습니다.karpenter가 unscheduled pods를 관찰하고 있다가 즉시(just-in-time) 최적화된 capacity에 pods를 배포합니다.Overprovisioning과거 EC2의 스케일링을 사용하는 CA에 대한 공식 문서에서, 다음과 같은 고려 사항을 확인할 수 있습니다.  노드를 확장하기 전에 노드가 확장될 때까지 기다려야 하므로 배포 대기 시간에 큰 영향을 미칩니다.노드를 사용할 수 있게 되려면 몇 분 정도 걸릴 수 있으며, 이로 인해 포드 예약 지연 시간이 크게 늘어날 수 있습니다.예약 대기 시간이 늘어나는 것을 감수하고 오버프로비저닝을 사용하여 이를 완화할 수 있습니다.그러나 이제 karpenter를 도입한다면 1분 이내 최적화된 인스턴스를 바로 프로비저닝 할 수 있으므로,더 이상 미리 프로비저닝을 할 필요도 없고 워커 노드의 크기 조정도 고려하지 않아도 됩니다.어떻게 오버프로비저닝을 방지하고 컨테이너와 클라우드 환경의 이점을 더 누릴 수 있게 해주는지 제가 진행해본 테스트와 함께 알아보겠습니다.👀 LabTMI : 이 글을 보는 시점에는 수정되어 있을 수도 있겠습니다만,공식 문서에 기재된 Default Region과 Module의 azs Config 값이 통일되지 않았습니다. 수행 시, 참고하시기 바랍니다.공식 문서 Terraform으로 시작하기의 가이드대로Terraform 코드를 실행시키면 EKS 내에 다음과 같은 karpenter-controller와 karpenter-webhook 포드가 올라온 것을 확인할 수 있습니다.우선, Karpenter가 정말 최적화된 capacity를 제공하는지 확인하기 위해 t3a 시리즈의 스펙을 첨부합니다.Test 1 : t3a.medium 인스턴스에 1cpu를 요구하는 5개의 pod 배포문서에서 제공하는 inflate manifest를 활용해 t3a.medium 인스턴스에 1cpu를 요구하는 5개의 pod를 배포하면,다음과 같이 t3a.2xlarge 인스턴스가 즉시 프로비저닝 됩니다.(1분 이내라고 소개되지만, 체감상 1분 보다 더 빠른 시간 안에 프로비저닝 되는 것 같습니다.)새롭게 생성된 t3a.2xlarge 노드를 확인하면 다음과 같이 5개의 pod가 배치된 것을 볼 수 있습니다.아키텍처로 보면 다음과 같습니다. 기존 t3a.medium에는 기본으로 있는 pod들 때문에 1cpu 조차 할당할 수 없습니다.inflate는 5cpu를 요구하므로, 이를 수용할 수 있는 t3a.2xlarge 인스턴스를 프로비저닝하고 pod들을 배치시켰습니다.요청 리소스를 기반으로 최적의 인스턴스를 할당한 것을 확인할 수 있었습니다.Test 2 : Test1환경에서 0.5cpu를 요구하는 5개의 pod 배포Test 1에서 Scalue out(worker node 1대 =&gt; 2대) &amp; Scale up(t3a.medium =&gt; t3a.2xlarge)를 동시에 경험해 봤다면, 이번에는 다음과 같이 필요한 리소스만 0.5 cpu로 줄여보겠습니다.기존 t3a.2xlarge 인스턴스가 사라지고, t3a.xlarge 인스턴스가 즉시 프로비저닝 되었습니다.⬆️ a minute ago에서 ⬇️ 2minutes ago로 변하는 것을 보니 정말 1분 이내로 동작하는 것 같습니다.아키텍처로 보면 다음과 같습니다.이번에는 t3a.medium에 0.5 cpu만큼의 capacity가 남아있으므로 1개의 inflate pod가 배포되었고,t3x.xlarge 나머지 4개의 inflate pod가 배포되었습니다.t3a.large(2cpu) &lt; inflate(0.5cpu * 4) + kube-proxy + aws-node &lt; t3a.xlarge(4cpu)정말 빠른 시간 내에 최적의 capacity를 할당하는 모습을 보니 유연하고 높은 성능을 제공한다는 소개가 맞는 것 같습니다.저는 위 실험에서 인스턴스에 관한 별도의 CRD 값들을 지정하지 않아 karpenter가 t시리즈 인스턴스들을 프로비저닝 하였지만,운영에서 Karpenter를 사용하기 위해서는 Provisioner API를 읽고 세밀한 manifest 값들을 조정해 주어야 합니다.Outro과거 AWS의 CA는 스케일링에 걸리는 시간도 상당할뿐더러,제한적인 스케일링으로 인해 리소스가 낭비되거나 운영환경에서 다운타임을 최소화하기 위해 오버프로비저닝 되는 경우도 많았습니다.기존의 방법보다 더 Kubernetes native 한 karpenter를 도입한다면, 아래 그림과 같은 효과를 기대할 수 있습니다.Karpenter의 빠른 프로비저닝과 유연한 스케일링 덕분에 클라우드를 더 클라우드답게 사용할 수 있게 된 만큼가까운 미래에 Karpenter가 기존의 CA를 대체할 것으로 예상됩니다.지금까지 아주 간단하게 Karpenter를 사용해 본 후기를 작성해 보았습니다.추후, Karpenter의 자세한 동작 원리와 제약 사항 혹은 더 많은 기능들에 대하여 다뤄보겠습니다.소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해주세요! 😃📚 References  AWS Whats new, aws karpenter  Introducing Karpenter  Karpenter Document",
        "url": "/Karpenter"
    }
    ,
    
    "ec2-clocksource": {
        "title": "EC2 Clocksource",
            "author": "HeuristicWave",
            "category": "",
            "content": "몰라도 되지만 알면 알수록 더 신비한 EC2 🙃Preview이번 포스팅에서는 AWS Well-Architected Labs - Performance Efficiency에 개재된 Calculating differences in clock source를 읽고 궁금증이 생겨 구글링을 하다 알게 된 사실들을 의식의 흐름대로 작성한 포스팅입니다.Performance Efficiency Summary일단 Performance Efficiency에 나오는 실험 내용을 요약하자면 다음과 같습니다.AWS의 5세대 가상머신 Nitro와 non-nitro 인스턴스 2개를 올리고 시간을 반환하는 테스트 코드를 돌려 성능 테스트를 진행합니다.당연히 5세대 Nitro가 기존 세대보다 월등한 결과를 보여 주지만,non-nitro 기반의 인스턴스에서 ‘리눅스 클럭 소스를 교체하면 유의미한 성능 향상의 결과를 얻을 수 있다’ 라는 실험 결과를 보여줍니다.  Nitro 기반 인스턴스의 default clocksource : kvm-clock(권장)  Non-nitro 인스턴스의 default clocksource : xen  실험에서 교체한 Non-nitro 인스턴스의 clocksource : tsc마지막으로 첨부된 How do I manage the clock source for EC2 instances running Linux?게시물에서 클럭 소스를 교체하는 방법(xen에서 tsc로 교체)을 소개하며 실험 내용을 마칩니다.궁금한 건 못 참아 ❓위에 소개한 Lab을 진행하다 보니 ‘왜 tsc로 교체하여 성능 향상 효과를 얻을 수 있는지’ 알 수가 없었습니다.궁금증을 해소하기 위해 구글링을 하다 보니 이해를 돕는 다음 3가지 자료를 찾을 수 있었습니다.⏱ TimestampingRed Hat Reference Guide에서 어느 정도 제 가려운 부분을 긁어 주었던 포스팅이 있습니다.기본적으로 멀티프로세서 시스템인 NUMA와 SMP 아키텍처에서는 여러 개의 clock source가 탑재되어 있습니다.멀티프로세서 기반의 EC2 인스턴스에서도 아래 명령어로 사용 가능한 clocksource를 확인하면 다음과 같은 결과를 확인할 수 있습니다.cat /sys/devices/system/clocksource/clocksource0/available_clocksourcexen tsc hpet acpi_pmRed Hat의 실험 결과에 따르면 tsc &gt; hpet &gt; acpi_pm 순으로 오버헤드가 적은데,tsc는 register에서 hpet은 memory area에서 읽기 때문에 수십만 개의 타임스탬프를 지정할 때 상당한 성능 이점을 제공한다고 합니다.⚙️ Heap Engineering PostRunning a database on EC2? Your clock could be slowing you down을 보면 더 정확한 분석이 있습니다.내용이 어려워 저는 완벽하게 이해하지 못했지만, 읽어보시면 굉장히 좋은 자료인 것 같습니다.Heap Engineering 해당 포스팅에서 밀당을 시도하는데…‘tsc에서는 낮은 가능성으로 clock drift 현상이 있어 프로덕션에서는 수행하지 말라’ 고 했다가,실제로는 clock drfit가 발생하지 않는다며 AWS가 tsc를 권장했던 슬라이드 자료를 함께 보여줍니다.그냥 맘놓고 kvm-clock이 탑재된 인스턴스를 사용하는게 좋을 것 같습니다.🎥 Tudum~ 또! Netflix클라우드를 공부하다 보면 Netflix 가 클라우드에 지대한 영향을 끼친 것 같다고 느낄 때가 많은데, 이번에도 그랬습니다.AWS re:Invent 2014에서 Netflix의 Senior Performance Architect, Brendan Gregg의 발표 자료를 보면xen에서 tsc로 교체하여 CPU 사용량은 30%, 평균 앱 레이턴시는 43%가 줄었다고 합니다.Result이번에도 구글링으로 딴짓을 하다 보니 많은 사실들을 알게 되었습니다. 사실 Current generation instances를사용하면 대부분 위에서 언급한 최적화는 T2 시리즈, Gravition 계열을 제외한 대부분의 인스턴스에서는 기본적으로 적용되어 있습니다.그래서 포스팅의 첫 포문을 ‘몰라도 되지만 ~’이라 지었습니다.clocksource와는 별도로 이번 포스팅을 준비하다 거의 주말 하루를 소비했는데,비교적 최근의 인스턴스가 과거 인스턴스들과 어떻게 다른지(Hypervisor, Jumbo Frame 등등)를 알 수 있었습니다.새롭게 알게 된 사실들 역시 그냥 Nitro 기반의 Amazon Linux 2를 사용하면, 운영하는데 몰라도 지장 없이 최고의 성능을 보장받는 것 같습니다.아직 알음알음 아는 지식이라 포스팅하기 어렵지만, 훗날 더 정확히 알게 되면 성능과 관련된 다른 튜닝 요소들도 적어보겠습니다.소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해주세요! 😃📚 포스팅과 직접적인 연관도는 떨어지지만 함께 보면 좋은 자료  AWS EC2 Virtualization 2017: Introducing Nitro  Linux AMI virtualization types  Reinventing virtualization with the AWS Nitro System",
        "url": "/EC2_Clocksource"
    }
    ,
    
    "selfservice": {
        "title": "Self-service Infrastructure",
            "author": "HeuristicWave",
            "category": "",
            "content": "본 글은 Kief Morris의 Infrastructure as Code와 HashiCorp 백서를 읽고 학습한 내용을 기반으로 작성한 글 입니다.서문IaC(Infrastructure as Code)에 관심을 갖고 공부를 하다보면 Self-service Infra라는 말을 자주 만나게 됩니다.몇 개월째 와닿지 않는 개념이였지만, 최근 키프 모리스의 책을 다시 읽고 조금은 알게 된 거 같아 그동안 공부한 Self-service Infra에 대한 자료들을 바탕으로 작성해 보았습니다.Self-service Infra에 관한 배경지식을 넓히는 데 도움이 되었으면 좋겠습니다.Dynamic Infrastructure동적 인프라는 서버, 스토리지, 네트워크와 같은 인프라 자원을 관리할 수 있는 시스템을 말합니다.동적 인프라의 종류로는 Public/Private 클라우드, 오픈스택을 활용하는 사설 클라우드, 베어메탈 등이 있습니다.앞선 정의만 보면 동적 인프라는 클라우드와 굉장히 유사하지만, 키프 모리스는 동적 인프라가 클라우드보다 범위가 더 넓다고 합니다.📣 동적 인프라를 소개하며 알려드리고 싶은 문장이 있습니다.클라우드로의 전환이라는 의미에 대해 많은 사람들이 여러 측면에서 설명을 하지만,저는 HashiCorp의 Unlocking the Cloud Operating Model: Provisioning백서에 소개된 다음 표현에 참 공감이 갑니다.  클라우드로의 전환의 본질적인 의미는 “정적” 인프라에서 “동적” 인프라로의 전환입니다.  The essential implications of the transition to the cloud is the shift from “static” infrastructure to “dynamic” infrastructure동적 인프라 플랫폼 요구 사항동적 인프라 플랫폼은 다음과 같은 특성이 있습니다.  Programmable  On-Demand  Self-Service💻 Programmable동적 인프라 플랫폼은 프로그래밍을 쉽게 할 수 있어야 합니다. 유저 인터페이스 외에도 스크립트, CLI와 같은 도구들과도 상호 작용 할 수 있도록 프로그래밍 API가 필요합니다.아래 각 플랫폼 별 SDK를 사용하면 클라우드 내 자원을 생성하고 관리하는 코드를 작성할 수 있습니다.  AWS SDK  Azure SDK  GCP SDK  Openstack SDK⏰ On-Demand동적 인프라 플랫폼에서 자원을 즉시 생성하고 삭제하는 기능은 필수입니다.또한 전통적인 인프라의 과금 정책이 일정 기간 동안의 계약을 기반으로 한다면, 동적 인프라에서는 시간당 과금 체계를 지원합니다.🎊 드디어 대망의 셀프 서비스가 처음 소개 됩니다!🏃🏻 Self-Service셀프서비스는 온디맨드 요구 사항을 좀 더 발전시킨 개념입니다. 전통적인 방법에서 인프라를 요구하기 위해서는 세부 요청 양식, 설계 및 명세 문서, 구현 계획 수립 등을 필요로 했습니다.셀프서비스는 전통적인 방법에서 더 진화하여 필요한 인프라를 즉시 프로비저닝하고 쉽게 수정할 수 있는 자동화된 절차를 의미합니다.🛠 셀프 서비스는 인프라 템플릿을 운용할 수 있는 IaC 도구를 기반으로 구현합니다.예를 들어 개발자가 로드밸런서로 ALB를 사용하고 있다가 NLB로 바꾸고 싶은 경우, 다음과 같이 정의된 인프라 코드를 재배포 하면 됩니다.resource \"aws_lb\" \"test\" {  name               = \"test-lb-tf\"  internal           = false    # load_balancer_type = \"application\"  load_balancer_type = \"network\"    # Leave out other config ...}HashiCorp가 정의한 Self-Service Infrastructure 게시물을 보면,Self-Service Infra가 어떤 의미인지 더 쉽게 다가옵니다. (첨부된 링크를 통해 셀프서비스의 장점을 꼭 한번 읽어보세요!)기존 방법개발자는 인프라 담당자가 인프라를 할당할 때까지 기다려야 합니다.셀프 서비스 적용작성된 인프라 템플릿을 활용해 온디맨드로 프로비저닝 할 수 있습니다.글을 마치며저는 테라폼을 공부한 이후, 간단한 웹서비스를 운영하는 토이프로젝트를 진행할 때 다음과 같은 인프라 환경을 자주 사용합니다.미리 코드로 정의한 인프라 덕분에 개발에만 집중할 수 있는 환경과 생산성 향상을 경험했습니다.또한 테라폼 모듈 덕분에 제가 원하는 대로 인프라 스펙을 변경하고 각종 클라우드 서비스 추가 혹은 제거가 가능했습니다.셀프서비스 인프라가 기존 승인 체계(리소스 요청 ➡️ 담당자 승인)를 부정하는 것은 아니라고 생각합니다.기존 체계가 갖고 있는 보안적 이점을 포함한 장점들을 유지하며, 유연하고 신속하게 인프라를 운용하는 Self-service 환경이 불러올 장점을 고민해 봐야겠습니다.마지막으로, DevOps 문화가 정착해가며 CI/CD 를 통해 지속적 배포가 가능해지며 더 잦은 서비스 출시가 가능해졌습니다.또한, IaC를 통해 Immutable Infra를 추구하며 인프라의 일관성과 안정성을 보장하게 되었습니다.앞선 두 개의 개념에 더해 Self-service를 추구한다면 조직의 민첩성과 생산성 향상에 도움이 될 것이라고 생각합니다.소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해주세요! 😃",
        "url": "/SelfService"
    }
    ,
    
    "packer": {
        "title": "Provision Infrastructure with Packer",
            "author": "HeuristicWave",
            "category": "",
            "content": "본 글은 HashiCorp Learn - Provision Infrastructure with Packer에서 다루는 내용을 기반으로 작성한 글 입니다.  Packer를 사용해 AWS AMI를 만들고 Terraform과 연계하여 활용하는 방법에 약간의 설명과 팁을 담아 한국어로 재작성해 보았습니다. (설치와 관련된 준비사항은 생략되어 있으므로 원문을 확인해주세요.)프로젝트 구조먼저, 해당 튜토리얼을 진행하기 위한 프로젝트 구조는 아래와 같습니다. 아래와 폴더와 파일을 준비해주세요..└── provision-infra-with-packer    ├── images    │   └── image.pkr.hcl    ├── instances    │   ├── main.tf    │   └── variables.tf    └── scripts        └── setup.shLocal SSH key 생성하기provision-infra-with-packer 폴더 안에서 AWS AMI로 만들 인스턴스에 접속하기 위한 SSH 키를 생성합니다.필자는 사용할 공개키를 Mac OS에서 ssh-keygen으로 생성하였습니다. 각자 환경에 맞는 방법으로 SSH 공개키를 생성하세요.암호화 타입(-t)을 RSA로 주석(-C)을 이메일로 생성되는 공개키의 위치(-f)를 현재 directory로 설정하고 공개키의 이름을 tf-packer로 설정했습니다.명령어를 입력하고 비밀번호를 입력하면 되지만 편의상 공백으로 두겠습니다.$ ssh-keygen -t rsa -C \"your_email@example.com\" -f ./tf-packer이후 tf-packer와 tf-packer.pub 2가지 파일이 생성되었다면 다음 단계 🚀Packer 코드 작성하기image.pkr.hcl 파일에서 진행합니다. 1️⃣ AMI 구축에 필요한 config 작성packer가 빌드한 이미지가 저장될 리전의 정보와 AMI에 timestamp 정보를 넣기 위한 config를 차례로 작성합니다.variable \"region\" {  type    = string  default = \"us-east-1\"}locals { timestamp = regex_replace(timestamp(), \"[- TZ:]\", \"\") }2️⃣️ Base AMI에 대한 Source AMI config 작성packer가 이미지를 만들기 위해 기본으로 사용되는 Base AMI에 대한 정보를 source 블록에 정의합니다.1단계에서 작성한 config 값을 활용해 리전과 timestamp를 넣어주는 코드와 Packer Builder로 사용할 인스턴스 타입을 지정합니다.이후, AWS에 존재하는 수많은 AMI 중에서 source로 활용할 이미지를 filter 코드로 작성합니다.(name 부분에 직접 ami 번호를 명시적으로 기재 할 수도 있습니다.)source \"amazon-ebs\" \"example\" {  ami_name      = \"learn-terraform-packer-${local.timestamp}\"  instance_type = \"t2.micro\"  region        = var.region  source_ami_filter {    filters = {      name                = \"ubuntu/images/*ubuntu-xenial-16.04-amd64-server-*\"      root-device-type    = \"ebs\"      virtualization-type = \"hvm\"    }    most_recent = true    owners      = [\"099720109477\"]  }  ssh_username = \"ubuntu\"}3️⃣ Packer build config 작성build 부분에서는 환경변수 세팅이나 명령어를 inline 형태로 기입 할 수 있지만,아래와 같은 간단한 기능만 수행하도록 코드를 작성합니다. hcl 문법에 따라 source를 지정하고, 프로비저닝을 위한 공개키의 위치를 명세합니다. source는 로컬 머신, destination은 원격 머신 입니다.마지막으로 Packer로 빌드한 이미지에서 Application Setup이 담긴 script를 지정합니다.build {  sources = [\"source.amazon-ebs.example\"]  provisioner \"file\" {    source      = \"../tf-packer.pub\"    destination = \"/tmp/tf-packer.pub\"  }  provisioner \"shell\" {    script = \"../scripts/setup.sh\"  }}4️⃣ Shell script 작성해당 작업은 scripts 폴더의 setup.sh에서 진행합니다. 이 부분은 Terraform으로 프로비저닝 한 인프라를 웹페이지에서 확인하기 위해 간단한 샘플을 띄우는 코드가 담겨있습니다.아래 Script에 필요한 종속성 설치, terraform을 user에 추가, 생성한 SSH키 설치, 샘플 Go App 설치가 단계가 작성되어 있습니다.  #!/bin/bashset -e# Install necessary dependenciessudo DEBIAN_FRONTEND=noninteractive apt-get -y -o Dpkg::Options::=&quot;--force-confdef&quot; -o Dpkg::Options::=&quot;--force-confold&quot; dist-upgradesudo apt-get -y -qq install curl wget git vim apt-transport-https ca-certificatessudo add-apt-repository ppa:longsleep/golang-backports -ysudo apt -y -qq install golang-go# Setup sudo to allow no-password sudo for &quot;hashicorp&quot; group and adding &quot;terraform&quot; usersudo groupadd -r hashicorpsudo useradd -m -s /bin/bash terraformsudo usermod -a -G hashicorp terraformsudo cp /etc/sudoers /etc/sudoers.origecho &quot;terraform ALL=(ALL) NOPASSWD:ALL&quot; | sudo tee /etc/sudoers.d/terraform# Installing SSH keysudo mkdir -p /home/terraform/.sshsudo chmod 700 /home/terraform/.sshsudo cp /tmp/tf-packer.pub /home/terraform/.ssh/authorized_keyssudo chmod 600 /home/terraform/.ssh/authorized_keyssudo chown -R terraform /home/terraform/.sshsudo usermod --shell /bin/bash terraform# Create GOPATH for Terraform user &amp; download the webapp from GitHubsudo -H -i -u terraform -- env bash &lt;&lt; EOFwhoamiecho ~terraformcd /home/terraformexport GOROOT=/usr/lib/goexport GOPATH=/home/terraform/goexport PATH=$PATH:$GOROOT/bin:$GOPATH/bingo get -d github.com/hashicorp/learn-go-webapp-demoEOF1 ~ 4단계를 마쳤다면 images 폴더 위치에 packer build image.pkr.hcl 명령어로 이미지를 빌드합니다.이미지 빌드 후, 콘솔 Images 탭의 AMIs을 확인하면 빌드한 이미지가 존재합니다.또한, 인스턴스 탭을 확인하면 아래와 같이 Base AMI를 만들기 위한 Packer Builder의 흔적을 볼 수 있습니다.해당 화면의 Instance ID를 눌러 설정들을 확인해보면 source 블록에 정의한 값을 바탕으로 만들어진 모습을 확인할 수 있습니다.Terraform으로 Packer 이미지 배포해당 작업은 instances 폴더에서 진행합니다. Sample App Infra Code 작성하기아래 🛠 이모티콘을 클릭하여 main.tf와 varaiables.tf를 작성합니다.  🔨 main.tf 🔨    terraform {  required_providers {    aws = {      source  = \"hashicorp/aws\"      version = \"~&gt; 3.26.0\"    }  }  required_version = \"~&gt; 1.0.2\"}provider \"aws\" {  region = var.region}resource \"aws_vpc\" \"vpc\" {  cidr_block           = var.cidr_vpc  enable_dns_support   = true  enable_dns_hostnames = true}resource \"aws_internet_gateway\" \"igw\" {  vpc_id = aws_vpc.vpc.id}resource \"aws_subnet\" \"subnet_public\" {  vpc_id     = aws_vpc.vpc.id  cidr_block = var.cidr_subnet}resource \"aws_route_table\" \"rtb_public\" {  vpc_id = aws_vpc.vpc.id  route {    cidr_block = \"0.0.0.0/0\"    gateway_id = aws_internet_gateway.igw.id  }}resource \"aws_route_table_association\" \"rta_subnet_public\" {  subnet_id      = aws_subnet.subnet_public.id  route_table_id = aws_route_table.rtb_public.id}resource \"aws_security_group\" \"sg_22_80\" {  name   = \"sg_22\"  vpc_id = aws_vpc.vpc.id  # SSH access from the VPC  ingress {    from_port   = 22    to_port     = 22    protocol    = \"tcp\"    cidr_blocks = [\"0.0.0.0/0\"]  }  ingress {    from_port   = 8080    to_port     = 8080    protocol    = \"tcp\"    cidr_blocks = [\"0.0.0.0/0\"]  }  ingress {    from_port   = 80    to_port     = 80    protocol    = \"tcp\"    cidr_blocks = [\"0.0.0.0/0\"]  }  egress {    from_port   = 0    to_port     = 0    protocol    = \"-1\"    cidr_blocks = [\"0.0.0.0/0\"]  }}resource \"aws_instance\" \"web\" {  ami                         = \"ami-YOUR-AMI-ID\"  instance_type               = \"t2.micro\"  subnet_id                   = aws_subnet.subnet_public.id  vpc_security_group_ids      = [aws_security_group.sg_22_80.id]  associate_public_ip_address = true  tags = {    Name = \"Learn-Packer\"  }}output \"public_ip\" {  value = aws_instance.web.public_ip}    🔧 varaiables.tf 🔧    variable \"cidr_vpc\" {  description = \"CIDR block for the VPC\"  default     = \"10.1.0.0/16\"}variable \"cidr_subnet\" {  description = \"CIDR block for the subnet\"  default     = \"10.1.0.0/24\"}variable \"environment_tag\" {  description = \"Environment tag\"  default     = \"Learn\"}variable \"region\"{  description = \"The region Terraform deploys your instance\"  default     = \"us-east-1\"}    인프라 코드에 대한 설명은 생략하겠습니다. 인프라에 대한 테라폼 코드가 궁금하시다면 다른 게시글이나 제 포스팅을 읽어보세요!AMI Query 하기본문에는 소개되지 않았지만, Packer와 Terraform의 통합을 위해 필자가 작성한 부분입니다.Terraform의 Data Sources를 활용해 빌드된 AMI를 Query하여 샘플 앱을 프로비저닝 해봅시다.선행 작업에서 진행한 main.tf 하단에 아래 코드를 추가합니다.data \"aws_ami_ids\" \"myami\" {  owners = [\"YOUR Account ID\"]  # sort_ascending = true    filter {    name   = \"name\"    values = [\"learn-terraform-packer-*\"]  }}Packer로 이미지를 빌드했으므로, ami owner가 되었습니다!owners 부분에 자신의 계정 ID를 작성하고 filter에 packer build를 할 떄 사용한 ami_name을 value 값으로 넣습니다.  sort_ascending default 값이 false  sort_ascending = false : list의 0번째 요소가 latest sort_ascending = true : 만들어진 순서대로 리스트 생성 (가장 먼저 생성된 ami가 0번)이어서 main.tf에 인스턴스 리소스를 정의한 부분의 ami = \"ami-YOUR-AMI-ID\" 코드를 아래와 같이 대체합니다.data 객체에 필터링한 결과 값들이 빌드된 AMI들이 리스트 형식으로 들어가는데 latest 버전을 사용하기 위해 0번째 이미지를 명시합니다.편의상 사용된 ami 번호를 터미널에서 확인 할 수 있도록 output에 대한 코드도 함께 작성합니다.resource \"aws_instance\" \"web\" {  ami = data.aws_ami_ids.myami.ids[0]   # \"ami-YOUR-AMI-ID\"  # Skip Other Config}output \"my_ami\" {  value = aws_instance.web.ami}코드를 작성하고 테라폼 코드가 위치한 폴더에서(instances 폴더 하위) 다음 명령어를 실행합니다.terraform init &amp;&amp; terraform apply 인스턴스를 생성하기 위해 yes를 기입합니다. 명령어로 인프라 상태를 점검해 아래와 같다면 다음 단계로 🚀$ terraform state listdata.aws_ami_ids.myamiaws_instance.webaws_internet_gateway.igwaws_route_table.rtb_publicaws_route_table_association.rta_subnet_publicaws_security_group.sg_22_80aws_subnet.subnet_publicaws_vpc.vpc  data object에 담긴 정보가 확인하고 싶다면? $ terraform state show data.aws_ami_ids.myami.ids인스턴스 확인하기SSH를 통해 인스턴스에 연결합니다. ssh terraform@$(terraform output -raw public_ip) -i ../tf-packerGo 디렉토리로 이동하세요. cd go/src/github.com/hashicorp/learn-go-webapp-demo데모 앱을 실행합니다. go run webapp.go배포한 앱 확인을 위해 terraform output public_ip로 얻은 IP에 8080 포트로 접속하면 간단한 테트리스 게임 앱을 확인할 수 있습니다.인스턴스 리소스 회수terraform destroy 명령어로 상기 프로젝트에서 사용한 인프라를 리소스를 회수합니다. Packer로 작성한 이미지는 파괴되지 않습니다.위와 같은 단계들을 통해 패커로 이미지를 만들고 테라폼과 통합하는 방법을 학습해보았습니다.이번 포스팅에서 다뤘던 내용은 Immutable Servers를 유지하기 위한 방법 중 하나입니다.오늘 포스팅에 추가로 Ansible을 통합한다면, Immutable Infrastructure를 구축할 수도 있습니다.Terraform Courses    3-Tier VPC Architecture with Terraform    ECR Pipeline with Terraform Ⅰ (CodeCommit)    ECR Pipeline with Terraform Ⅱ (ECR, CodeBuild, IAM)    ECR Pipeline with Terraform Ⅲ (CodePipeline)    Provision Infrastructure with Packer",
        "url": "/Packer"
    }
    ,
    
    "eks-max-pods": {
        "title": "Look into EKS max pods",
            "author": "HeuristicWave",
            "category": "",
            "content": "EKS 노드에서 사용 가능한 Pod의 개수는 몇 개일까?Preview이번 포스팅에서는 EKS의 노드 그룹에서는 최대 몇 개의 포드(Private IP)가 할당 가능하고 어떠한 방법으로 최대 포드의 개수를 제어할 수 있는지 알아보자.Build Up쿠버네티스 도규먼트에 따르면 노드당 110개의 포드를 생성할 수 있으며, 노드는 5000개까지 생성 가능해 총 15만 개의 포드가 생성 가능하다고 한다. GCP의 GKE 가이드에 따르면 기본 클러스터 노드 하나에 최대 110개의 포드가 생성 가능하다고 한다. 구글링을 통해 확인하니, 노드에서 포드의 갯수가 증가할수록 kubelet, cAdvisor 등과 같은 K8s 에이전트에 오버헤드를 발생시키므로 110개 정도를 권장한다고 한다.그래서 그런지 kubelet docs에서도 아래와 같은 max-pods가 110을 기본값으로 가진다.--max-pods int32     Default: 110EKS eni max podsIP addresses per network interface per instance type을 확인해보면 AWS의 인스턴스 타입별 ENI 개수를 파악할 수 있다.EKS 설명서를 보면 다음과 같은 공식을 확인할 수 있다. (# of network interfaces for the instance type × (# of IPv4 per network interface - 1)) + 2ENI의 첫 번째 IP는 포드가 사용할 수 없으므로 1을 빼고 AWS CNI와 kube-proxy가 차지하는 2개의 IP를 마지막 수식에 더해 최종 사용 가능한 max-pod 값을 알 수 있다. 설명한 공식 이외에도 kubectl 명령어를 통해 Maximum Pods를 파악할 수 있다.❯ kubectl get nodes -ANAME                                            STATUS   ROLES    AGE   VERSIONip-10-0-0-178.ap-northeast-2.compute.internal   Ready    &lt;none&gt;   18h   v1.19.6-eks-49a6c0ip-10-0-1-143.ap-northeast-2.compute.internal   Ready    &lt;none&gt;   18h   v1.19.6-eks-49a6c0❯ kubectl describe nodes ip-10-0-0-178.ap-northeast-2.compute.internal | grep -i pods  pods:                        17  pods:                        17Non-terminated Pods:          (17 in total)👀 Labs 1kubectl의 명령어에서 노드(t3.medium)당 17개의 포드를 사용할 수 있다고 했는데, 직접 노드그룹 내에서 포드를 최대로 띄워 확인해보자. 노드그룹 내에 nginx 32개를 올려보았다.K9s 쉘을 통해 총 38개의 포드가 확인되었고 38개 중 aws-node, coredns, kube-proxy가 각각 2개의 노드에 위치하고 4개의 nginx 포드가 Pending 상태라는 것을 파악했다.즉, 38(Total) - 4(Pending) = 17(t3.medium Maximum Pods) * 2(# of Node) 실험 결과와 앞서 알아본 Maximum Pods가 동일하다.Maximum Pods 변경하기EKS에서 Maximum Pods를 결정 짓는 요소는 ENI다. 그러나 클러스터의 노드그룹을 생성할 때 kubelet의 max-pods 값을 변경해 커스터마이징 할 수 있다. AWS Docs에서 다음과 같은 (불친절한?) 설명을 통해 ENI와 별개로 max-pods를 제어할 수 있는 힌트를 얻었다.공식 문서에서 설명이 굉장히 빈약하지만, EKS에서 노드 그룹을 커스텀으로 생성할 때 Launch templates의 UserData를 아래와 같이 정의하면 Maximum Pods가 변경된다.MIME-Version: 1.0Content-Type: multipart/mixed; boundary=\"==MYBOUNDARY==\"--==MYBOUNDARY==Content-Type: text/x-shellscript; charset=\"us-ascii\"#!/bin/bash/etc/eks/bootstrap.sh {Cluster Name} --use-max-pods false --kubelet-extra-args '--max-pods=10'--==MYBOUNDARY==--\\  GCP에서 Max Pods를 제어하는 방법 👈 Click!   GCP에서는 클러스터를 생성할 때아래 명령어의 --default-max-pods-per-node 파라미터를 통해 max-pods(👆 Build Up 단계에서 default 110 👆)를 조절할 수 있다.  gcloud container clusters create CLUSTER_NAME \\  # 생략  --default-max-pods-per-node MAXIMUM_PODS \\  # 생략  👀 Labs 2아래 사진의 왼쪽은 Labs 1의 Pure한 t3.medium, 오른쪽은 max-pods를 지정한 Custom t3.medium이다. 콘솔화면에서 스펙은 같지만 할당된 포드의 수가 다르다.Result지금까지 다양한 방법을 통해 EKS에서 Maximum Pods를 파악하는 방법과 변경하는 방법 배웠다.AWS에서는 ENI라는 가상 네트워크 카드를 나타내는 논리적 네트워크 구성 요소 덕분에 인스턴스 타입마다 생성될 수 있는 포드의 수가 달랐다.그 밖에도 GCP에 쿠버네티스의 설계 철학을 그대로 이어받아 kubelet의 max-pods 값이 동일하고 클러스터 생성 순간에도 max-pods 설정에 대한 자유도가 높다는 사실을 알 수 있었다.어떠한 방법이 더 우위에 있는지 결론짓기 어렵지만, CSP의 쿠버네티스 max-pods 생성 원리를 파악하여 최적의 IP 할당에 도움이 되면 좋겠다.소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해주세요! 😃EKS Series    Look into EKS max pods",
        "url": "/EKS_Max_Pods"
    }
    ,
    
    "codepipeline": {
        "title": "ECR CodePipeline with Terraform Ⅲ",
            "author": "HeuristicWave",
            "category": "",
            "content": "Terraform으로 ECR 파이프라인 구축하기 3 (CodePipeline)Preview3편에서는 CodePipeline을 생성하고 IAM 정책과 역할을 부여하는 법을 배워보겠습니다.문서를 확인하면 artifact가 담기는 버킷, pipeline을 생성하는 리소스, 관련된 IAM Role과 Policy가 보입니다.테라폼은 선언형 언어이므로 Role과 Resource의 작성 순서가 바뀌어도 상관이 없지만, 콘솔에서 작업할 경우 Role을 먼저 작성하고 리소스를 생성하니 3편에서는 IAM을 먼저 작성하겠습니다.IAM Role아래 Role을 codepipeline.tf에 작성합니다.cat &lt;&lt;EOF &gt; codepipeline.tfresource \"aws_iam_role\" \"codepipeline_role\" {  name = \"terraform-codepipeline\"  assume_role_policy = &lt;&lt;EOF{  \"Version\": \"2012-10-17\",  \"Statement\": [    {      \"Action\": \"sts:AssumeRole\",      \"Principal\": {        \"Service\": \"codepipeline.amazonaws.com\"      },      \"Effect\": \"Allow\"    }  ]}EOF}EOFIAM Policy본래 필요한 정책만을 골라 정책 생성기에서 생생된 정책을 활용하는 방법이 있지만, 어떤 정책이 필요한지 한번에 맞추기는 너무 어렵습니다.(저의 경우 인터넷에서 타인이 작성한 정책과 에러메시지를 맞아가며 정책을 작성하고 있습니다 😅)➕ 아래 Policy를 방금전 생성한 codepipeline.tf에 아래 코드를 추가합니다.resource \"aws_iam_policy\" \"codepipeline_policy\" {  description = \"Codepipeline Execution Policy\"  policy      = &lt;&lt;EOF{  \"Version\": \"2012-10-17\",  \"Statement\": [    {      \"Action\": [        \"s3:GetObject\", \"s3:GetObjectVersion\", \"s3:PutObject\",        \"s3:GetBucketVersioning\"      ],      \"Effect\": \"Allow\",      \"Resource\": \"${aws_s3_bucket.artifact_bucket.arn}/*\"    },    {      \"Action\" : [        \"codebuild:StartBuild\", \"codebuild:BatchGetBuilds\",        \"iam:PassRole\"      ],      \"Effect\": \"Allow\",      \"Resource\": \"*\"    },    {      \"Action\" : [        \"codecommit:CancelUploadArchive\",        \"codecommit:GetBranch\",        \"codecommit:GetCommit\",        \"codecommit:GetUploadArchiveStatus\",        \"codecommit:UploadArchive\"      ],      \"Effect\": \"Allow\",      \"Resource\": \"${aws_codecommit_repository.test.arn}\"    }  ]}EOF}🚩 이어서 생성한 Policy를 Role에 부여합니다. 이것 역시 codepipeline.tf에 추가합니다.resource \"aws_iam_role_policy_attachment\" \"codepipeline-attach\" {  role       = aws_iam_role.codepipeline_role.name  policy_arn = aws_iam_policy.codepipeline_policy.arn}CodePipelineaws_codepipeline리소스의 config에는 artifact store와 암호화 키, Source-Build-Deploy로 이어지는 각 Stage가 선언되어 있습니다.리소스 안에 기재된 설정들은 필수가 아니므로 선택하여 사용할 수 있습니다. 이번 포스팅에서는 deploy stage와 암호화 config는 제외하고 진행하겠습니다.resource \"aws_codepipeline\" \"pipeline\" {  name     = \"${var.source_repo_name}-${var.source_repo_branch}-Pipeline\"  role_arn = aws_iam_role.codepipeline_role.arn  artifact_store {    location = aws_s3_bucket.artifact_bucket.bucket    type     = \"S3\"  }  stage {    name = \"Source\"    action {      name             = \"Source\"      category         = \"Source\"      owner            = \"AWS\"      version          = \"1\"      provider         = \"CodeCommit\"      output_artifacts = [\"SourceOutput\"]      run_order        = 1      configuration = {        RepositoryName       = var.source_repo_name        BranchName           = var.source_repo_branch        PollForSourceChanges = \"false\"      }    }  }  stage {    name = \"Build\"    action {      name             = \"Build\"      category         = \"Build\"      owner            = \"AWS\"      version          = \"1\"      provider         = \"CodeBuild\"      input_artifacts  = [\"SourceOutput\"]      output_artifacts = [\"BuildOutput\"]      run_order        = 1      configuration = {        ProjectName = aws_codebuild_project.codebuild.id      }    }  }}1편에서 작성한 CodeCommit을 Stage의 Source, 2편에서 작성한 CodeBuild를 Build단계 지정했습니다.terraform apply, plan 명령어를 차례로 반영해 오류가 없는지 확인합니다.지금까지 작성된 인프라를 terraform state list명령어를 통해 확인하면 아래와 같습니다.❯ terraform state listaws_codebuild_project.codebuildaws_codecommit_repository.testaws_codepipeline.pipelineaws_ecr_repository.image_repoaws_iam_policy.codebuild_policyaws_iam_policy.codepipeline_policyaws_iam_role.codebuild_roleaws_iam_role.codepipeline_roleaws_iam_role_policy_attachment.codebuild-attachaws_iam_role_policy_attachment.codepipeline-attachaws_s3_bucket.artifact_bucketCodePipeline 콘솔에서 확인하면 권한이 없어 실패한 화면이 나올 것 입니다.이를 해결하기 위해 또 다른 권한이 필요합니다.CodePipeline TriggerCodeCommit에서 발생한 이벤트가 CodePipeline으로 트리거되기 위해서는 아래 정의된 권한이 필요합니다.➕ 아래 코드를 codepipeline.tf에 추가하고 인프라를 생성해주세요.  resource &quot;aws_iam_role&quot; &quot;trigger_role&quot; {  name               = &quot;terraform-trigger&quot;  assume_role_policy = &lt;&lt;EOF{  &quot;Version&quot;: &quot;2012-10-17&quot;,  &quot;Statement&quot;: [    {      &quot;Action&quot;: &quot;sts:AssumeRole&quot;,      &quot;Principal&quot;: {        &quot;Service&quot;: &quot;events.amazonaws.com&quot;      },      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Sid&quot;: &quot;&quot;    }  ]}EOF}resource &quot;aws_iam_policy&quot; &quot;trigger_policy&quot; {  description = &quot;CodePipeline Trigger Execution Policy&quot;  policy      = &lt;&lt;EOF{  &quot;Version&quot;: &quot;2012-10-17&quot;,  &quot;Statement&quot;: [    {      &quot;Action&quot;: [        &quot;codepipeline:StartPipelineExecution&quot;      ],      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Resource&quot;: &quot;${aws_codepipeline.pipeline.arn}&quot;    }  ]}EOF}resource &quot;aws_iam_role_policy_attachment&quot; &quot;trigger-attach&quot; {  role       = aws_iam_role.trigger_role.name  policy_arn = aws_iam_policy.trigger_policy.arn}ResultTrigger 까지 정상적으로 적용하고 테스트용으로 활용할 아무 Dockerfile을 CodeCommit에 Push합니다.다시 CodePipeline 콘솔에 접속해 우상단에 위치한 변경사항 릴리스를 누르면,아래와 같이 정상적으로 코드 파이프라인이 작동하여 운영되는 것을 확인 할 수 있습니다.마지막으로, 빌드된 ECR 이미지를 다운받아 로컬에서 실행시켜보며 정상적으로 작성되었는지 확인해보겠습니다.  ECR에 올라온 이미지를 사용하기 위해 환경변수 세팅 🔨    export tf_image_repo_url=$(terraform output -raw image_repo_url)        AWSCLI로 ECR 로그인 (Region명 주의!) 🔑    aws ecr get-login-password --region {YOUR_REGION} | docker login --username AWS --password-stdin $tf_image_repo_url        이미지 Pull 후, Run 명령어 실행 💻    docker pull $tf_image_repo_urldocker run $tf_image_repo_url      🏵 Docker 실행 후, Shell에서 Hello, Go examples! 메시지를 확인하였다면 성공!CleanupS3 bucket은 빈상태여야 제거가 가능하기에 S3 콘솔에서 ecr-pipeline의 데이터를 모두 삭제합니다.이어서 terraform destory 명령어로 모든 리소스를 회수합니다.총 3편에 걸쳐서 테라폼으로 최소한의 리소스로 ECR Pipeline 구축법을 알아보았습니다. (CloudWatch 기능을 추가해 CodePipeline을 구축해보세요 👍)해당 과정을 통해 AWS 인프라 생성법과, IAM 활용법, Variable, Output, tfvars 등을 활용해 코드를 작성하는 법을 공부했습니다.다른 CI/CD 파이프라인 구축법도 이번 포스팅에서 다룬 방법과 크게 다르지 않으니, 해당 포스팅이 도움이 되면 좋겠습니다. 😁Terraform Courses    3-Tier VPC Architecture with Terraform    ECR Pipeline with Terraform Ⅰ (CodeCommit)    ECR Pipeline with Terraform Ⅱ (ECR, CodeBuild, IAM)    ECR Pipeline with Terraform Ⅲ (CodePipeline)    Provision Infrastructure with Packer",
        "url": "/CodePipeline"
    }
    ,
    
    "codebuild": {
        "title": "ECR CodePipeline with Terraform Ⅱ",
            "author": "HeuristicWave",
            "category": "",
            "content": "Terraform으로 ECR 파이프라인 구축하기 2 (ECR, CodeBuild, IAM)2편에서는 ECR과 CodeBuild를 생성하고 IAM 역할, 정책을 부여하는 법을 학습합니다.ECRECR 역시 공식 문서에서 사용방법을 확인합니다.공식문서에서 image_scanning_configuration config를 사용하면 취약점 스캔이 가능하다 설명되어 있지만, 필요하지 않기 때문에 제외하겠습니다.더불어, output도 함께 작성하겠습니다.cat &lt;&lt;EOF &gt; ecr.tfresource \"aws_ecr_repository\" \"image_repo\" {  name                 = var.image_repo_name  image_tag_mutability = \"MUTABLE\"}output \"image_repo_url\" {  value = aws_ecr_repository.image_repo.repository_url}output \"image_repo_arn\" {  value = aws_ecr_repository.image_repo.arn}EOF이어서 ecr.tf에서 변수로 사용하기 위한 var.image_repo_name 부분이 작동하도록 1편에서 작성한 variables.tf 아래 값을 추가합니다.✅ 편의상 이번 단계에 필요한 variable을 함께 포함했습니다.variable \"image_repo_name\" {  description = \"Image repo name\"  type        = string}variable \"container_name\" {  description = \"Container Name\"  default     = \"my-container\"}variable \"source_repo_branch\" {  description = \"Source repo branch\"  type        = string}ecr 작성을 완료햇으니 plan, apply 명령어를 차례로 입력해 인프라를 생성하고 terraform state list명령어나 콘솔에서 생성된 인프라를 확인합니다.CodeBuildCodeBuild를 사용하기 위해 Terraform 도큐먼트에서 사용법을 확인합니다.기존까지의 작업과는 달리 상당히 어려워 보입니다. 그러나 쓱 훝어보면 크게 4가지(bucket, IAM Role과 Policy, Codebuild)로 정리됩니다.Bucket도큐먼트와 같이 우선적으로 S3를 생성합니다. bucket의 이름은 선택이지만, 여러개의 버킷을 가지고 있는 저는 식별을 위해 이름을 부여했습니다.cat &lt;&lt;EOF &gt; codebuild.tfresource \"aws_s3_bucket\" \"artifact_bucket\" {  bucket = \"ecr-pipeline\"}EOFIAM Role도큐먼트를 따라 AssumeRole을 사용합시다. ➕ S3을 만들때 사용한 codebuild.tf에 아래 코드를 추가합니다.resource \"aws_iam_role\" \"codebuild_role\" {  name = \"terraform-codebuild\"  assume_role_policy = &lt;&lt;EOF{   \"Version\": \"2012-10-17\",   \"Statement\": [      {         \"Effect\": \"Allow\",         \"Principal\": {            \"Service\": \"codebuild.amazonaws.com\"         },         \"Action\": \"sts:AssumeRole\"      }   ]}EOF}IAM Policy정책은 IAM 콘솔에서 기존에 만들어진 정책을 사용할 수도 있지만, 아래와 같이 직접 작성할 수도 있습니다.도큐먼트에서 EC2에 대한 정책을 사용하지만, 우리는 ECR을 사용하므로 아래와 같은 정책을 사용하겠습니다.  resource &quot;aws_iam_policy&quot; &quot;codebuild_policy&quot; {  description = &quot;CodeBuild Execution Policy&quot;  policy      = &lt;&lt;EOF{  &quot;Version&quot;: &quot;2012-10-17&quot;,  &quot;Statement&quot;: [    {      &quot;Action&quot;: [        &quot;logs:CreateLogGroup&quot;, &quot;logs:CreateLogStream&quot;, &quot;logs:PutLogEvents&quot;,        &quot;ecr:GetAuthorizationToken&quot;      ],      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Resource&quot;: &quot;*&quot;    },    {      &quot;Action&quot;: [        &quot;s3:GetObject&quot;, &quot;s3:GetObjectVersion&quot;, &quot;s3:PutObject&quot;      ],      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Resource&quot;: &quot;${aws_s3_bucket.artifact_bucket.arn}/*&quot;    },    {      &quot;Action&quot;: [        &quot;ecr:GetDownloadUrlForLayer&quot;, &quot;ecr:BatchGetImage&quot;,        &quot;ecr:BatchCheckLayerAvailability&quot;, &quot;ecr:PutImage&quot;,        &quot;ecr:InitiateLayerUpload&quot;, &quot;ecr:UploadLayerPart&quot;,        &quot;ecr:CompleteLayerUpload&quot;      ],      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Resource&quot;: &quot;${aws_ecr_repository.image_repo.arn}&quot;    }  ]}EOF}20, 30라인에서 앞서 생성한 리소스를 ${채움참조} 문법으로 유연한 코드를 작성합니다.🚩 이어서 생성한 Policy를 Role에 부여합니다. 이것 역시 codebuild.tf에 추가합니다.resource \"aws_iam_role_policy_attachment\" \"codebuild-attach\" {  role       = aws_iam_role.codebuild_role.name  policy_arn = aws_iam_policy.codebuild_policy.arn}CodeBuildTerraform 도큐먼트를 보아도 어떻게 해야 ECR에 적용시킬 수 있는지 알기 어렵습니다.우선 CodeBuild를 이해하기 위해 AWS docs를 읽어봅시다.대략 리소스 이름을 정하고, 환경을 구성하고 빌드를 하기 위한 방법을 정의해야 한다는 사실을 알 수 있습니다.CodeBuild가 정의된 아래 코드를 활용해 codebuild.tf에 추가합니다.  resource &quot;aws_codebuild_project&quot; &quot;codebuild&quot; {  name         = &quot;codebuild-${var.source_repo_name}-${var.source_repo_branch}&quot;  service_role = aws_iam_role.codebuild_role.arn  artifacts {    type = &quot;CODEPIPELINE&quot;  }    environment {    compute_type                = &quot;BUILD_GENERAL1_MEDIUM&quot;    image                       = &quot;aws/codebuild/standard:3.0&quot;    type                        = &quot;LINUX_CONTAINER&quot;    privileged_mode             = true    image_pull_credentials_type = &quot;CODEBUILD&quot;    environment_variable {      name  = &quot;REPOSITORY_URI&quot;      value = aws_ecr_repository.image_repo.repository_url    }    environment_variable {      name  = &quot;AWS_DEFAULT_REGION&quot;      value = var.aws_region    }    environment_variable {      name  = &quot;CONTAINER_NAME&quot;      value = var.container_name    }  }  source {    type      = &quot;CODEPIPELINE&quot;    buildspec = &lt;&lt;BUILDSPEC${file(&quot;buildspec.yml&quot;)}BUILDSPEC  }}31라인이 참조하는 buildspec.yml을 생성하고, pre_build, build, post_build에 맞춰 작성합니다.version: 0.2phases:  install:    runtime-versions:      docker: 18  pre_build:    commands:      - echo Logging in to Amazon ECR...      - $(aws ecr get-login --region $AWS_DEFAULT_REGION --no-include-email)      - COMMIT_HASH=$(echo $CODEBUILD_RESOLVED_SOURCE_VERSION | cut -c 1-7)      - IMAGE_TAG=${COMMIT_HASH:=latest}  build:    commands:      - echo Build started on `date`      - echo Building the Docker image...      - docker build -t $REPOSITORY_URI:latest .      - docker tag $REPOSITORY_URI:latest $REPOSITORY_URI:$IMAGE_TAG  post_build:    commands:      - echo Build completed on `date`      - echo Pushing the Docker image...      - docker push $REPOSITORY_URI:latest      - docker push $REPOSITORY_URI:$IMAGE_TAG      - echo Writing image definitions file...      - printf '[{\"name\":\"%s\",\"imageUri\":\"%s\"}]' $CONTAINER_NAME $REPOSITORY_URI:$IMAGE_TAG &gt; imagedefinitions.jsonartifacts:  files: imagedefinitions.json지금까지 작성된 인프라를 terraform state list명령어를 통해 확인하면 아래와 같습니다.❯ terraform state listaws_codebuild_project.codebuildaws_codecommit_repository.testaws_ecr_repository.image_repoaws_iam_policy.codebuild_policyaws_iam_role.codebuild_roleaws_iam_role_policy_attachment.codebuild-attachaws_s3_bucket.artifact_bucket  생성한 인프라가 위와 같지 않을 경우, 👉 Click  실수로 의도치 않은 인프라가 프로비저닝 되었다면 2가지 방법을 통해 원 상태로 복구 할 수 있습니다.      terraform destroy 명령어로 특정 인프라만 되돌리거나 프로비저닝 하고싶은 경우, -target 옵션과 함께 resource 명으로 명령어를 작성합니다. 예시) terraform destory -target aws_vpc.main    잘못 작성한 코드를 수정 후, terraform apply명령어를 적용하여 최신 상태의 인프라를 반영합니다.  Terraform Courses    3-Tier VPC Architecture with Terraform    ECR Pipeline with Terraform Ⅰ (CodeCommit)    ECR Pipeline with Terraform Ⅱ (ECR, CodeBuild, IAM)    ECR Pipeline with Terraform Ⅲ (CodePipeline)    Provision Infrastructure with Packer",
        "url": "/CodeBuild"
    }
    ,
    
    "codecommit": {
        "title": "ECR CodePipeline with Terraform Ⅰ",
            "author": "HeuristicWave",
            "category": "",
            "content": "Terraform으로 ECR 파이프라인 구축하기 1 (CodeCommit)Overview이번 포스팅에서는 커밋 후, 도커의 이미지를 자동으로 배포하는 ECR Pipeline을 테라폼으로 생성해보겠습니다.AWS에서 저장소 역할을 하는 CodeCommit, 코드를 빌드하는 CodeBuild, 파이프라인을 자동화 하는 CodePipeline, 컨테이너 이미지를 저장하는 ECR을 활용해 구축합니다.1편에서는 CodeCommit 구축과 terraform의 Output, Variables, tfvars 등을 배워 보겠습니다.준비 작업이번 포스팅의 작업공간(~/terraform)을 생성하고 해당 위치에서 아래 코드 블럭을 터미널에 복사합니다.cat &lt;&lt;EOF &gt; provider.tfprovider \"aws\" {  region  = var.aws_region}EOF이후, terraform init 명령어를 실행시켜주세요.CodeCommitCodeCommit을 사용하기 위해 Terraform 도큐먼트에서 사용법을 확인합니다.링크의 Example Usage를 활용해 코드를 작성할 수도 있지만, 이번 포스팅에서는 제 방식대로아래 코드를 활용해 작성해보겠습니다.링크에서 소개하는 코드와 다른 부분은 output과 variable의 사용 여부입니다.✅ 아래 코드와 도큐먼트의 코드가 어떻게 다른지 꼭 확인해보세요!cat &lt;&lt;EOF &gt; codecommit.tfresource \"aws_codecommit_repository\" \"test\" {  repository_name = var.source_repo_name  description     = \"This is the Sample App Repository\"}output \"source_repo_clone_url_http\" {  value = aws_codecommit_repository.test.clone_url_http}EOFOutput은 향후 clone할 원격 저장소의 위치를 파악하기 위해 넣어줍니다. 또한 Variable을 사용해 보다 유연한 코드를 작성해 보겠습니다.준비 작업에 정의한 리전과 CodeCommit Repo 이름에 Variable을 사용하겠습니다.cat &lt;&lt;EOF &gt; variables.tfvariable \"aws_region\" {  description = \"The AWS region\"  default     = \"ap-northeast-2\"}variable \"source_repo_name\" {  description = \"Source repo name\"  type        = string}EOF위 코드를 복사한 후, terraform plan 명령어로 아래와 같은 화면을 확인 할 수 있습니다.앞서 작성한 variables.tf의 region은 default 값이 있지만, repository는 variable의 형식만 정의되어 있기 때문에 인프라를 생성할 때 필수적으로 이름을 입력받습니다.✅ variable의 input값을 수기로 작성하는 것을 피하고 싶으면 tfvars를 사용합니다. 편의상 이번 프로젝트에서 사용할 값들을 미리 작성하겠습니다.cat &lt;&lt;EOF &gt; terraform.tfvarsaws_ecr=\"my-image\"source_repo_name=\"my-pipeline\"source_repo_branch=\"main\"image_repo_name=\"my-pipeline\"EOFtfvars는 위와 같이 변수의 값을 지정하기도 하지만, .env처럼 외부로 노출하면 안되는 값을 넣어두고 git에 ignore시켜 사용하기도 합니다.위 작업을 진행 후, terraform apply명령어를 적용하면 “Apply complete”과 함께 Outputs 값이 나옵니다.terraform state list명령어 이외에도, 콘솔로 이동하면 생성된 인프라를 확인 할 수 있습니다.생성된 원격저장소를 사용하기 위해 terraform output을 활용해 export 환경 변수를 지정합니다.export tf_source_repo_clone_url_http=$(terraform output -raw source_repo_clone_url_http)echo $tf_source_repo_clone_url_http\t# 확인  윈도우 환경변수 설정 방법 Powershell : $Env:Key=\"Value\" CMD : set Key=\"Value\"Git SettingCodeCommit의 Repo 활용법을 아래 2가지 방법으로 기재하였지만, 해당 실습에서는 1번만 다룹니다.1. 로컬에 위치한 코드를 CodeCommit에 push하기 (원격저장소가 비어있음)로컬의 빈공간에서 CodeCommit Repo 사용을 위한 git remote 지정git initgit remote add origin $tf_source_repo_clone_url_httpgit remote -v   # 원격 저장소 확인해당 포스팅의 2 &amp; 3편에서 ECR에 올릴 이미지 파일 생성하기 위해 아래 샘플 Dockerfile을 생성하겠습니다.cat &lt;&lt;EOF &gt; DockerfileFROM golang:1.12-alpine AS build#Install gitRUN apk add --no-cache git#Get the hello world package from a GitHub repositoryRUN go get github.com/golang/example/helloWORKDIR /go/src/github.com/golang/example/hello# Build the project and send the output to /bin/HelloWorld RUN go build -o /bin/HelloWorldFROM golang:1.12-alpine#Copy the build's output binary from the previous build containerCOPY --from=build /bin/HelloWorld /bin/HelloWorldENTRYPOINT [\"/bin/HelloWorld\"]EOF작성된 Dockerfile을 Commit하고, CodeCommit에 Push합니다.git add Dockerfilegit commit -m \"Create Dockerfile\"git statusgit push origin # main branch로 push자격 증명 문제가 있다면 아래 명령어로 해결합니다. 자격 증명 헬퍼 및 AWS CodeCommit에 대한 HTTPS 연결 문제 해결git config --global credential.helper '!aws codecommit credential-helper $@'git config --global credential.UseHttpPath true2. 로컬에 원격저장소의 코드를 clone하기 (원격저장소가 비어있지 않음)git clone $tf_source_repo_clone_url_http지금까지 도큐먼트를 활용해 코드를 작성하고, variable, output, tfvars의 활용법을 배워보았습니다.앞서 작성된 작업들이 정상적으로 커밋과 clone이 가능하면, 다음 단계로 🚀Terraform Courses    3-Tier VPC Architecture with Terraform    ECR Pipeline with Terraform Ⅰ (CodeCommit)    ECR Pipeline with Terraform Ⅱ (ECR, CodeBuild, IAM)    ECR Pipeline with Terraform Ⅲ (CodePipeline)    Provision Infrastructure with Packer",
        "url": "/CodeCommit"
    }
    ,
    
    "3tier": {
        "title": "3-Tier VPC Architecture with Terraform",
            "author": "HeuristicWave",
            "category": "",
            "content": "본 글은 Configure and Deploying VPCs with Multiple Subnets에서 다루는Production-Ready: The 3-Tier VPC 강의를 바탕으로, 테라폼으로 구축하는 3계층 VPC 아키텍처에 대한 글입니다.  Multi-Tier VPC 란?  VPC를 구축할 때 단일 계층 VPC에 모든 자원을 넣는다면, 네트워크에 접근할 수 있는 잠재적 공격자에게 자원이 노출됩니다. 이를 보완하기 위해 서브넷으로다중 계층 VPC 아키텍처를 만들어 방어 계층을 이룰 수 있습니다.  디자인 패턴 : 3 Tier VPC Architecture테라폼 코드는 모듈로 관리하는 것을 권장하지만, 이번 포스팅에서는 3-tier 아키텍처 중 네트워크와 관련된 부분만을 다뤄 하나의 파일에서 코드를 관리합니다.  Step 0 (테라폼을 활용하실 줄 안다면 넘어가세요)  Step 0  아키텍처를 구성할 폴더를 만들고 provider를 주입합니다.  mkdir architecturecd architectureterraform inittouch threeTierVPC.tf    threeTierVPC.tf에 벤더 정보를 작성합니다.  provider \"aws\" {  region = \"ap-northeast-2\"}    명령어terraform plan, terraform apply를 통해, 오류 없이 통과하는 화면을 확인하고 다음 단계로 🚀      terraform apply로 인프라를 반영 할때, -auto-approve옵션을 주면 yes입력 없이 진행 할 수 있습니다. 그러나 yes를 입력하기 전, 한번 더 검토할 수 있는 기회가 있으므로 권장하지 않습니다.  💡각 소제목 링크에 첨부된 코드를 활용해 Step 0 에서 만든 threeTierVPC.tf에 이어서 작성하거나, 따로 새로운 파일을 만들어 terraform plan, terraform apply 명령어를 차례로 작성하며 계층을 쌓아 올립니다.Step 1Layer 1️⃣ : Public subnet하나의 VPC에 2개의 AZ를 만들고 각각의 Public 서브넷을 위치시킵니다.  퍼블릭 서브넷은 프라이빗 서브넷 보다 적은 수의 IP 예약하는 것이 좋습니다.Step 1 코드를 적용 후, 명령어로 인프라 상태를 점검해 아래와 같다면 다음 단계로 🚀$ terraform state listdata.aws_availability_zones.availableaws_internet_gateway.igwaws_subnet.pub_sub_1aws_subnet.pub_sub_2aws_vpc.main  생성한 인프라가 위와 같지 않을 경우  실수로 의도치 않은 인프라가 프로비저닝 되었다면 2가지 방법을 통해 원 상태로 복구 할 수 있습니다.      terraform destroy 명령어로 특정 인프라만 되돌리거나 프로비저닝 하고싶은 경우, -target 옵션과 함께 resource 명으로 명령어를 작성합니다. 예시) terraform destory -target aws_vpc.main    잘못 작성한 코드를 수정 후, terraform apply명령어를 적용하여 최신 상태의 인프라를 반영합니다.  Step 2Layer 1️⃣ : Internet access resources외부 인터넷과의 노출을 제한하고 나가는 트래픽을 위해 NAT Gateway를 활용합니다. 또한 들어오는 트래픽을 위해 ALB를 위치시켰습니다.로드밸런서와 NAT Gateway는 가용성이 높은 관리형 서비스로 병목 현상에 대해 걱정할 필요가 없습니다.  💡Nat Gateway 알아보기  NAT(네트워크 주소 변환) 게이트웨이를 사용하면 프라이빗 서브넷의 인스턴스를 인터넷 또는 기타 AWS 서비스에 연결하는 한편, 인터넷에서 해당 인스턴스와의 연결을 시작하지 못하게 할 수 있습니다.NAT 게이트웨이를 만들려면 NAT 게이트웨이가 속할 퍼블릭 서브넷을 지정해야 하기 때문에 Step2에서 우선적으로 생성합니다.  도큐먼트로 더 알아보기  그림에서는 보이지 않지만, VPC에는 암시적 라우터가 있으며 라우팅 테이블을 사용하여 네트워크 트래픽이 전달되는 위치를 제어합니다.VPC의 각 서브넷을 라우팅 테이블에 연결해야 합니다. 테이블에서는 서브넷에 대한 라우팅을 제어합니다.Step 2 코드를 적용 후, 명령어로 인프라 상태를 점검해 아래와 같다면 다음 단계로 🚀$ terraform state listdata.aws_availability_zones.availableaws_eip.nat_1aws_eip.nat_2aws_internet_gateway.igwaws_nat_gateway.nat_gateway_1aws_nat_gateway.nat_gateway_2aws_route_table.route_table_pubaws_route_table_association.route_table_association_1aws_route_table_association.route_table_association_2aws_subnet.pub_sub_1aws_subnet.pub_sub_2aws_vpc.main이번 포스팅에서 ALB와 인스턴스는 다루지 않습니다. 추후, 모듈로 테라폼을 관리하는 방법에서 학습하겠습니다. Step 3Layer 2️⃣️ : Apps in a private subnet2개의 프라이빗 서브넷에 각각의 인스턴스를 놓습니다. 이후, 두 퍼블릭 서브넷에 연결된 ALB는 프라이빗 서브넷 리소스 간의 트래픽을 분산시킵니다.  ❗️예제 그림에서는 Private subnet의 cidr block을 10.0.2.0/22로 가이드 하지만, 이는 앞서 만든 서브넷과 범위가 겹치므로 10.0.4.0/22로 바꿔 진행합니다.CIDR 계산기 에서 정확하게 확인해 볼 수 있습니다.  Step3에서도 Step2와 같이 그림에서는 보이지 않는 라우트 테이블을 만들고 NAT 게이트웨이와 프라이빗 서브넷을 연결해 줍니다.프라이빗 서브넷의 요청이 외부로 나갈때는 NAT 게이트웨이의 고정 IP를 사용합니다.(프라이빗 서브넷의 라우트 테이블은 퍼블릭과 달리 2개를 만들어 각각 연결해 주었습니다.)Step 3 코드를 적용 후, 명령어로 인프라 상태를 점검해 아래와 같다면 다음 단계로 🚀$ terraform state listdata.aws_availability_zones.availableaws_eip.nat_1aws_eip.nat_2aws_internet_gateway.igwaws_nat_gateway.nat_gateway_1aws_nat_gateway.nat_gateway_2aws_route.private_nat_1aws_route.private_nat_2aws_route_table.route_table_pri_1aws_route_table.route_table_pri_2aws_route_table.route_table_pubaws_route_table_association.route_table_association_1aws_route_table_association.route_table_association_2aws_route_table_association.route_table_pri_association_1aws_route_table_association.route_table_pri_association_2aws_subnet.pri_sub_1aws_subnet.pri_sub_2aws_subnet.pub_sub_1aws_subnet.pub_sub_2aws_vpc.main이번 포스팅에서 DB와 인스턴스 연결은 다루지 않습니다. 추후, 모듈로 테라폼을 관리하는 방법에서 학습하겠습니다. Step 4Layer 3️⃣ : Data in a second private subnet첫 번째 프라이빗 서브넷 뒤에 두 번째 프라이빗 서브넷을 배치합니다. (코드 생략) 장애가 발생할 경우를 대비해 read-replica 혹은 standby 구성으로 배치합니다.  ❗Step4의 서브넷 작성법은 Step3의 방법과 동일합니다. 아래 사진은 CIDR의 범위가 겹치므로, Private subnet의 cidr block을 10.0.8.0/23을 10.0.12.0/23로, 10.0.10.0/23을 10.0.14.0/23으로 바꿔 진행하세요.  데이터 리소스(/23)보다를 앱 리소스(/22)를 확장할 가능성이 커, 더 큰 서브넷 마스크를 할당합니다.Step 5Leave extra IPs available배포된 인프라가 확장되어 아키텍처가 변경될 때 사용할 수 있는 여유분의 IP를 예약을 할 수도 있습니다. (코드 생략)위와 같은 단계들을 통해 3 Tier VPC Architecture를 학습해보았습니다.Terraform으로 생성된 자원들은 terraform destory명령어를 통해 학습을 시작하기 전 상태로 되돌리세요.다음 포스팅에서는 이번 포스팅에서 생략했던 인스턴스, DB, LB 등을 모듈로 관리하며 다뤄 보겠습니다.Terraform Courses    3-Tier VPC Architecture with Terraform    ECR Pipeline with Terraform Ⅰ (CodeCommit)    ECR Pipeline with Terraform Ⅱ (ECR, CodeBuild, IAM)    ECR Pipeline with Terraform Ⅲ (CodePipeline)    Provision Infrastructure with Packer",
        "url": "/3Tier"
    }
    ,
    
    "pipenv01": {
        "title": "Pipenv, Nginx, Gunicorn 서버 운영하기",
            "author": "HeuristicWave",
            "category": "",
            "content": "Virtualenv 환경이 아닌 Pipenv를 사용하며 만난 에러 해결 과정 정리사전 작업자세한 방법은 하단 참고자료를 통해 확인 할 수 있다      gunicorn, nginx 설치          ec2에서 nginx 설치하기 : CentOS 7 Nginx 설치 방법            gunicorn 작동확인  Gunicorn 🦄서비스 등록 스크립트 생성/etc/systemd/system/gunicorn.service 파일을 아래와 같은 내용으로 생성.pipenv는 venv와 ExecStart 경로가 다르다는 점을 유념해 작성하자[Unit]Description=gunicorn daemonAfter=network.target[Service]User=ec2-userGroup=ec2-userWorkingDirectory=/home/ec2-user/django/repoExecStart=/usr/local/bin/pipenv run gunicorn --workers 3 \\        &lt;wsgi가 위치한 폴더&gt;.wsgi:application --bind 0.0.0.0:8000[Install]WantedBy=multi-user.target  본래 --bind부분에 unix:/home/ec2-user/django/gunicorn.sock 를 넣어 구동하면 repo의 상위 폴더에  gunicorn.sock가 생긴다.nginx의 proxy_pass 부분도 http://unix:/{$PATH}/gunicorn.sock을 기재해 sock로 구성하는 것이 맞는 방법 같은데… 이 부분에 대해서는 학습이 필요하다.서비스 등록sudo systemctl start gunicornsudo systemctl enable gunicorn서비스 구동 확인sudo systemctl status gunicornNginx사이트 설정 추가ec2에 nginx를 받았을 때, etc/nginx/sites-enabled 와 etc/nginx/sites-availabe 이 존재하지 않는다. 해당 경로에 없다면 만들어주고 있으면 default 파일을 삭제하자.server {        listen 80;        server_name &lt;IP or 도메인&gt;;        charset utf-8;        location / {                include proxy_params;                proxy_pass http://0.0.0.0:8000        }        location /static/ {                root /home/ec2-user/django/repo;        }                location /media/ {                root /home/ec2-user/django/repo;        }}  nginx 주요 개념, nginx : root vs aliasinclude proxy_params의 경우 /etc/nginx/proxy_params 에 프록시 헤더를 기재 해야 한다. (다음 링크 참고) nginx &amp; aws사이트 추가sudo ln -s /etc/nginx/sites-available/django_test /etc/nginx/sites-enabled기동sudo systemctl start nginx기타 도움이 되는 명령어sudo systemctl daemon-reloadsudo systemctl stop, restart nginxps -efpspkill gunicorn  dotenv 관련 에러 해결하기            gunicorn 을 활용해 연결 할 경우      $ pip uninstall dotenv$ pip install python-dotenv                     docker + nginx + gunicorn 을 활용할 경우      $ pip uninstall dotenv$ pip install python-dotenv             참고자료  gunicorn 사전작업  Nginx, Gunicorn, Django 연동하기",
        "url": "/pipenv01"
    }
    
    
    };
</script>
<script src="assets/js/lunr.js"></script>
<script src="assets/js/search.js"></script>

            </section>

        </article>

    </div>
</main>

<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->
<script>
$(function() {
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
});
</script>



        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="https://heuristicwave.github.io/">Heuristic Wave Blog</a> &copy; 2023</section>
                <section class="poweredby">Proudly published with <a href="https://jekyllrb.com/">Jekyll</a> &
                    <a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a> using
                    <a href="https://github.com/jekyller/jasper2" target="_blank" rel="noopener">Jasper2</a></section>
                <nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    
                    
                    <a href="https://ghost.org" target="_blank" rel="noopener">Ghost</a>
                </nav>
            </div>
        </footer>

    </div>

    <!-- The big email subscribe modal content -->
    
        <div id="subscribe" class="subscribe-overlay">
            <a class="subscribe-overlay-close" href="#"></a>
            <div class="subscribe-overlay-content">
                
                <h1 class="subscribe-overlay-title">Search on Heuristic Wave Blog</h1>
                <p class="subscribe-overlay-description">lunr.js를 이용한 posts 검색</p>
                <span id="searchform" method="post" action="/search/" class="">
    <input class="confirm" type="hidden" name="confirm"  />
    <input class="location" type="hidden" name="location"  />
    <input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()"
               id="searchtext" type="text" name="searchtext"
               placeholder="Search..." />
    </div>
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
</span>
                <br>
            </div>
        </div>
    

    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.10.0/components/prism-abap.min.js"></script>
    <script>$(document).ready(function() {
      $('pre code').each(function(i, block) {
        hljs.highlightBlock(block);
      });
    });</script>

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://demo.ghost.io/assets/js/jquery.fitvids.js?v=724281a32e"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google Analytics Tracking code -->
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'G-0FTXSPJZFY', 'auto');
  ga('send', 'pageview');

 </script>


    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>
$(function() {
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
});
</script>

    

    <!-- Ghost outputs important scripts and data with this tag - it should always be the very last thing before the closing body tag -->
    <!-- ghost_foot -->

</body>
</html>
