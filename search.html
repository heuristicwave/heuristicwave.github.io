<!DOCTYPE html>
<html>
<head>

    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>Search Result</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />

    <!-- Custom.css -->
    <link rel="stylesheet" type="text/css" href="/assets/built/custom.css" />

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">

    <!-- webfont -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/earlyaccess/nanumgothic.css">

    <!-- syntax.css -->
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />

    <!-- highlight.js -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>

    <!--[if IE]>
        <style>
            p, ol, ul{
                width: 100%;
            }
            blockquote{
                width: 100%;
            }
        </style>
    <![endif]-->
    
    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="Careful Writer" />
    <link rel="shortcut icon" href="https://heuristicwave.github.io/assets/built/images/water-wave-48.png" type="image/png" />
    <link rel="canonical" href="https://heuristicwave.github.io/search" />
    <meta name="referrer" content="no-referrer-when-downgrade" />

     <!--title below is coming from _includes/dynamic_title-->
    <meta property="og:site_name" content="Heuristic Wave Blog" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Search Result" />
    <meta property="og:description" content="Careful Writer" />
    <meta property="og:url" content="https://heuristicwave.github.io/search" />
    <meta property="og:image" content="https://heuristicwave.github.io/assets/built/images/blog-cover.jpg" />
    <meta property="article:publisher" content="https://www.facebook.com/" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Search Result" />
    <meta name="twitter:description" content="Careful Writer" />
    <meta name="twitter:url" content="https://heuristicwave.github.io/" />
    <meta name="twitter:image" content="https://heuristicwave.github.io/assets/built/images/blog-cover.jpg" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Heuristic Wave Blog" />
    <meta name="twitter:site" content="@" />
    <meta name="twitter:creator" content="@" />
    <meta property="og:image:width" content="2000" />
    <meta property="og:image:height" content="666" />

    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "Heuristic Wave Blog",
        "logo": "https://heuristicwave.github.io/"
    },
    "url": "https://heuristicwave.github.io/search",
    "image": {
        "@type": "ImageObject",
        "url": "https://heuristicwave.github.io/assets/built/images/blog-cover.jpg",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://heuristicwave.github.io/search"
    },
    "description": "Careful Writer"
}
    </script>
    <script data-ad-client="ca-pub-6093187208665634" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <!-- <script type="text/javascript" src="https://demo.ghost.io/public/ghost-sdk.min.js?v=724281a32e"></script>
    <script type="text/javascript">
    ghost.init({
    	clientId: "ghost-frontend",
    	clientSecret: "f84a07a72b17"
    });
    </script> -->

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-0FTXSPJZFY"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-0FTXSPJZFY');
    </script>
    <meta name="generator" content="Jekyll 3.6.2" />
    <link rel="alternate" type="application/rss+xml" title="Search Result" href="https://heuristicwave.github.io/feed.xml" />


</head>
<body class="page-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- < default -->
<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<!-- The big featured header, it uses blog cover image as a BG if available -->
<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        
            
                <a class="site-nav-logo" href="https://heuristicwave.github.io/">Heuristic Wave Blog</a>
            
        
        
            <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-about" role="menuitem"><a href="/about/">About</a></li>
    <li class="nav-extracurricular" role="menuitem"><a href="/tag/extracurricular/">Extracurricular</a></li>
    <li class="nav-devops" role="menuitem"><a href="/tag/devops/">DevOps</a></li>
    <li class="nav-aws" role="menuitem"><a href="/tag/aws/">AWS</a></li>
    <li class="nav-backend" role="menuitem"><a href="/tag/backend/">Back-end</a></li>
    <li class="nav-archive" role="menuitem">
        <a href="/archive">All Posts</a>
    </li>
    <li class="nav-archive" role="menuitem">
        <a href="/author_archive.html">Tag별 Posts</a>
    </li>
</ul>

        
    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
            
        </div>
        
            <a class="subscribe-button" href="#subscribe">Search</a>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full  post page no-image">

            <header class="post-full-header">
                <h1 class="post-full-title">Search Result</h1>
            </header>

            

            <section class="post-full-content">
                <form action="/search" method="get" hidden="hidden">
    <label for="search-box"></label>
    <input type="text" id="search-box" name="query">
</form>

<ul class="mylist" id="search-results"></ul>

<script>
    window.store = {
    
    "terraformtips3": {
        "title": "Terraform Tips 3 - Refresh &amp; Replace",
            "author": "HeuristicWave",
            "category": "",
            "content": "Terraform 더 익숙하게 3 - Refresh &amp; ReplaceIntroIaC(Infrastructure as Code)를 운용하며 중요하게 생각하는 포인트 중 하나는, 코드로 정의한 형상과 실제 인프라의 형상을 동일하게 유지하는 것입니다.Terraform에서는 Configuration Drift(정의한 형상과 달라지는 경우)를 방지하기 위해 다양한 명령어를 제공합니다.이번 포스팅에서는 형상을 유지하는 다양한 기법 중 하나인 Refresh와 비슷하면서도 헷갈리는 개념인 Replace에 대하여 알아보겠습니다.♻️ Refresh문서에는 다음과 같이 기재되어 있지만, 처음 접한다면 무엇을 말하는지 쉽게 와닿지 않습니다.  The terraform refresh command reads the current settings from all managed remote objects and updates the Terraform state to match. terraform refresh 명령어는 원격 객체의 현재 상태를 읽어 Terraform state와 일치시킵니다.클라우드 환경에서 클러스터를 운용하면 인스턴스의 Scale이 변화함에 따라 인스턴스 ID 값도 변합니다.이 경우 코드로 정의한 상태는 프로비저닝 당시 시점을 기억하지만, 실제 인프라의 현상은 최신 인스턴스의 상태를 가지고 있으므로 Drift가 발생합니다.위와 같은 상황에서 refresh 명령어를 사용하면 terraform.tfstate의 값이 현재 인프라의 상태와 일치하게 됩니다.하지만, 위 명령어는 deprecate 되었습니다. 왜냐하면 관리자가 무엇이 변경되는지 알지 못하고 tfstate가 최신화되기 때문입니다.그래서 테라폼 v0.15.4.에서부터는 plan과 apply에 -refresh-only 옵션을 제공하기 시작했습니다.–refresh-only왼쪽은 프로비저닝 당시 *.tfstate입니다. 오른쪽은 해당 환경을 콘솔에서 health check를 하는 간격을 120초로 변경한 화면입니다.의도적으로 Drfit 상황을 만들었기 때문에 terraform plan --refresh-only 명령어로 점검하면 다음과 같이 어떤 리소스 객체가 변경될 것인지 확인 가능합니다.위 사진에서, elb의 AutoScalingGroup 내의 인스턴스가 바뀌어 최신화됨과, 기존에 콘솔에서 변경한 health_check 값을 확인할 수 있습니다.이어서 해당 차이를 terraform apply --refresh-only 명령어로 *.tfstate를 최신화 시킵니다.⚠️ 주의*.tfstate는 Refresh로 최신화되었지만, 여전히 코드로 정의한 interval 값은 180입니다.그러므로 코드로 정의된 부분을 변경하지 않은 상황에서 terraform apply 명렁어를 치면, 현재 120의 interval 값이 180으로 원복 됩니다.☠️ TaintReplace를 설명하기 앞서, 기존 테라폼에는 taint라는 명령어가 있습니다. 문서에는 다음과 같이 기재되어 있습니다.  The terraform taint command informs Terraform that a particular object has become degraded or damaged. terraform taint 명령어는 특정 객체가 저하되거나 손상되었음을 Terraform에 알립니다.운영을 하다 보면 인프라를 정의한 코드는 그대로인 상태에서 리소스만 교체할 경우가 발생합니다.저의 경우 스파이크성 트래픽을 갑자기 받아 로드밸런서의 성능이 저하되었을 때, ELB를 교체한 경험이 있습니다.이런 상황에서 성능이 저하된 혹은 교체가 필요한 리소스 객체만을 on/off 방식으로 주석 처리 및 해제하며 apply 명령어로 교체할 수 있지만,taint(교체 리소스를 마킹) 명령어로 교체하고 untaint(교체 리소스 마킹 해제) 명령어로 교체할 필요가 없다고 명령할 수 있습니다.그러나 위와 같은 워크플로우는 테라폼 v0.15.2.에서 deprecate 되었고, -replace 옵션을 제공하며 더 직관적인 사용자 경험을 제공하게 됩니다.🌗 -replace-replace 옵션은 taint 명령어와 동일하게 작용하며, untaint 명령어를 칠 필요가 없습니다.$ terraform apply -replace=\"aws_instance.example[0]\"교체가 필요한 리소스 객체를 파악하기 위해 아래 state list 명령어로 target을 찾아 -replace 옵션과 함께 지정하면,코드는 그대로지만 리소스 객체가 변경되는 것을 확인할 수 있습니다.-replace 옵션 역시, --refresh-only 옵션과 동일하게 plan 명령어와 함께 적용하여 변경 지점을 미리 파악변경되는 리소스에 대한 검토를 하는 습관을 들입시다!OutroRefresh와 Replace 비슷하면서도 전혀 다른 두 명령어의 변천사를 확인하며 인프라의 동일한 형상을 유지하기 위한 방법을 알아보았습니다.이번 포스팅에서 언급한 방법 외에도 형상을 일치시키는 방법들이 존재하지만, 위 2가지 명령어만 제대로 활용하면 대부분의 인프라의 형상이 달라지는 사태를 예방할 수 있습니다. IaC에서는 항상 형상을 변경하기 전, 변경되는 리소스에 대한 검토를 하는 습관이 매우 중요하다는 것을 강조하며 이번 3편을 마칩니다.지금까지 테라폼 더 익숙하게 Refresh &amp; Replace 편을 읽어주셔서 감사합니다! 잘못된 내용은 지적해 주세요! 😃Terraform Tips    Module &amp; Output    Data &amp; Index    Refresh &amp; Replace",
        "url": "/TerraformTips3"
    }
    ,
    
    "geultto": {
        "title": "글또 7기 다짐글",
            "author": "HeuristicWave",
            "category": "",
            "content": "글또 7기 다짐글 입니다.글또를 시작하며… 🏃🏻저는 최근 ‘글 쓰는 개발자 모임, 글또’ 7기로 활동할 수 있는 기회를 얻었습니다. 글또가 어떤 활동인지는 글또 페이지 에서 확인할 수 있습니다.글또의 존재는 2기 때부터 알고 있었지만, 스스로 활동을 하기에는 아직 부족하다 생각되어지난 1년간 한 달에 글 1편씩을 작성하는 습관을 만들고 나서 7기에 가입하게 되었습니다.활동은 올해 5월부터 10월까지 약 6개월간 활동하게 되는데요, 첫 미션으로 다짐글 작성이라는 주제를 받았습니다.그 동안 이 기술 블로그에 개인적인 얘기는 담지 않을뿐더러,평상시에도 일기와 같은 자신의 마음(다짐 포함)을 적어 본 적이 없어 글을 써 내려가기가 부끄럽네요.🧩 계획계획하고 있는 콘텐츠는 다음과 같습니다.      기술 도서 리뷰 현재 제 책상 위에는 구입해두고 읽지 못한 책들이 꽤나 쌓였습니다.‘컨테이너 보안’, ‘Go 언어를 활용한 네트워크 프로그래밍’ 등 밀린 기술 서적 부채(?)를 청산하도록 노력하겠습니다.        기존 시리즈물 마감 작년 제 블로그에 ‘테라폼 더 익숙하게’ 라는 시리즈물을 연재하기로 하고 게으른 탓 2편 밖에 작성하지 못했습니다.머릿속에만 남아있는 후속 글 들을 작성해 시리즈물 다운 글을 작성해 보도록 하겠습니다.     아직 2편 밖에 없는 작고 초라한 나의 시리즈물        관심 기술 스터디 Kubernetes, Terraform, Istio, AWS 서비스들에 대한 글을 작성하며, 성장의 기록들을 남기겠습니다.        주제를 추천받아 작성 저는 어떻게 보면 글을 작성하는 것보다, 주제를 선정하는 게 더욱 어려운 것 같습니다.글또를 통해 비슷한 직군의 엔지니어 분들이 고민하는 혹은 알고 싶은 주제들이 무엇인지 파악해 관련 글을 작성하고 싶습니다.  ✋ 다짐자발적 번아웃 🔥글또에는 여러 기수를 걸쳐 지속적으로 활동하고 계신 분들이 꽤나 많습니다.저도 이번 7기가 끝이 아니라 지속적인 활동으로 글또를 이어가고 싶습니다.그렇지만, 7기 활동이 끝날 무렵 회고를 하는 시점에서 7기 활동 간 생산된 12편의 글로 인하여 후회 없는 활동을 하고 싶습니다.다시 바꾸어 말하면, “너무 힘들어서 8기는 쉬어야겠다.” 싶을 정도의 감정을 느끼도록 열심히 활동하고 싶습니다.부끄러움을 늦추는 글의 유효기간 🙈이 페이지 를 보면 제가 한 해 동안, 작성한 글들이 나옵니다. 당시에도 한편 한편 공들여 작성하며 “이정도면 꽤나 괜찮은 글 아닐까?” 라는 생각을 종종했었는데,시간이 지난 시점에 다시 읽어보면 부끄럽다는 생각이 들때가 있습니다.온라인 어디선가 본 내용인데, 좋은 글을 작성한 것으로 평가 받는 작가들도 과거 자신이 작성한 글을 보면 부끄럽다고 합니다.이런 것들을 보면 자신의 글을 부끄러워 하는게 당연한 것일 수도 있는데, 다음과 같은 생각이 들었습니다.위 그래프에서 보이다시피, 글의 완성도와 부끄러움의 발현 시기는 양의 상관관계를 갖고 있다고 생각합니다.작년에 제가 작성한 글 들의 경우, 아무리 길어도 대략 한 계절정도 지나면 부끄러움이 스멀스멀 올라오는것 같더군요.그래서, 이번 활동 기간 동안에는 과거 제가 썻던 글보다 더 부끄러움이 오는 시기가 늦는 글을 작성해 보려합니다.활동기간이 약 6개월 정도되니 아마 5월 말에 쓰는 글에 대한 부끄러움의 정도를 7기 활동이 끝날 무렵인 회고 때 다뤄보면 좋을 것 같습니다.",
        "url": "/geultto"
    }
    ,
    
    "export-data-to-s3-lambda": {
        "title": "Export cloudwatch log data to Amazon S3 using lambda",
            "author": "HeuristicWave",
            "category": "",
            "content": "본 글은 Cloudwatch Logs groups의 데이터를 Lambda를 사용해 주기적으로 S3로 export 하는 방법을 다룹니다.Intro글을 시작하기 앞서, Shraddha Paghdar - Export AWS CloudWatch logs to S3 using lambda functions in Node.js를 참고하여 해당 글을 작성했음을 알립니다. 본격적으로 방법을 소개하기 앞서, Cloudwatch Logs groups의 데이터를 Lambda를 사용해 주기적으로 S3로 export 하기 위한 플로우는 다음과 같습니다.  Amazon EventBridge에 정의한 Rule에 의해 Lambda를 호출합니다.  이후 람다가 지정한 CloudWatch의 Logs에 적재된 데이터를 찾습니다.  2번에 연속되어 이관될 대상의 로그들이 S3로 이관됩니다.📜 Workshop1️⃣ Log를 담을 Bucket 및 정책 생성하기Amazon S3 콘솔 화면에 접속해 Log들이 담길 Bucket을 생성하고 이어서 Permissions에서 S3 버킷에 대한 권한을 설정합니다.해당 방법은 공식 문서, 1단계와 3단계 자세히 설명되어 있습니다.  💡 공식 문서 3단계에서 기재된 random-string은 필요에 의한 경우 사용하세요. 해당 글에서는 편의상 생략하였습니다.2️⃣ Lambda 생성하기Step 1 : Lambda가 사용하는 IAM Role &amp; Policy 생성Lambda가 Log를 Export 할 수 있도록 다음 정책을 생성합니다.IAM에서 Create policy를 선택하고 아래 JSON을 복사하여 붙여 넣고, cloudwatch_export_task라는 이름으로 정책을 생성합니다.{    \"Version\": \"2012-10-17\",    \"Statement\": [        {            \"Effect\": \"Allow\",            \"Action\": \"logs:CreateExportTask\",            \"Resource\": \"arn:aws:logs:{Region}:{AccountNumber}:*\"        }    ]}Lambda에 권한을 부여해 주기 위해서 IAM &gt; Roles &gt; Create Role을 선택합니다.Use case로 Lambda를 선택하고 앞서 생성한 정책을 부여한 뒤, export_S3_lambda라는 이름으로 Role을 생성합니다.Step 2 : 코드 작성Lambda 콘솔 화면에서 아래와 같이 빈칸을 채우고, Step 1에서 만들어둔 role을 부여해 람다 함수를 생성합니다.  💡 만약, Export 역할을 수행하는 람다 함수가 생성하는 로그를 수집하고 싶을 경우에는 Create a new role with basic Lambda permissions을선택하고 Console에 의해 자동적으로 생성되는 Role에 Step 1에서 만든 정책을 부여하면 됩니다.이어서 아래 코드를 복사하여 상황에 맞는 인자 값을 넣어주고 Deploy 합니다.Parameter  region : 로그 그룹과 대상 버킷은 동일 리전에 위치  destination : 로그가 이관되는 대상 버킷  logGroupName : Cloudwatch Log group 이름  destinationPrefix : 1️⃣ 에서 언급한 random-string 값  from/to : Lambda 함수가 호출 되는 시점을 기준으로, from/to 기간의 로그 그룹들을 exportconst AWS = require('aws-sdk')const cloudconfig = {  apiVersion: '2014-03-28',  region: 'region',}const cloudwatchlogs = new AWS.CloudWatchLogs(cloudconfig)exports.handler =  async (event, context) =&gt; {   const params = {      destination: 'bucket-name',      logGroupName: 'log-groups-name',      destinationPrefix: '',      from: new Date().getTime() - 86400,      to: new Date().getTime(),  };await cloudwatchlogs.createExportTask(params).promise().then((data) =&gt; {    console.log(data)    return ({      statusCode: 200,        body: data,    });  }).catch((err) =&gt; {    console.error(err)    return ({      statusCode: 501,        body: err,    });  });}이후 테스트 버튼을 눌러 결과값을 보면 taskId 값이 생성되고 S3에 로그가 이관된 것을 확인할 수 있습니다.3️⃣ EventBridge 트리거 생성하기작성한 람다 함수 콘솔 화면 상단에서 Add trigger 버튼을 눌러 다음과 같이 Rule을 생성합니다. 저는 주기를 점검하기 위해 아래와 같이 5분을 주었습니다.  💡 스케쥴 표현식 작성법Outro지금까지 Lambda 함수와 EventBrdige를 사용하여 자동으로 로그를 S3으로 백업하는 방법을 알아보았습니다.해당 방법 외에도 로그를 이관하는 다양한 방법들이 있으므로, 더 쉽고 좋은 방법이 있다면 알려주세요!소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해 주세요! 😃",
        "url": "/Export_data_to_S3_Lambda"
    }
    ,
    
    "ecs-cw-logs": {
        "title": "Using the awslogs log driver in ECS(Fargate)",
            "author": "HeuristicWave",
            "category": "",
            "content": "ECS Task의 컨테이너가 생산하는 로그들은 CloudWatch를 활용하여 수집할 수 있습니다.Cloudwatch Logs를 운영하며 로그 적재가 제대로 되지 않거나, Timestamp가 일치하지 않거나, 지나친 지연 시간이 발생하거나, 알아보기 어려운 형태로 로그가 쌓인다면 아래 요소들을 고민해 볼 수 있습니다.📚 References  Using the awslogs log driver  Regular expression WikipediaFargate에서 필요한 awslogs 로그 드라이버공식 문서에서는 awslogs-region, awslogs-group 만이 필요하다고 하지만, Fargate를 사용하는 경우 awslogs-stream-prefix이 추가적으로 필요합니다.또한, 가시성을 확보하기 위해 CloudWatch Logs에 수집된 여러 줄의 로그를 하나의 메시지로 보기 위해서는 awslogs-multiline-pattern이 필수적으로 필요합니다.awslogs-multiline-pattern공식 문서의 Note 부분을 보면 다음과 같은 메모를 확인할 수 있습니다. (정말 공식 문서는 한 줄도 그냥 지나칠 수 없는 것 같습니다!)  Multiline logging performs regular expression parsing and matching of all log messages.This may have a negative impact on logging performance.실제로 저는 정규 표현식을 간과하고 검증되지 않은 정규식들을 적용했다가 다음과 같은 Negative Impact를 만났습니다.  로그가 수집되기까지의 지나친 지연 시간 발생 (10분 이상)  지연시간으로 인한 Timestamp 불일치 (Ingestion time과 Event Timestamp의 과도한 오차)  1, 2번 이유로 인한 로그 미수집Regular Expression Lab지금부터 예시들을 통해, CW Logs를 운영하며 만날 수 있는 상황들을 체험해 보겠습니다.  샘플 로그를 복사하여 RegExr에서 match 여부를 테스트해 볼 수 있습니다.Case 1️⃣awslogs-multiline-pattern의 value로 ^INFO를 설정할 경우 3개의 Line 중 match 되는 라인은 몇 라인일까요?INFO | (pkg/trace/info/stats.go:104 in LogStats)                # Line 1INFO | (pkg/trace/info/stats.go:104 in LogStats)                # Line 212:15:10 UTC | INFO | (pkg/trace/info/stats.go:104 in LogStats) # Line 3  🖍 정답 보기      INFO | (pkg/trace/info/stats.go:104 in LogStats)                # Line 1    ^(caret) 은 전체 문자열의 시작 위치에만 일치하므로, Line 1 만이 match 됩니다.Case 2️⃣다음은 시:분:초를 표현하는 정규 표현식 입니다. 해당 정규 표현식은 아래 3줄을 모두 Match 시킬 수 있을까요? (0[1-9]|1[0-9]|2[0-4]):(0[1-9]|[1-5][0-9]):(0[1-9]|[1-5][0-9])07:36:35 | Which line will match? Line 108:00:01 | I am Line 2!08:01:00 | I am Line 3!  🖍 정답 보기      07:36:35 | I was matched 08:00:01 | I was not matched! 08:01:00 | I was not matched!     그렇다면 왜? 첫 번째 라인만이 매칭되었을까요? 분, 초에 해당하는 표현식을 유심히 살펴보면 00분 00시는 매칭되지 않습니다.때문에 각각 (분 : (0[0-9]|[1-5][0-9]), 초 : (0[0-9]|[1-5][0-9]))로 수정해야 위 3줄을 매칭 시킬 수 있습니다.Other Case위 2가지 케이스만 준비된다면 모든 로그들을 제대로 분리하여 수집할 수 있을까요?  🤔 생각해보기      Flag가 INFO 형식이 아닌 WARN이 발생할 경우    Timestamp로 매칭 작업을 하는데 한 줄에 1회 이상 Timestamp가 포함된 경우              ex) 08:00:01 | It’s 08:02:03 right now.              Application Crash로 인한 예상치 못한 메시지가 포함될 경우    awslogs 로그 드라이버 내의 우선순위  아마도 위에 기재한 것들 외에도 더 고려 할 것들이 많을 것 같습니다.개발이나 알고리즘 문제를 풀 때와 마찬가지로 항상 예상치 못한 실패 지점을 예상하는 습관이 필요한 것 같습니다.ECS Error Handling and Troubleshooting    Using Amazon ECS Exec for debugging    Using the awslogs log driver in ECS(Fargate)",
        "url": "/ECS_CW_Logs"
    }
    ,
    
    "ecs-exec": {
        "title": "Using Amazon ECS Exec for debugging",
            "author": "HeuristicWave",
            "category": "",
            "content": "Docker에서는 exec 명령어를 통해 실행중인 컨테이너에 접속하여 디버깅이 가능하다.21년 3월 부터 해당 기능이 AWS의 ECS에서도 사용가능하게 되었는데, 해당 기능을 사용하며 만났던 문제들을 기록.📚 References  Using Amazon ECS Exec for debugging  AWS CLI Cmd Ref : excute-commnadExec 활성화  SSM 에이전트와 SSM 서비스 간의 통신에 필요한 권한 부여  task-definition config 추가    \"linuxParameters\": {    \"initProcessEnabled\": true}        CLI로 execute-command enable 후, 점검    aws ecs create-service \\ --cluster cluster-name \\ --task-definition task-definition-name \\ --enable-execute-command \\ --service-name service-name \\ --desired-count 1        아래 명령어로 활성화 여부 확인. (grep option 활용, grep -F4 \"managedAgents\", grep \"enableExecuteCommand\")    aws ecs describe-tasks \\ --cluster cluster-name \\ --tasks task-id         활성화 상태일 때의 Snippet    {    \"tasks\": [        {            ...            \"containers\": [                {                    ...                    \"managedAgents\": [                        {                            \"lastStartedAt\": \"2021-03-01T14:49:44.574000-06:00\",                            \"name\": \"ExecuteCommandAgent\",                            \"lastStatus\": \"RUNNING\"                        }                    ]                }            ],            ...            \"enableExecuteCommand\": true,            ...        }    ]}        Running ECS Exec    aws ecs execute-command --cluster cluster-name \\ --task task-id \\ --container container-name \\ --interactive \\ --command \"/bin/sh\"      문제 해결Exec 명령어 이후 에러 핸들 (공식 문서들에 답이 다 있엇다 😂)An error occurred (ClusterNotFoundException) when calling the ExecuteCommand operation: Cluster not found.  Cluster ARN 기입 (From AWS CLI Ref : The Amazon Resource Name (ARN) or short name of the cluster from AWS CLI Ref)SessionManagerPlugin is not found. Please refer to SessionManager Documentation here: http://docs.aws.amazon.com/console/systems-manager/session-manager-plugin-not-found  클라이언트 PC에 SSM Plugin 설치 (From AWS Docs : Prerequisites for using ECS Exec)ECS Error Handling and Troubleshooting    Using Amazon ECS Exec for debugging    Using the awslogs log driver in ECS(Fargate)",
        "url": "/ECS_Exec"
    }
    ,
    
    "imagebuilder": {
        "title": "Create Immutable Server using AWS Image Builder &amp; Auto Scaling Group",
            "author": "HeuristicWave",
            "category": "",
            "content": "본 글은 2021년 4월 30일 EC2 Image Builder supports Auto Scaling launch template에 소개된 기능을 활용해 Image Builder로 Golden AMI를 만들고,해당 이미지로 Auto Scaling Group의 launch template을 업데이트하는 방법을 소개합니다.Intro클라우드를 다루다 보면 Netflix 가 여러 분야에서 앞장서 있는 것을 확인할 수 있습니다.그중에서 Immutable Server Pattern을 알리는데 기여한 2013년에 발간된 AMI Creation with Aminator를 꼭 한번 읽어보시기 바랍니다. 과거 넷플릭스는 AWS의 이미지 파일을 생성하기 위해 Aminator라는 도구를 사용했지만,본 포스팅에서는 이미지 빌더로 AMI를 만들고 Auto Scaling Group에 적용시키는 방법을 배워 Immutable Server Pattern을 학습해 보겠습니다.Immutable ServerImmutable Server 대한 내용은 martin fowler 블로그에 소개된 ImmutableServer글을 보시면 굉장히 잘 설명되어 있습니다. 요약 + 그동안의 제 뇌피셜을 함께 말씀드리면 다음과 같습니다.서버를 운영하다 보면 업데이트를 비롯한 다양한 이슈들로 인해 구성요소가 자주 변하게 됩니다.그렇게 운영하다 보면 무언가 꼬여 서버를 재 생성하는 일이 발생하는데 아래와 같은 라이프 사이클을 만나게 됩니다.이런 상황에서 구성이 다른 여러 대의 서버를 가지게 되면 Configuration drift가 발생했다고 표현합니다.또한 위 그림처럼 자꾸 새로운 이미지로 회귀하니까, Shift Left라고도 표현하고 이를 한국어로 표현하는 말을 찾아보니 원점 회귀라는 표현도 쓰입니다.구성 변경이 잦은 서버는 깨지기(fragile) 쉽고, 또 장애가 발생할 경우 재현이 어려울뿐더러 변경 시 연쇄적인 장애를 유발할 가능성이 있습니다.이를 두고 깨지기 쉬운 눈송이 같다 하여 Snowflake Server라 부릅니다.이 상황을 피하기 위해, Configuration drift를 피하는 Base Image(Configuration이 발생하면 새롭게 생성)를 활용한 방법을 Phoenix Server라 부릅니다. (Phoenix Server 용어는 마틴 파울러의 동료 Kornelis Sietsma 가 제안했다 합니다.)피닉스 서버의 개념은 기본 이미지(Base AMI, Golden AMI)를 통해 언제나 일관성 있는 환경을 제공했고,이러한 개념들이 자연스럽게 Immutable Server의 개념으로 이어졌습니다.(최근에는 Immutable Server를 넘어 Immutable Infrastructure의 개념도 있습니다!)이미지 빌더와 오토 스케일 그룹으로 이뮤터블 서버 구축하기클라우드에서의 Immutable Server Pattern 검색하면 아래와 같은 좋은 예시들이 나옵니다.  Create immutable servers using EC2 Image Builder and AWS CodePipeline  Tutorial: Immutable infrastructure for Azure, using VSTS, Terraform, Packer and Ansible  Provision Infrastructure with Packer  하시코프 튜토리얼을 보고 만든 필자의 블로그 🥲그러나 이제부터 다룰, 이미지 빌더와 오토 스케일 그룹으로 이뮤터블 서버 패턴을 만족시킬 수 있습니다.AWS에서 제공하는 AutoScale은 탄력적인 확장과 축소를 제공하지만 Desired Capacity 기능을 활용해 항상 동일한 서버의 수를 유지할 수 있습니다.  Image Builder의 Image pipelines를 활용해 원본 AMI로부터 원하는 형태의 Output(Custom/Golden/Base AMI)을 제작합니다.  Distribution settings을 통해 lt(Launch Template)에 1번에서 생성한 AMI로 교체하여 새로운 버전을 만듭니다.  이제, 새롭게 생성되는 ASG(Auto Scaling Group)은 새롭게 버전 업된 lt를 통해 인스턴스를 생성합니다.3번의 ASG의 경우, Refresh를 하기 전까지는 이전 상태의 lt를 기준으로 인스턴스가 운영되고 있습니다.이미지 생성과 동시에 새로운 AMI로 EC2 Refresh를 할 수 있는 방법이 있지만,새롭게 생성된 인스턴스가 운영환경에 바로 적용되는 것은 바람직하지 않으므로 해당 부분에 대한 자동화는 제외하였습니다.만약 검증된 AMI를 생성한다면, Lambda 혹은 기타 방법 등을 통해 Refresh 하여 이미지 생성부터 배포까지 자동화할 수 있습니다.추후 다른 포스팅에서 자세히 다룰 예정이지만, 이러한 패턴은 Immutable Infrastructure의 한 요소를 이루기도 합니다.📜 Workshop0️⃣ Launch Template &amp; Auto Scaling groups 생성기존 환경에 0️⃣이 준비되어 있다면 다음 단계인 Distribution settings으로 넘어가도 좋습니다.해당 단계는 제가 굉장히 게으른 관계로 AWS Documentation : Creating an Auto Scaling group using a launch template으로 대체하겠습니다.  💡 만약 기존 환경이 Launch Template이 아닌 Launch Configurations으로 구성되어 있을 경우,콘솔 화면에서 Copy to launch template 버튼을 누르시면 손쉽게 lt로 변경 가능합니다. 💡 Launch Template과 Launch Configurations는 굉장히 유사하지만, Launch Template의 경우 더 다양한 기능들을 제공합니다.특히 버전관리 기능을 통해 Rollback을 하거나, 업데이트시 ASG를 활용한 Rolling Update가 가능해 Launch Template 사용을 권장합니다.1️⃣ Distribution settingsEC2 Image Builder 콘솔 화면에 접속해 Distribution settings에서 새로운 세팅을 생성합니다.필수 항목인 이름을 작성하고 Region settings에서 배포할 리전을 확인한 후,하단의 Launch template configuration에서 Step 0️⃣에서 작성한 lt를 지정하고 create settings로 생성합니다.2️⃣ Image pipelinesStep 1Distribution settings 작성이 완료되었다면, 콘솔에서 Image pipelines에 접속하여 아래 정보들을 기재합니다.아래 사진의 좌측 Step 5까지의 과정을 거치면 목표로 했던 환경이 완성됩니다.Build schedule에서는 주기적으로 파이프라인을 실행할 수 있는 방법들을 제공하는데 운영자가 원하는 방식으로 설정할 수 있습니다.Step 2Choose recipe 단계에서는 기존에 만들어둔 recipe가 없으므로 Create new recipe를 선택하고, Image type으로 AMI를 선택합니다.다음 Base image를 고르는 단계에서는 아래 화면과 같이 관리형 이미지를 사용하거나 기존에 작성한 Custom AMI ID를 사용해도 됩니다.그다음 Instance configuration와 Working directory에서는 기본 값으로 둬도 상관없지만SSM, User data, Working directory path의 필요 여부에 따라 활용하시면 됩니다.이어서 Components에서는 Golden AMI를 구축하기 위해 선행되어야 하는 각종 Agent나 소프트웨어(Apache, dotnet etc)를 선택할 수 있습니다.저는 편의상 CloudWatch Agent를 선택했습니다.그다음 이어지는 Test components, Storage, Tags 역시 필요 여부에 따라 활용하시면 됩니다.Step 33단계에 진입하면 아래 사진과 같이 이미지 빌더의 인프라 Config 값들을 정의할 수 있는 공간이 나옵니다.3번째 버튼인 Create New infrastructure configuration으로 직접 인스턴스 유형, 네트워크, SNS topic을 설정이 가능하지만,저는 1번 Create infrastructure configuration using service defaults로 기본 구성 값들을 잡아주었습니다.(IAM Role과 SNS Topic도 자동으로 생성해 주고 굉장히 편리하네요.)Step 4드디어, 이전 1️⃣ Distribution settings 과정에서 만들어둔 배포 설정 해당 단계에서 선택합니다.Step 5Review 단계까지 구성 요소들을 검토해 보고 Create pipeline을 누르고 Image pipelines 콘솔로 돌아오면 아래와 같은 화면을 만나게 됩니다.아래 캡처화면의 경우 이미 빌드가 끝난 상태이지만, 테스트를 위해서 Actions - Run pipeline 단계를 거치면,ec2 콘솔에서 image build를 위한 builder 인스턴스가 생성되고 종료되는 것을 과정을 확인할 수 있습니다.3️⃣ Review  EC2 - Images - AMIs에서 새롭게 생성된 AMI 확인  EC2 - Launch templates에서 새롭게 버전이 올라간 lt 확인, 여기서 Versions 정보를 누르면 Image Builder가 생성한 Description을 확인할 수 있습니다.  제대로 작동하는지 확인하기 위해 기존의 EC2 하나를 종료시키면, 저의 경우 ASG의 Desired가 2로 설정했으므로 하나의 인스턴스가 새로운 버전으로 변경됩니다.이는 EC2 - Auto Scaling groups에서 확인할 수 있습니다.4️⃣ Clean Up분해는 조립의 역순으로?! 2️⃣ -&gt; 1️⃣ -&gt; 0️⃣ 역순으로 리소스를 정리하고 3️⃣ Review 항목을 점검하여 모든 리소스가 회수되었는지 확인합니다.Outro지금까지 각종 이야기 거리들과 EC2 Image Builder를 사용하며 Immutable Server Pattern을 학습해 보았습니다.과거 제가 Packer로 관련 환경을 구축한 적이 있는데, 역시 AWS 환경에서는 AWS의 서비스를 사용하는 게 연계도 용이하고 구축도 쉽네요.소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해 주세요! 😃📚 References글 중간중간 하이퍼 링크로 Reference 들을 달아두었습니다.특히, 이번 포스팅 레퍼런스들은 좋은 내용들이 많으니 원본들을 읽어보시고 당시 제가 느낀 기쁨들을 함께 느낄수 있으면 좋겠습니다.",
        "url": "/ImageBuilder"
    }
    ,
    
    "terraformtips2": {
        "title": "Terraform Tips 2 - Data &amp; Index",
            "author": "HeuristicWave",
            "category": "",
            "content": "Terraform 더 익숙하게 2 - Data &amp; IndexIntro이번 포스팅은 Tip이라 하기에는 부끄러운 사소한 지식이지만, 제가 자주 실수 하는 내용이라 글로 남기게 되었습니다. Terraform Data를 잘 활용하면 디스크 이미지, 코드로 정의한 다양한 리소스 및 클라우드 공급자 API에서 가져온 정보들을 알 수 있습니다.모든 Data Sources가 동일한 방법으로 간편하게 조회할 수 있으면 좋겠지만, 막상 사용하려고 하면 이런 저런 문제들을 만나게 됩니다.공식문서(Tutorial : Query Data Sources) 에서도 Data 활용방법을 배울 수 있지만,이번 포스팅에서는 3가지 예제와 함께 리소스를 Query 하는 방법을 배워 보겠습니다.Query AMIAWS의 최신 AMI는 주기적으로 갱신됩니다. 따라서 재사용 가능한 코드를 작성하기 위해, 항상 최신 AMI를 참조하는 코드를 작성하는데 다음과 같은 방법을 사용할 수 있습니다.data \"aws_ami\" \"amazon_linux\" {  most_recent = true  owners = [\"amazon\"]  filter {    name   = \"name\"    values = [\"amzn2-ami-kernel-*-hvm-*-x86_64-gp2\"]  }}output \"name\" {  value = data.aws_ami.amazon_linux.id}위와 같은 방법으로 filter와 owners 값을 조정하며 어떤 이미지든지 id 값(output)을 얻어 낼 수 있습니다.예를 들어 ECS의 Optimized AMI를 사용하는 경우, 다음과 같은 filter 값을 줄 수 있습니다.  values = [“amzn2-ami-ecs-hvm-*-x86_64-ebs”]ECS와 달리 EKS는 AMI 명명 규칙이 약간 달라 filter 기능을 활용해야, 조건에 맞는 Optimized 이미지를 얻을 수 있습니다.EKS 이미지의 경우 모든 이미지의 첫 문자열이 amazon-eks-으로 시작하기 때문에, *를 함께 넣어 Optimized AMI를 얻을 수 있습니다.data \"aws_ami\" \"amazon_linux_eks\" {  most_recent = true  owners = [\"amazon\"]  filter {    name   = \"name\"    values = [\"amazon-eks-*\"]  }  filter {    name   = \"architecture\"    values = [\"x86_64\"]  }}그러나 위 Quert의 결과 값을 공식 문서 에 기재된 AMI ID와 비교해 보면,x86 ID가 아닌, x86 가속 ID 값과 일치한 다는 것을 알 수 있을 겁니다.x86 가속 gpu가 사용 가능한 Optimized AMI입니다. gpu를 사용하는 노드의 명명 규칙이 amazon-eks-gpu로 시작하기 때문에 위 filter 조건으로는 gpu 노드가 조회됩니다.그렇다면 일반 x86 노드는 어떻게 조회해야 할까요? 어느 문서에도 기재되어 있지 않지만,대략적인 명명 규칙을 유추하여 보니 일반 EKS 노드는 다음과 같은 필터를 사용해야 한다는 것을 알게 되었습니다. 🧐  values = [“amazon-eks-node-1.22-*”]위 prefix 규칙을 보니 EKS의 버전도 prefix 안에 포함되어 이 값을 응용하면 다양한 버전의 EKS Optimized AMI를 얻을 수 있습니다!이외에도 리전마다 다른 Optimized AMI는 data에는 명시하지 않았지만, Provider에 명시한 리전에 종속성을 갖게 됩니다.또 하나의 팁을 드리자면, 만약 Filter에서 지원하지 않는 명명 규칙을 가진 AMI라면 정규식으로도 조회가 가능합니다!지금까지 실무에 자주 사용되는 다양한 AMI ID를 조회하는 방법을 알아보았습니다. 이제 어떤 AMI라도 조회가 가능하겠죠? 😎Query AZAWS의 리전마다 사용가능한 az가 다르기 때문에, 조금 더 유연한 코드를 작성하기 위해 다음 코드를 사용해 사용가능한 az를 검색합니다.이후, data.&lt;NAME&gt;.&lt;ATTRIBUTE&gt;.names 로 사용가능한 az 값들을 확인할 수 있습니다.$ data \"aws_availability_zones\" \"available\" {  state = \"available\"}$ output \"azs\" { value = data.aws_availability_zones.available.names}names에는 사용가능한 az가 배열 형태로 들어가 있어, names[0], names[1]과 같이 Index 값으로 특정 값을 지정할 수 있습니다.그러나, 모든 data가 Index 값을 가지고 있는 것은 아닙니다.Query vpc_id다른 리소스와 AWS 솔루션들을 연계하기 위해서는 vpc_id 값이 필수적으로 들어갑니다.vpc의 id를 구하기 위해서는 다음과 같은 방법으로 id를 조회할 수 있습니다.(tags 값을 활용해 일종의 필터링을 사용할 수도 있습니다.)data \"aws_vpcs\" \"vpcs\" {    tags = {        Name = var.vpc_name    }}output \"vpc_id\" {    value = data.aws_vpcs.vpcs.ids}위 코드로 다음과 같이 Output 값을 얻을 수 있지만, 하나의 az 값을 얻을때와 동일한 방식으로 ids[0] 형식으로 값을 조회하려 하면,“This value does not have any indices.” 라는 에러 메시지와 함께 출력을 지원하지 않습니다.Changes to Outputs:  + vpc_id = [      + \"vpc-0x1234567890\",    ]도대체 무엇이 잘못된 것일까요? az와 동일한 방법으로 접근했지만, 왜 지원하지 않는지는 아직까지도 모르겠습니다…누구 아시는 분이 있다면 알려주세요.count로 index 부여하기위 문제를 해결하기 위해서는 az를 검색할 때보다는 불편하지만, count를 사용해 해결할 수 있습니다.data \"aws_vpc\" \"target\" {  count = length(data.aws_vpcs.vpcs.ids)  id    = tolist(data.aws_vpcs.vpcs.ids)[count.index]}# sample code using vpc_idresource \"aws_lb_target_group\" \"sample_resource\" {  count = length(data.aws_vpcs.vpcs.ids)  # Skip Config  vpc_id      = data.aws_vpc.target[count.index].id}aws_vpcs가 아닌 aws_vpc를 추가하고 index 를 부여하기 위한 내장 함수들을 사용해 index를 부여합니다.이후, 리소스에서 data 값들을 식별하기 위한 count를 기입하고 위와 같이 index 값으로 조회가 가능합니다.Outro지금까지 Data를 활용해 각종 리소스들을 검색하는 방법을 알아 보았습니다.vpc_id도 az와 같이 별도의 index 과정 없이,간편한 조회가 가능하면 좋겠습니다. (제가 아직 방법을 모르는 것일 수도 있어요!)지금까지 테라폼 더 익숙하게 Data &amp; Index 편을 읽어주셔서 감사합니다! 잘못된 내용은 지적해 주세요! 😃Terraform Tips    Module &amp; Output    Data &amp; Index    Refresh &amp; Replace",
        "url": "/TerraformTips2"
    }
    ,
    
    "constructhub": {
        "title": "AWS CDK Library, Construct Hub",
            "author": "HeuristicWave",
            "category": "",
            "content": "본 글은 지난 AWS re:Invent 2021에서 발표된 Construct Hub를 둘러보며 느낀 첫인상에 대한 글입니다.IntroAWS re:Invent 2021, 아마존 CTO인 Dr. Werner Vogels 키노트 에서 AWS CDK 라이브러리들을 활용할 수 있는 Construct Hub 가 발표되었습니다.보통 AWS 리인벤트에서는 AWS 고유의 서비스들이 소개되는데요,이번에 말씀드릴 Construct Hub는 AWS의 솔루션이 아닌 오픈 소스 커뮤니티이자,CDK 라이브러리를 검색하고 공유할 수 있는 장이라고 할 수 있습니다.Construct Hub를 통해 AWS가 그리는 IaC(Infrastructure as Code)의 미래를 함께 만나보겠습니다.우선, Construct Hub를 소개하기 앞서 AWS의 IaC 도구들에 대하여 간략하게 알아보겠습니다.️📂 AWS CloudFormation2011년 AWS의 리소스를 JSON 또는 YAML 형식으로 인프라를 템플릿화 할 수 있는 CloudFormation을 발표했습니다.  예) AWS S3 버킷 리소스를 제어하는 CloudFormation 코드  Resources:  HelloBucket:    Type: AWS::S3::Bucket    Properties:      AccessControl: PublicRead      WebsiteConfiguration:        IndexDocument: index.html        ErrorDocument: error.html그러나, 인프라가 복잡해질수록 CloudFormation 코드의 길이도 길어지고 리소스 간의 관계도 파악하기 어려워 관리의 피로도가 증가하게 되었습니다.AWS는 이러한 CloudFormation의 약점을 보완하고 개발자들이 YAML 형식으로 인프라를 정의하는 게 아니라,선호하는 프로그래밍 언어로 클라우드 인프라를 정의할 수 있도록 Cloud Development Kit(CDK) 를 2019년에 정식으로 출시했습니다.⌨️ AWS Cloud Development KitCDK는 익숙한 프로그래밍 언어를 사용해 클라우드 애플리케이션 리소스를 정의할 수 있는 오픈 소스 소프트웨어 개발 프레임워크입니다.CDK는 어떤 방식으로, 앞서 언급된 CloudFormation의 약점들을 보완하는지 그림과 함께 알아보겠습니다.CDK는 Construct의 집합체라 말할 수 있습니다.Construct는 클라우드 서비스를 이루는 컴포넌트라 생각하셔도 좋습니다.CDK 프레임워크를 통해 개발자 혹은 클라우드 운영자는 타입스크립트, 파이썬, 닷넷, 자바 등의 익숙한 프로그래밍 언어 중하나를 선택하여, construct 라이브러리를 사용해 프로그래밍 합니다.하나의 Stack으로 엮어진 소스코드를 cdk synth 라는 명령어로 CloudFormation에서 사용되는 템플릿으로 변환하고 cdk deploy 명령어로 인프라를 배포합니다.CDK로 작성된 인프라 코드는 프로그래밍 언어의 이점을 그대로 채택하여,해당 언어에 익숙한 사람이라면 YAML로 작성된 CloudFormation 보다 인프라의 환경을 파악하기 쉽습니다.실제로 올해 키노트에 언급된 Liberty Mutual INSURANCE사의 CDK 도입 사례 에서1500라인의 CloudFormation 코드를 CDK에서 단 14줄로 구현하며 CDK의 뛰어난 가시성을 알렸습니다.🗂 Construct Hub대망의 Construct Hub를 소개하기 위해 먼 길을 돌아왔습니다.😓맨 처음에 Construct Hub를 오픈 소스 커뮤니티이자, CDK 라이브러리를 검색하고 공유할 수 있는 장이라고 소개한 말이 맞는지 그림과 함께 확인해 보겠습니다.Construct Hub 의 메인 홈페이지를 확인해 보면 CDK 라이브러리를 검색할 수 있는‘서치 바’와 현재 ‘지원하는 언어’와 ‘서비스 프로바이더’(AWS, Datadog, Mongo DB, Aqua Security 등)가 보입니다.이 중에서도 화면 왼쪽에 위치한 다양한 CDK들의 종류에 대해 궁금증이 생기실 것 같습니다.앞서 소개한 AWS CDK는 프로그래밍 언어로 작성한 인프라 코드를 Cloudformation으로 템플릿을 생성했습니다.이처럼 CDK가 생성하는 템플릿이 AWS Cloudformation으로 활용 가능하도록 하는 것을 AWS CDK,쿠버네티스로 활용 가능하도록 하는 것을 CDK8s, 테라폼으로 활용 가능하도록 하는 CDKtf라고 합니다.위와 같이 현재 3가지 Type을 지원하고 있으며, 향후 다른 도구들도 지원할 가능성이 있다고 합니다.다음으로는 Construct Hub에 등록된 다양한 Construct 검색 결과입니다. 현재는 대부분 Construct는 Hahicorp, Datadog과 같은 클라우드 서비스 Publisher 들이 참여했지만,개인도 JSII (CDK가 다중 언어 라이브러리를 제공할 수 있도록 하는 기술)기반의 construct를 만들고 aws-cdk, cdk8s, cdktf 등의 키워드와 함께 npm 레지스트리에 공개되어 있다면 약 30분 내에 Construct Hub에도 개시된다고 합니다.화면에 보이는 특이점으로는 HashiCorp가 제공하는 CDKtf를 통해,다른 클라우드 서비스들과 통합하여 AWS 서비스 만이 아닌 모든 클라우드를 CDK로 제어 가능하게 하려는 큰 그림을 그려나가고 있다는 것을 알 수 있습니다.Outro지금까지 Construct Hub를 간단하게 살펴보며, AWS IaC 도구들의 변천사와 AWS가 그리는 IaC의 미래를 엿볼 수 있었습니다.그중에서도 흥미로웠던 요소들은 다음 2가지로 말씀드릴 수 있습니다.  IaC를 도입한 조직의 경우, Terraform, Ansible 등 여러 IaC 도구를 각각의 IaC 도구들의 특성에 맞게 복합적으로 운영합니다. CDK가 다방면으로 IaC 도구를 지원(AWS CDK, CDK8s, CDKtf) 하게 만들어 관리 복잡도를 줄이려는 노력이 흥미롭습니다.  CDKtf를 통해, 타 클라우드 서비스를 CDK로 제어하게 된다는 점이 흥미롭습니다.사실 CDK8s, CDKtf 모두 Construct Hub가 나오기 이전부터 존재했지만, Construct Hub에 개시된 문서들을 보니 더 흥미롭게 다가옵니다.아직 세상에 알려진지 얼마 되지 않은 Construct Hub를 활용하기에는 어려움이 있지만, 누구나 Construct Hub에 기여할 수 있는 오픈소스 생태계를 구축한 만큼 빠른 성장이 기대됩니다. Construct Hub가 기여할 IaC 미래에 긍정적인 기대를 걸어봅니다.소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해주세요! 😃📚 References  AWS Construct Hub availability  Construct Hub",
        "url": "/ConstructHub"
    }
    ,
    
    "karpenter": {
        "title": "My first impression of AWS Karpenter",
            "author": "HeuristicWave",
            "category": "",
            "content": "본 글은 지난 11월 29일에 GA된 Karpenter를 가볍게 사용해 보며 느낀 첫인상에 대한 글입니다.IntroEKS 환경에서 더 빠르고 효율적인 Kubernetes Cluster Autoscaler Karpenter가 GA 되었습니다.오픈소스인 Karpenter는 모든 클라우드 벤더와의 통합도 목표하지만, 아직 타 클라우드와의 통합은 진행 중이라 v0.5으로 공개되었다고 합니다.즉 GA 된 v0.5만큼은 AWS 리소스와 완벽하게 통합되었기에 이번 업데이트에 공개되었다고 할 수 있습니다.사실 기존 EKS 환경에서도 EC2의 Autoscaling을 활용한 CA를 지원하였는데, Karpenter 출시가 어떤 영향을 미치게 될 것인지 알아보겠습니다.Kubernetes Autoscaling쿠버네티스에서는 다음과 같은 3가지 Autoscaling 방법이 있습니다.  HPA(Horizontal Pod Autoscaler)  VPA(Vertical Pod Autoscaler)  CA(Cluster Autoscaler)Pod Scaling의 경우 CPU 사용량, 메트릭 등을 관찰하여 스케일링하지만, EKS 클러스터 자체의 자원이 모자라는 경우 CA를 고려해야 합니다.⚙️ Cluster Autoscaler with EC2 Auto Scaling기존 EKS는 다음과 같이 EC2의 Autoscaler 기능을 활용해 탄력적인 환경을 구성했습니다.hpa와 vpa가 eks에서 내에서 scaling을 진행하는 것과는 달리,worker node를 확보하기 위해 EC2의 Auto Scaling Group을 사용하며 k8s와 ec2 별도의 Layer를 관리해야 하는 운영 복잡도가 발생하게 되었습니다.🪓 Cluster Autoscaler with Karpenter반면 Karpenter의 경우 k8s의 native method를 확장한 개념이기 때문에, 기존의 CA 방법과는 달리 효율적인 환경을 제공할 수 있습니다.(실제로 카펜터를 운영해 보면 기존의 Auto Scaling Group을 사용하지 않는 것은 아니지만,사용자 입장에서는 고려하지 않아도 되니 k8s layer에서만 관리된다고 해도 틀린 말은 아닌 것 같습니다.)karpenter.sh 의 홈 화면을 보면 간단한 동작 원리를 설명하는 그림이 있습니다.karpenter가 unscheduled pods를 관찰하고 있다가 즉시(just-in-time) 최적화된 capacity에 pods를 배포합니다.Overprovisioning과거 EC2의 스케일링을 사용하는 CA에 대한 공식 문서에서, 다음과 같은 고려 사항을 확인할 수 있습니다.  노드를 확장하기 전에 노드가 확장될 때까지 기다려야 하므로 배포 대기 시간에 큰 영향을 미칩니다.노드를 사용할 수 있게 되려면 몇 분 정도 걸릴 수 있으며, 이로 인해 포드 예약 지연 시간이 크게 늘어날 수 있습니다.예약 대기 시간이 늘어나는 것을 감수하고 오버프로비저닝을 사용하여 이를 완화할 수 있습니다.그러나 이제 karpenter를 도입한다면 1분 이내 최적화된 인스턴스를 바로 프로비저닝 할 수 있으므로,더 이상 미리 프로비저닝을 할 필요도 없고 워커 노드의 크기 조정도 고려하지 않아도 됩니다.어떻게 오버프로비저닝을 방지하고 컨테이너와 클라우드 환경의 이점을 더 누릴 수 있게 해주는지 제가 진행해본 테스트와 함께 알아보겠습니다.👀 LabTMI : 이 글을 보는 시점에는 수정되어 있을 수도 있겠습니다만,공식 문서에 기재된 Default Region과 Module의 azs Config 값이 통일되지 않았습니다. 수행 시, 참고하시기 바랍니다.공식 문서 Terraform으로 시작하기 의 가이드대로Terraform 코드를 실행시키면 EKS 내에 다음과 같은 karpenter-controller와 karpenter-webhook 포드가 올라온 것을 확인할 수 있습니다.우선, Karpenter가 정말 최적화된 capacity를 제공하는지 확인하기 위해 t3a 시리즈의 스펙을 첨부합니다.Test 1 : t3a.medium 인스턴스에 1cpu를 요구하는 5개의 pod 배포문서에서 제공하는 inflate manifest를 활용해 t3a.medium 인스턴스에 1cpu를 요구하는 5개의 pod를 배포하면,다음과 같이 t3a.2xlarge 인스턴스가 즉시 프로비저닝 됩니다.(1분 이내라고 소개되지만, 체감상 1분 보다 더 빠른 시간 안에 프로비저닝 되는 것 같습니다.)새롭게 생성된 t3a.2xlarge 노드를 확인하면 다음과 같이 5개의 pod가 배치된 것을 볼 수 있습니다.아키텍처로 보면 다음과 같습니다. 기존 t3a.medium에는 기본으로 있는 pod들 때문에 1cpu 조차 할당할 수 없습니다.inflate는 5cpu를 요구하므로, 이를 수용할 수 있는 t3a.2xlarge 인스턴스를 프로비저닝하고 pod들을 배치시켰습니다.요청 리소스를 기반으로 최적의 인스턴스를 할당한 것을 확인할 수 있었습니다.Test 2 : Test1환경에서 0.5cpu를 요구하는 5개의 pod 배포Test 1에서 Scalue out(worker node 1대 =&gt; 2대) &amp; Scale up(t3a.medium =&gt; t3a.2xlarge)를 동시에 경험해 봤다면, 이번에는 다음과 같이 필요한 리소스만 0.5 cpu로 줄여보겠습니다.기존 t3a.2xlarge 인스턴스가 사라지고, t3a.xlarge 인스턴스가 즉시 프로비저닝 되었습니다.⬆️ a minute ago에서 ⬇️ 2minutes ago로 변하는 것을 보니 정말 1분 이내로 동작하는 것 같습니다.아키텍처로 보면 다음과 같습니다.이번에는 t3a.medium에 0.5 cpu만큼의 capacity가 남아있으므로 1개의 inflate pod가 배포되었고,t3x.xlarge 나머지 4개의 inflate pod가 배포되었습니다.t3a.large(2cpu) &lt; inflate(0.5cpu * 4) + kube-proxy + aws-node &lt; t3a.xlarge(4cpu)정말 빠른 시간 내에 최적의 capacity를 할당하는 모습을 보니 유연하고 높은 성능을 제공한다는 소개가 맞는 것 같습니다.저는 위 실험에서 인스턴스에 관한 별도의 CRD 값들을 지정하지 않아 karpenter가 t시리즈 인스턴스들을 프로비저닝 하였지만,운영에서 Karpenter를 사용하기 위해서는 Provisioner API를 읽고 세밀한 manifest 값들을 조정해 주어야 합니다.Outro과거 AWS의 CA는 스케일링에 걸리는 시간도 상당할뿐더러,제한적인 스케일링으로 인해 리소스가 낭비되거나 운영환경에서 다운타임을 최소화하기 위해 오버프로비저닝 되는 경우도 많았습니다.기존의 방법보다 더 Kubernetes native 한 karpenter를 도입한다면, 아래 그림과 같은 효과를 기대할 수 있습니다.Karpenter의 빠른 프로비저닝과 유연한 스케일링 덕분에 클라우드를 더 클라우드답게 사용할 수 있게 된 만큼가까운 미래에 Karpenter가 기존의 CA를 대체할 것으로 예상됩니다.지금까지 아주 간단하게 Karpenter를 사용해 본 후기를 작성해 보았습니다.추후, Karpenter의 자세한 동작 원리와 제약 사항 혹은 더 많은 기능들에 대하여 다뤄보겠습니다.소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해주세요! 😃📚 References  AWS Whats new, aws karpenter  Introducing Karpenter  Karpenter Document",
        "url": "/Karpenter"
    }
    ,
    
    "terraformtips1": {
        "title": "Terraform Tips 1 - Module &amp; Output",
            "author": "HeuristicWave",
            "category": "",
            "content": "Terraform 더 익숙하게 1 - Module &amp; OutputIntroTerraform Module을 잘 활용하면 본인이 원하는 대로 인프라를 레고처럼 조립할 수 있습니다.레고처럼 인프라를 조립하기 위해서는 모듈 간의 연계가 중요한데요,이번 포스팅에서는 모듈 내에 작성된 Output value들을 활용하는 방법을 알아보겠습니다.해당 포스팅은 Output에 대한 이해가 있다는 가정하에 기술하였으므로, Output에 대한 설명이 필요하시면 아래 링크들을 참고해 주세요!🥲 사실, 아래 문서보다 더 잘 설명할 자신이 없어서… 언제나 가장 좋은 교재는 공식 문서입니다!  Terraform Docs  Tutorial : Output Data from TerraformOutput Command OptionOutput은 terraform apply 명령어를 수행하고 난 후, 맨 마지막에 Value 들이 렌더링 되어 나옵니다.그러나 테라폼 코드를 작성하는 중간중간 인프라의 value 값들이 필요할 때가 있습니다.그럴 때, output이 정의된 상황에서 terraform output {label} 명령어로 Ouput 값을 확인합니다.-raw 옵션을 함께 주면 문자열 형태가 아닌 raw한 텍스트 값만을 얻을 수 있습니다.$ terraform output -raw security_group_idAccessing Child Module Outputs하위 모듈의 아웃풋에 접근할 경우도 종종 있는데요, 이때는 module.&lt;모듈 명&gt;.&lt;Output 명&gt; 이런 형식으로 조회가 가능합니다.하위 모듈 출력값에 접근하는 것이, 모듈과 모듈은 연계하는 방법이기에 아래 예시에서 알아보겠습니다.module \"vpc\" {  source = \"terraform-aws-modules/vpc/aws\"  name   = \"sample_vpc\"  cidr = \"10.10.0.0/16\"  azs            = [\"us-west-2a\"]  public_subnets = [\"10.10.1.0/24\"]  tags = {    Owner       = \"me\"    Environment = \"stage\"  }}위와 같은 vpc 모듈은 security group 모듈과 거의 단짝 수준으로 함께 움직이는데요,security group 모듈을 활용하려면 다음과 같이 vpc_id 값이 필요합니다.이 경우, vpc를 먼저 생성하고 vpc_id 값을 알아내어 사용할 수 있지만, 다음과 같은 방법으로 모듈을 연계합니다.module \"security_group\" {  source      = \"terraform-aws-modules/security-group/aws\"  name        = \"ssh\"  description = \"ssh from workstation\"  vpc_id      = module.vpc.vpc_id  ingress_cidr_blocks = [\"0.0.0.0/0\"]  ingress_rules       = [\"ssh-tcp\"]}Find Module Output Label방금 전, 모듈을 연계하는 방법을 배워 보았습니다. 그런데, 모듈을 연계하기 위해서는 미리 사전에 작성된 모듈의 Output Label을 알아야 합니다.우선 에디터의 Explorer 탭에서 .terraform 폴더를 열어봅시다.apply를 적용한 security-group, vpc 모듈이 내 로컬 머신에 숨어 있습니다.해당 모듈 폴더 안에 들어가면 outputs.tf 가 정의되어 있으므로 해당 파일을 참고하여 Label 값을 얻어오면 됩니다!Example상황 : VPC 모듈을 사용해 VPC와 서브넷을 구축한 상황에서, EFS를 각 서브넷에서 사용하려고 합니다.EFS에서 모듈로 생성한 Subnets을 어떻게 참조할까요?  🖍 정답 보기  위에서 언급한대로 modules/vpc/output.tf 에서 프라이빗 서브넷의 Label을 확인해보면,private_subnets 이라 기재된 것을 확인할 수 있습니다. 이를 활용해 아래와 같이 모듈을 참조 할 수 있습니다.  resource \"aws_efs_mount_target\" \"mount\" {  count = length(module.vpc.private_subnets)  file_system_id = aws_efs_file_system.foo.id  subnet_id      = module.vpc.private_subnets[count.index]}  Outro이렇게 Module의 Output 값을 활용하는 방법을 알게 되니, 테라폼 모듈 조립에 대한 자신감이 생겼습니다. 앞으로도 테라폼에 더 익숙하지기 위한 방법들을 시리즈로 연재할 계획인데, 언제 끝날지 모르겠습니다. 😑지금까지 테라폼 더 익숙하게 Module &amp; Output 편을 읽어주셔서 감사합니다! 잘못된 내용은 지적해 주세요! 😃Terraform Tips    Module &amp; Output    Data &amp; Index    Refresh &amp; Replace",
        "url": "/TerraformTips1"
    }
    ,
    
    "ec2-clocksource": {
        "title": "EC2 Clocksource",
            "author": "HeuristicWave",
            "category": "",
            "content": "몰라도 되지만 알면 알수록 더 신비한 EC2 🙃Preview이번 포스팅에서는 AWS Well-Architected Labs - Performance Efficiency 에 개재된 Calculating differences in clock source를 읽고 궁금증이 생겨 구글링을 하다 알게 된 사실들을 의식의 흐름대로 작성한 포스팅입니다.Performance Efficiency Summary일단 Performance Efficiency에 나오는 실험 내용을 요약하자면 다음과 같습니다.AWS의 5세대 가상머신 Nitro와 non-nitro 인스턴스 2개를 올리고 시간을 반환하는 테스트 코드를 돌려 성능 테스트를 진행합니다.당연히 5세대 Nitro가 기존 세대보다 월등한 결과를 보여 주지만,non-nitro 기반의 인스턴스에서 ‘리눅스 클럭 소스를 교체하면 유의미한 성능 향상의 결과를 얻을 수 있다’ 라는 실험 결과를 보여줍니다.  Nitro 기반 인스턴스의 default clocksource : kvm-clock(권장)  Non-nitro 인스턴스의 default clocksource : xen  실험에서 교체한 Non-nitro 인스턴스의 clocksource : tsc마지막으로 첨부된 How do I manage the clock source for EC2 instances running Linux?게시물에서 클럭 소스를 교체하는 방법(xen에서 tsc로 교체)을 소개하며 실험 내용을 마칩니다.궁금한 건 못 참아 ❓위에 소개한 Lab을 진행하다 보니 ‘왜 tsc로 교체하여 성능 향상 효과를 얻을 수 있는지’ 알 수가 없었습니다.궁금증을 해소하기 위해 구글링을 하다 보니 이해를 돕는 다음 3가지 자료를 찾을 수 있었습니다.⏱ TimestampingRed Hat Reference Guide 에서 어느 정도 제 가려운 부분을 긁어 주었던 포스팅이 있습니다.기본적으로 멀티프로세서 시스템인 NUMA와 SMP 아키텍처에서는 여러 개의 clock source가 탑재되어 있습니다.멀티프로세서 기반의 EC2 인스턴스에서도 아래 명령어로 사용 가능한 clocksource를 확인하면 다음과 같은 결과를 확인할 수 있습니다.cat /sys/devices/system/clocksource/clocksource0/available_clocksourcexen tsc hpet acpi_pmRed Hat의 실험 결과에 따르면 tsc &gt; hpet &gt; acpi_pm 순으로 오버헤드가 적은데,tsc는 register에서 hpet은 memory area에서 읽기 때문에 수십만 개의 타임스탬프를 지정할 때 상당한 성능 이점을 제공한다고 합니다.⚙️ Heap Engineering PostRunning a database on EC2? Your clock could be slowing you down. 을 보면 더 정확한 분석이 있습니다.내용이 어려워 저는 완벽하게 이해하지 못했지만, 읽어보시면 굉장히 좋은 자료인 것 같습니다.Heap Engineering 해당 포스팅에서 밀당을 시도하는데…‘tsc에서는 낮은 가능성으로 clock drift 현상이 있어 프로덕션에서는 수행하지 말라’ 고 했다가,실제로는 clock drfit가 발생하지 않는다며 AWS가 tsc를 권장했던 슬라이드 자료 를 함께 보여줍니다.그냥 맘놓고 kvm-clock이 탑재된 인스턴스를 사용하는게 좋을 것 같습니다.🎥 Tudum~ 또! Netflix클라우드를 공부하다 보면 Netflix 가 클라우드에 지대한 영향을 끼친 것 같다고 느낄 때가 많은데, 이번에도 그랬습니다.AWS re:Invent 2014에서 Netflix의 Senior Performance Architect, Brendan Gregg의 발표 자료 를 보면xen에서 tsc로 교체하여 CPU 사용량은 30%, 평균 앱 레이턴시는 43%가 줄었다고 합니다.Result이번에도 구글링으로 딴짓을 하다 보니 많은 사실들을 알게 되었습니다. 사실 Current generation instances 를사용하면 대부분 위에서 언급한 최적화는 T2 시리즈, Gravition 계열을 제외한 대부분의 인스턴스에서는 기본적으로 적용되어 있습니다.그래서 포스팅의 첫 포문을 ‘몰라도 되지만 ~’이라 지었습니다.clocksource와는 별도로 이번 포스팅을 준비하다 거의 주말 하루를 소비했는데,비교적 최근의 인스턴스가 과거 인스턴스들과 어떻게 다른지(Hypervisor, Jumbo Frame 등등)를 알 수 있었습니다.새롭게 알게 된 사실들 역시 그냥 Nitro 기반의 Amazon Linux 2를 사용하면, 운영하는데 몰라도 지장 없이 최고의 성능을 보장받는 것 같습니다.아직 알음알음 아는 지식이라 포스팅하기 어렵지만, 훗날 더 정확히 알게 되면 성능과 관련된 다른 튜닝 요소들도 적어보겠습니다.소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해주세요! 😃📚 포스팅과 직접적인 연관도는 떨어지지만 함께 보면 좋은 자료  AWS EC2 Virtualization 2017: Introducing Nitro  Linux AMI virtualization types  Reinventing virtualization with the AWS Nitro System",
        "url": "/EC2_Clocksource"
    }
    ,
    
    "selfservice": {
        "title": "Self-service Infrastructure",
            "author": "HeuristicWave",
            "category": "",
            "content": "본 글은 Kief Morris의 Infrastructure as Code 와 HashiCorp 백서 를 읽고 학습한 내용을 기반으로 작성한 글 입니다.서문IaC(Infrastructure as Code)에 관심을 갖고 공부를 하다보면 Self-service Infra라는 말을 자주 만나게 됩니다.몇 개월째 와닿지 않는 개념이였지만, 최근 키프 모리스의 책을 다시 읽고 조금은 알게 된 거 같아 그동안 공부한 Self-service Infra에 대한 자료들을 바탕으로 작성해 보았습니다.Self-service Infra에 관한 배경지식을 넓히는 데 도움이 되었으면 좋겠습니다.Dynamic Infrastructure동적 인프라는 서버, 스토리지, 네트워크와 같은 인프라 자원을 관리할 수 있는 시스템을 말합니다.동적 인프라의 종류로는 Public/Private 클라우드, 오픈스택을 활용하는 사설 클라우드, 베어메탈 등이 있습니다.앞선 정의만 보면 동적 인프라는 클라우드와 굉장히 유사하지만, 키프 모리스는 동적 인프라가 클라우드보다 범위가 더 넓다고 합니다.📣 동적 인프라를 소개하며 알려드리고 싶은 문장이 있습니다.클라우드로의 전환이라는 의미에 대해 많은 사람들이 여러 측면에서 설명을 하지만,저는 HashiCorp의 Unlocking the Cloud Operating Model: Provisioning백서에 소개된 다음 표현에 참 공감이 갑니다.  클라우드로의 전환의 본질적인 의미는 “정적” 인프라에서 “동적” 인프라로의 전환입니다.  The essential implications of the transition to the cloud is the shift from “static” infrastructure to “dynamic” infrastructure동적 인프라 플랫폼 요구 사항동적 인프라 플랫폼은 다음과 같은 특성이 있습니다.  Programmable  On-Demand  Self-Service💻 Programmable동적 인프라 플랫폼은 프로그래밍을 쉽게 할 수 있어야 합니다. 유저 인터페이스 외에도 스크립트, CLI와 같은 도구들과도 상호 작용 할 수 있도록 프로그래밍 API가 필요합니다.아래 각 플랫폼 별 SDK를 사용하면 클라우드 내 자원을 생성하고 관리하는 코드를 작성할 수 있습니다.  AWS SDK  Azure SDK  GCP SDK  Openstack SDK⏰ On-Demand동적 인프라 플랫폼에서 자원을 즉시 생성하고 삭제하는 기능은 필수입니다.또한 전통적인 인프라의 과금 정책이 일정 기간 동안의 계약을 기반으로 한다면, 동적 인프라에서는 시간당 과금 체계를 지원합니다.🎊 드디어 대망의 셀프 서비스가 처음 소개 됩니다!🏃🏻 Self-Service셀프서비스는 온디맨드 요구 사항을 좀 더 발전시킨 개념입니다. 전통적인 방법에서 인프라를 요구하기 위해서는 세부 요청 양식, 설계 및 명세 문서, 구현 계획 수립 등을 필요로 했습니다.셀프서비스는 전통적인 방법에서 더 진화하여 필요한 인프라를 즉시 프로비저닝하고 쉽게 수정할 수 있는 자동화된 절차를 의미합니다.🛠 셀프 서비스는 인프라 템플릿을 운용할 수 있는 IaC 도구를 기반으로 구현합니다.예를 들어 개발자가 로드밸런서로 ALB를 사용하고 있다가 NLB로 바꾸고 싶은 경우, 다음과 같이 정의된 인프라 코드를 재배포 하면 됩니다.resource \"aws_lb\" \"test\" {  name               = \"test-lb-tf\"  internal           = false    # load_balancer_type = \"application\"  load_balancer_type = \"network\"    # Leave out other config ...}HashiCorp가 정의한 Self-Service Infrastructure 게시물을 보면,Self-Service Infra가 어떤 의미인지 더 쉽게 다가옵니다. (첨부된 링크를 통해 셀프서비스의 장점을 꼭 한번 읽어보세요!)기존 방법개발자는 인프라 담당자가 인프라를 할당할 때까지 기다려야 합니다.셀프 서비스 적용작성된 인프라 템플릿을 활용해 온디맨드로 프로비저닝 할 수 있습니다.글을 마치며저는 테라폼을 공부한 이후, 간단한 웹서비스를 운영하는 토이프로젝트를 진행할 때 다음과 같은 인프라 환경을 자주 사용합니다.미리 코드로 정의한 인프라 덕분에 개발에만 집중할 수 있는 환경과 생산성 향상을 경험했습니다.또한 테라폼 모듈 덕분에 제가 원하는 대로 인프라 스펙을 변경하고 각종 클라우드 서비스 추가 혹은 제거가 가능했습니다.셀프서비스 인프라가 기존 승인 체계(리소스 요청 ➡️ 담당자 승인)를 부정하는 것은 아니라고 생각합니다.기존 체계가 갖고 있는 보안적 이점을 포함한 장점들을 유지하며, 유연하고 신속하게 인프라를 운용하는 Self-service 환경이 불러올 장점을 고민해 봐야겠습니다.마지막으로, DevOps 문화가 정착해가며 CI/CD 를 통해 지속적 배포가 가능해지며 더 잦은 서비스 출시가 가능해졌습니다.또한, IaC를 통해 Immutable Infra를 추구하며 인프라의 일관성과 안정성을 보장하게 되었습니다.앞선 두 개의 개념에 더해 Self-service를 추구한다면 조직의 민첩성과 생산성 향상에 도움이 될 것이라고 생각합니다.소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해주세요! 😃",
        "url": "/SelfService"
    }
    ,
    
    "packer": {
        "title": "Provision Infrastructure with Packer",
            "author": "HeuristicWave",
            "category": "",
            "content": "본 글은 HashiCorp Learn - Provision Infrastructure with Packer 에서 다루는 내용을 기반으로 작성한 글 입니다.  Packer를 사용해 AWS AMI를 만들고 Terraform과 연계하여 활용하는 방법에 약간의 설명과 팁을 담아 한국어로 재작성해 보았습니다. (설치와 관련된 준비사항은 생략되어 있으므로 원문을 확인해주세요.)프로젝트 구조먼저, 해당 튜토리얼을 진행하기 위한 프로젝트 구조는 아래와 같습니다. 아래와 폴더와 파일을 준비해주세요..└── provision-infra-with-packer    ├── images    │   └── image.pkr.hcl    ├── instances    │   ├── main.tf    │   └── variables.tf    └── scripts        └── setup.shLocal SSH key 생성하기provision-infra-with-packer 폴더 안에서 AWS AMI로 만들 인스턴스에 접속하기 위한 SSH 키를 생성합니다.필자는 사용할 공개키를 Mac OS에서 ssh-keygen으로 생성하였습니다. 각자 환경에 맞는 방법으로 SSH 공개키를 생성하세요.암호화 타입(-t)을 RSA로 주석(-C)을 이메일로 생성되는 공개키의 위치(-f)를 현재 directory로 설정하고 공개키의 이름을 tf-packer로 설정했습니다.명령어를 입력하고 비밀번호를 입력하면 되지만 편의상 공백으로 두겠습니다.$ ssh-keygen -t rsa -C \"your_email@example.com\" -f ./tf-packer이후 tf-packer와 tf-packer.pub 2가지 파일이 생성되었다면 다음 단계 🚀Packer 코드 작성하기image.pkr.hcl 파일에서 진행합니다. 1️⃣ AMI 구축에 필요한 config 작성packer가 빌드한 이미지가 저장될 리전의 정보와 AMI에 timestamp 정보를 넣기 위한 config를 차례로 작성합니다.variable \"region\" {  type    = string  default = \"us-east-1\"}locals { timestamp = regex_replace(timestamp(), \"[- TZ:]\", \"\") }2️⃣️ Base AMI에 대한 Source AMI config 작성packer가 이미지를 만들기 위해 기본으로 사용되는 Base AMI에 대한 정보를 source 블록에 정의합니다.1단계에서 작성한 config 값을 활용해 리전과 timestamp를 넣어주는 코드와 Packer Builder로 사용할 인스턴스 타입을 지정합니다.이후, AWS에 존재하는 수많은 AMI 중에서 source로 활용할 이미지를 filter 코드로 작성합니다.(name 부분에 직접 ami 번호를 명시적으로 기재 할 수도 있습니다.)source \"amazon-ebs\" \"example\" {  ami_name      = \"learn-terraform-packer-${local.timestamp}\"  instance_type = \"t2.micro\"  region        = var.region  source_ami_filter {    filters = {      name                = \"ubuntu/images/*ubuntu-xenial-16.04-amd64-server-*\"      root-device-type    = \"ebs\"      virtualization-type = \"hvm\"    }    most_recent = true    owners      = [\"099720109477\"]  }  ssh_username = \"ubuntu\"}3️⃣ Packer build config 작성build 부분에서는 환경변수 세팅이나 명령어를 inline 형태로 기입 할 수 있지만,아래와 같은 간단한 기능만 수행하도록 코드를 작성합니다. hcl 문법에 따라 source를 지정하고, 프로비저닝을 위한 공개키의 위치를 명세합니다. source는 로컬 머신, destination은 원격 머신 입니다.마지막으로 Packer로 빌드한 이미지에서 Application Setup이 담긴 script를 지정합니다.build {  sources = [\"source.amazon-ebs.example\"]  provisioner \"file\" {    source      = \"../tf-packer.pub\"    destination = \"/tmp/tf-packer.pub\"  }  provisioner \"shell\" {    script = \"../scripts/setup.sh\"  }}4️⃣ Shell script 작성해당 작업은 scripts 폴더의 setup.sh에서 진행합니다. 이 부분은 Terraform으로 프로비저닝 한 인프라를 웹페이지에서 확인하기 위해 간단한 샘플을 띄우는 코드가 담겨있습니다.아래 Script에 필요한 종속성 설치, terraform을 user에 추가, 생성한 SSH키 설치, 샘플 Go App 설치가 단계가 작성되어 있습니다.  #!/bin/bashset -e# Install necessary dependenciessudo DEBIAN_FRONTEND=noninteractive apt-get -y -o Dpkg::Options::=&quot;--force-confdef&quot; -o Dpkg::Options::=&quot;--force-confold&quot; dist-upgradesudo apt-get -y -qq install curl wget git vim apt-transport-https ca-certificatessudo add-apt-repository ppa:longsleep/golang-backports -ysudo apt -y -qq install golang-go# Setup sudo to allow no-password sudo for &quot;hashicorp&quot; group and adding &quot;terraform&quot; usersudo groupadd -r hashicorpsudo useradd -m -s /bin/bash terraformsudo usermod -a -G hashicorp terraformsudo cp /etc/sudoers /etc/sudoers.origecho &quot;terraform ALL=(ALL) NOPASSWD:ALL&quot; | sudo tee /etc/sudoers.d/terraform# Installing SSH keysudo mkdir -p /home/terraform/.sshsudo chmod 700 /home/terraform/.sshsudo cp /tmp/tf-packer.pub /home/terraform/.ssh/authorized_keyssudo chmod 600 /home/terraform/.ssh/authorized_keyssudo chown -R terraform /home/terraform/.sshsudo usermod --shell /bin/bash terraform# Create GOPATH for Terraform user &amp; download the webapp from GitHubsudo -H -i -u terraform -- env bash &lt;&lt; EOFwhoamiecho ~terraformcd /home/terraformexport GOROOT=/usr/lib/goexport GOPATH=/home/terraform/goexport PATH=$PATH:$GOROOT/bin:$GOPATH/bingo get -d github.com/hashicorp/learn-go-webapp-demoEOF1 ~ 4단계를 마쳤다면 images 폴더 위치에 packer build image.pkr.hcl 명령어로 이미지를 빌드합니다.이미지 빌드 후, 콘솔 Images 탭의 AMIs을 확인하면 빌드한 이미지가 존재합니다.또한, 인스턴스 탭을 확인하면 아래와 같이 Base AMI를 만들기 위한 Packer Builder의 흔적을 볼 수 있습니다.해당 화면의 Instance ID를 눌러 설정들을 확인해보면 source 블록에 정의한 값을 바탕으로 만들어진 모습을 확인할 수 있습니다.Terraform으로 Packer 이미지 배포해당 작업은 instances 폴더에서 진행합니다. Sample App Infra Code 작성하기아래 🛠 이모티콘을 클릭하여 main.tf와 varaiables.tf를 작성합니다.  🔨 main.tf 🔨    terraform {  required_providers {    aws = {      source  = \"hashicorp/aws\"      version = \"~&gt; 3.26.0\"    }  }  required_version = \"~&gt; 1.0.2\"}provider \"aws\" {  region = var.region}resource \"aws_vpc\" \"vpc\" {  cidr_block           = var.cidr_vpc  enable_dns_support   = true  enable_dns_hostnames = true}resource \"aws_internet_gateway\" \"igw\" {  vpc_id = aws_vpc.vpc.id}resource \"aws_subnet\" \"subnet_public\" {  vpc_id     = aws_vpc.vpc.id  cidr_block = var.cidr_subnet}resource \"aws_route_table\" \"rtb_public\" {  vpc_id = aws_vpc.vpc.id  route {    cidr_block = \"0.0.0.0/0\"    gateway_id = aws_internet_gateway.igw.id  }}resource \"aws_route_table_association\" \"rta_subnet_public\" {  subnet_id      = aws_subnet.subnet_public.id  route_table_id = aws_route_table.rtb_public.id}resource \"aws_security_group\" \"sg_22_80\" {  name   = \"sg_22\"  vpc_id = aws_vpc.vpc.id  # SSH access from the VPC  ingress {    from_port   = 22    to_port     = 22    protocol    = \"tcp\"    cidr_blocks = [\"0.0.0.0/0\"]  }  ingress {    from_port   = 8080    to_port     = 8080    protocol    = \"tcp\"    cidr_blocks = [\"0.0.0.0/0\"]  }  ingress {    from_port   = 80    to_port     = 80    protocol    = \"tcp\"    cidr_blocks = [\"0.0.0.0/0\"]  }  egress {    from_port   = 0    to_port     = 0    protocol    = \"-1\"    cidr_blocks = [\"0.0.0.0/0\"]  }}resource \"aws_instance\" \"web\" {  ami                         = \"ami-YOUR-AMI-ID\"  instance_type               = \"t2.micro\"  subnet_id                   = aws_subnet.subnet_public.id  vpc_security_group_ids      = [aws_security_group.sg_22_80.id]  associate_public_ip_address = true  tags = {    Name = \"Learn-Packer\"  }}output \"public_ip\" {  value = aws_instance.web.public_ip}    🔧 varaiables.tf 🔧    variable \"cidr_vpc\" {  description = \"CIDR block for the VPC\"  default     = \"10.1.0.0/16\"}variable \"cidr_subnet\" {  description = \"CIDR block for the subnet\"  default     = \"10.1.0.0/24\"}variable \"environment_tag\" {  description = \"Environment tag\"  default     = \"Learn\"}variable \"region\"{  description = \"The region Terraform deploys your instance\"  default     = \"us-east-1\"}    인프라 코드에 대한 설명은 생략하겠습니다. 인프라에 대한 테라폼 코드가 궁금하시다면 다른 게시글이나 제 포스팅을 읽어보세요!AMI Query 하기본문에는 소개되지 않았지만, Packer와 Terraform의 통합을 위해 필자가 작성한 부분입니다.Terraform의 Data Sources를 활용해 빌드된 AMI를 Query하여 샘플 앱을 프로비저닝 해봅시다.선행 작업에서 진행한 main.tf 하단에 아래 코드를 추가합니다.data \"aws_ami_ids\" \"myami\" {  owners = [\"YOUR Account ID\"]  # sort_ascending = true    filter {    name   = \"name\"    values = [\"learn-terraform-packer-*\"]  }}Packer로 이미지를 빌드했으므로, ami owner가 되었습니다!owners 부분에 자신의 계정 ID를 작성하고 filter에 packer build를 할 떄 사용한 ami_name을 value 값으로 넣습니다.  sort_ascending default 값이 false  sort_ascending = false : list의 0번째 요소가 latest sort_ascending = true : 만들어진 순서대로 리스트 생성 (가장 먼저 생성된 ami가 0번)이어서 main.tf에 인스턴스 리소스를 정의한 부분의 ami = \"ami-YOUR-AMI-ID\" 코드를 아래와 같이 대체합니다.data 객체에 필터링한 결과 값들이 빌드된 AMI들이 리스트 형식으로 들어가는데 latest 버전을 사용하기 위해 0번째 이미지를 명시합니다.편의상 사용된 ami 번호를 터미널에서 확인 할 수 있도록 output에 대한 코드도 함께 작성합니다.resource \"aws_instance\" \"web\" {  ami = data.aws_ami_ids.myami.ids[0]   # \"ami-YOUR-AMI-ID\"  # Skip Other Config}output \"my_ami\" {  value = aws_instance.web.ami}코드를 작성하고 테라폼 코드가 위치한 폴더에서(instances 폴더 하위) 다음 명령어를 실행합니다.terraform init &amp;&amp; terraform apply 인스턴스를 생성하기 위해 yes를 기입합니다. 명령어로 인프라 상태를 점검해 아래와 같다면 다음 단계로 🚀$ terraform state listdata.aws_ami_ids.myamiaws_instance.webaws_internet_gateway.igwaws_route_table.rtb_publicaws_route_table_association.rta_subnet_publicaws_security_group.sg_22_80aws_subnet.subnet_publicaws_vpc.vpc  data object에 담긴 정보가 확인하고 싶다면? $ terraform state show data.aws_ami_ids.myami.ids인스턴스 확인하기SSH를 통해 인스턴스에 연결합니다. ssh terraform@$(terraform output -raw public_ip) -i ../tf-packerGo 디렉토리로 이동하세요. cd go/src/github.com/hashicorp/learn-go-webapp-demo데모 앱을 실행합니다. go run webapp.go배포한 앱 확인을 위해 terraform output public_ip로 얻은 IP에 8080 포트로 접속하면 간단한 테트리스 게임 앱을 확인할 수 있습니다.인스턴스 리소스 회수terraform destroy 명령어로 상기 프로젝트에서 사용한 인프라를 리소스를 회수합니다. Packer로 작성한 이미지는 파괴되지 않습니다.위와 같은 단계들을 통해 패커로 이미지를 만들고 테라폼과 통합하는 방법을 학습해보았습니다.이번 포스팅에서 다뤘던 내용은 Immutable Servers를 유지하기 위한 방법 중 하나입니다.오늘 포스팅에 추가로 Ansible을 통합한다면, Immutable Infrastructure를 구축할 수도 있습니다.Terraform Courses    3-Tier VPC Architecture with Terraform    ECR Pipeline with Terraform Ⅰ (CodeCommit)    ECR Pipeline with Terraform Ⅱ (ECR, CodeBuild, IAM)    ECR Pipeline with Terraform Ⅲ (CodePipeline)    Provision Infrastructure with Packer",
        "url": "/Packer"
    }
    ,
    
    "eks-max-pods": {
        "title": "Look into EKS max pods",
            "author": "HeuristicWave",
            "category": "",
            "content": "EKS 노드에서 사용 가능한 Pod의 개수는 몇 개일까?Preview이번 포스팅에서는 EKS의 노드 그룹에서는 최대 몇 개의 포드(Private IP)가 할당 가능하고 어떠한 방법으로 최대 포드의 개수를 제어할 수 있는지 알아보자.Build Up쿠버네티스 도규먼트 에 따르면 노드당 110개의 포드를 생성할 수 있으며, 노드는 5000개까지 생성 가능해 총 15만 개의 포드가 생성 가능하다고 한다. GCP의 GKE 가이드 에 따르면 기본 클러스터 노드 하나에 최대 110개의 포드가 생성 가능하다고 한다. 구글링을 통해 확인하니, 노드에서 포드의 갯수가 증가할수록 kubelet, cAdvisor 등과 같은 K8s 에이전트에 오버헤드를 발생시키므로 110개 정도를 권장한다고 한다.그래서 그런지 kubelet docs 에서도 아래와 같은 max-pods가 110을 기본값으로 가진다.--max-pods int32     Default: 110EKS eni max podsIP addresses per network interface per instance type 을 확인해보면 AWS의 인스턴스 타입별 ENI 개수를 파악할 수 있다.EKS 설명서 를 보면 다음과 같은 공식을 확인할 수 있다. (# of network interfaces for the instance type × (# of IPv4 per network interface - 1)) + 2ENI의 첫 번째 IP는 포드가 사용할 수 없으므로 1을 빼고 AWS CNI와 kube-proxy가 차지하는 2개의 IP를 마지막 수식에 더해 최종 사용 가능한 max-pod 값을 알 수 있다. 설명한 공식 이외에도 kubectl 명령어를 통해 Maximum Pods를 파악할 수 있다.❯ kubectl get nodes -ANAME                                            STATUS   ROLES    AGE   VERSIONip-10-0-0-178.ap-northeast-2.compute.internal   Ready    &lt;none&gt;   18h   v1.19.6-eks-49a6c0ip-10-0-1-143.ap-northeast-2.compute.internal   Ready    &lt;none&gt;   18h   v1.19.6-eks-49a6c0❯ kubectl describe nodes ip-10-0-0-178.ap-northeast-2.compute.internal | grep -i pods  pods:                        17  pods:                        17Non-terminated Pods:          (17 in total)👀 Labs 1kubectl의 명령어에서 노드(t3.medium)당 17개의 포드를 사용할 수 있다고 했는데, 직접 노드그룹 내에서 포드를 최대로 띄워 확인해보자. 노드그룹 내에 nginx 32개를 올려보았다.K9s 쉘을 통해 총 38개의 포드가 확인되었고 38개 중 aws-node, coredns, kube-proxy가 각각 2개의 노드에 위치하고 4개의 nginx 포드가 Pending 상태라는 것을 파악했다.즉, 38(Total) - 4(Pending) = 17(t3.medium Maximum Pods) * 2(# of Node) 실험 결과와 앞서 알아본 Maximum Pods가 동일하다.Maximum Pods 변경하기EKS에서 Maximum Pods를 결정 짓는 요소는 ENI다. 그러나 클러스터의 노드그룹을 생성할 때 kubelet의 max-pods 값을 변경해 커스터마이징 할 수 있다. AWS Docs 에서 다음과 같은 (불친절한?) 설명을 통해 ENI와 별개로 max-pods를 제어할 수 있는 힌트를 얻었다.공식 문서에서 설명이 굉장히 빈약하지만, EKS에서 노드 그룹을 커스텀으로 생성할 때 Launch templates의 UserData를 아래와 같이 정의하면 Maximum Pods가 변경된다.MIME-Version: 1.0Content-Type: multipart/mixed; boundary=\"==MYBOUNDARY==\"--==MYBOUNDARY==Content-Type: text/x-shellscript; charset=\"us-ascii\"#!/bin/bash/etc/eks/bootstrap.sh {Cluster Name} --use-max-pods false --kubelet-extra-args '--max-pods=10'--==MYBOUNDARY==--\\  GCP에서 Max Pods를 제어하는 방법 👈 Click!   GCP에서는 클러스터를 생성할 때아래 명령어의 --default-max-pods-per-node 파라미터를 통해 max-pods(👆 Build Up 단계에서 default 110 👆)를 조절할 수 있다.  gcloud container clusters create CLUSTER_NAME \\  # 생략  --default-max-pods-per-node MAXIMUM_PODS \\  # 생략  👀 Labs 2아래 사진의 왼쪽은 Labs 1의 Pure한 t3.medium, 오른쪽은 max-pods를 지정한 Custom t3.medium이다. 콘솔화면에서 스펙은 같지만 할당된 포드의 수가 다르다.Result지금까지 다양한 방법을 통해 EKS에서 Maximum Pods를 파악하는 방법과 변경하는 방법 배웠다.AWS에서는 ENI라는 가상 네트워크 카드를 나타내는 논리적 네트워크 구성 요소 덕분에 인스턴스 타입마다 생성될 수 있는 포드의 수가 달랐다.그 밖에도 GCP에 쿠버네티스의 설계 철학을 그대로 이어받아 kubelet의 max-pods 값이 동일하고 클러스터 생성 순간에도 max-pods 설정에 대한 자유도가 높다는 사실을 알 수 있었다.어떠한 방법이 더 우위에 있는지 결론짓기 어렵지만, CSP의 쿠버네티스 max-pods 생성 원리를 파악하여 최적의 IP 할당에 도움이 되면 좋겠다.소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해주세요! 😃EKS Series    Look into EKS max pods",
        "url": "/EKS_Max_Pods"
    }
    ,
    
    "codepipeline": {
        "title": "ECR CodePipeline with Terraform Ⅲ",
            "author": "HeuristicWave",
            "category": "",
            "content": "Terraform으로 ECR 파이프라인 구축하기 3 (CodePipeline)Preview3편에서는 CodePipeline을 생성하고 IAM 정책과 역할을 부여하는 법을 배워보겠습니다.문서를 확인하면 artifact가 담기는 버킷, pipeline을 생성하는 리소스, 관련된 IAM Role과 Policy가 보입니다.테라폼은 선언형 언어이므로 Role과 Resource의 작성 순서가 바뀌어도 상관이 없지만, 콘솔에서 작업할 경우 Role을 먼저 작성하고 리소스를 생성하니 3편에서는 IAM을 먼저 작성하겠습니다.IAM Role아래 Role을 방금전 생성한 codepipeline.tf에 작성합니다.resource \"aws_iam_role\" \"codepipeline_role\" {  name = \"terraform-codepipeline\"  assume_role_policy = &lt;&lt;EOF{  \"Version\": \"2012-10-17\",  \"Statement\": [    {      \"Action\": \"sts:AssumeRole\",      \"Principal\": {        \"Service\": \"codepipeline.amazonaws.com\"      },      \"Effect\": \"Allow\"    }  ]}EOF}IAM Policy본래 필요한 정책만을 골라 정책생성기 에서 생생된 정책을 활용하는 방법이 있지만, 어떤 정책이 필요한지 한번에 맞추기는 너무 어렵습니다.(저의 경우 인터넷에서 타인이 작성한 정책과 에러메시지를 맞아가며 정책을 작성하고 있습니다 😅)➕ 아래 Policy를 방금전 생성한 codepipeline.tf에 아래 코드를 추가합니다.resource \"aws_iam_policy\" \"codepipeline_policy\" {  description = \"Codepipeline Execution Policy\"  policy      = &lt;&lt;EOF{  \"Version\": \"2012-10-17\",  \"Statement\": [    {      \"Action\": [        \"s3:GetObject\", \"s3:GetObjectVersion\", \"s3:PutObject\",        \"s3:GetBucketVersioning\"      ],      \"Effect\": \"Allow\",      \"Resource\": \"${aws_s3_bucket.artifact_bucket.arn}/*\"    },    {      \"Action\" : [        \"codebuild:StartBuild\", \"codebuild:BatchGetBuilds\",        \"iam:PassRole\"      ],      \"Effect\": \"Allow\",      \"Resource\": \"*\"    },    {      \"Action\" : [        \"codecommit:CancelUploadArchive\",        \"codecommit:GetBranch\",        \"codecommit:GetCommit\",        \"codecommit:GetUploadArchiveStatus\",        \"codecommit:UploadArchive\"      ],      \"Effect\": \"Allow\",      \"Resource\": \"${aws_codecommit_repository.test.arn}\"    }  ]}EOF}🚩 이어서 생성한 Policy를 Role에 부여합니다. 이것 역시 codebuild.tf에 추가합니다.resource \"aws_iam_role_policy_attachment\" \"codepipeline-attach\" {  role       = aws_iam_role.codepipeline_role.name  policy_arn = aws_iam_policy.codepipeline_policy.arn}CodePipelineaws_codepipeline리소스의 config에는 artifact store와 암호화 키, Source-Build-Deploy로 이어지는 각 Stage가 선언되어 있습니다.리소스 안에 기재된 설정들은 필수가 아니므로 선택하여 사용할 수 있습니다. 이번 포스팅에서는 deploy stage와 암호화 config는 제외하고 진행하겠습니다.resource \"aws_codepipeline\" \"pipeline\" {  name     = \"${var.source_repo_name}-${var.source_repo_branch}-Pipeline\"  role_arn = aws_iam_role.codepipeline_role.arn  artifact_store {    location = aws_s3_bucket.artifact_bucket.bucket    type     = \"S3\"  }  stage {    name = \"Source\"    action {      name             = \"Source\"      category         = \"Source\"      owner            = \"AWS\"      version          = \"1\"      provider         = \"CodeCommit\"      output_artifacts = [\"SourceOutput\"]      run_order        = 1      configuration = {        RepositoryName       = var.source_repo_name        BranchName           = var.source_repo_branch        PollForSourceChanges = \"false\"      }    }  }  stage {    name = \"Build\"    action {      name             = \"Build\"      category         = \"Build\"      owner            = \"AWS\"      version          = \"1\"      provider         = \"CodeBuild\"      input_artifacts  = [\"SourceOutput\"]      output_artifacts = [\"BuildOutput\"]      run_order        = 1      configuration = {        ProjectName = aws_codebuild_project.codebuild.id      }    }  }}1편에서 작성한 CodeCommit을 Stage의 Source, 2편에서 작성한 CodeBuild를 Build단계 지정했습니다.terraform apply, plan 명령어를 차례로 반영해 오류가 없는지 확인합니다.지금까지 작성된 인프라를 terraform state list명령어를 통해 확인하면 아래와 같습니다.❯ terraform state listaws_codebuild_project.codebuildaws_codecommit_repository.testaws_codepipeline.pipelineaws_ecr_repository.image_repoaws_iam_policy.codebuild_policyaws_iam_policy.codepipeline_policyaws_iam_role.codebuild_roleaws_iam_role.codepipeline_roleaws_iam_role_policy_attachment.codebuild-attachaws_iam_role_policy_attachment.codepipeline-attachaws_s3_bucket.artifact_bucketCodePipe line 콘솔 에서 확인하면 권한이 없어 실패한 화면이 나올 것 입니다.이를 해결하기 위해 또 다른 권한이 필요합니다.CodePipeline TriggerCodeCommit에서 발생한 이벤트가 CodePipeline으로 트리거되기 위해서는 아래 정의된 권한이 필요합니다.➕ 아래 코드를 codepipeline.tf에 추가하고 인프라를 생성해주세요.  resource &quot;aws_iam_role&quot; &quot;trigger_role&quot; {  name               = &quot;terraform-trigger&quot;  assume_role_policy = &lt;&lt;EOF{  &quot;Version&quot;: &quot;2012-10-17&quot;,  &quot;Statement&quot;: [    {      &quot;Action&quot;: &quot;sts:AssumeRole&quot;,      &quot;Principal&quot;: {        &quot;Service&quot;: &quot;events.amazonaws.com&quot;      },      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Sid&quot;: &quot;&quot;    }  ]}EOF}resource &quot;aws_iam_policy&quot; &quot;trigger_policy&quot; {  description = &quot;CodePipeline Trigger Execution Policy&quot;  policy      = &lt;&lt;EOF{  &quot;Version&quot;: &quot;2012-10-17&quot;,  &quot;Statement&quot;: [    {      &quot;Action&quot;: [        &quot;codepipeline:StartPipelineExecution&quot;      ],      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Resource&quot;: &quot;${aws_codepipeline.pipeline.arn}&quot;    }  ]}EOF}resource &quot;aws_iam_role_policy_attachment&quot; &quot;trigger-attach&quot; {  role       = aws_iam_role.trigger_role.name  policy_arn = aws_iam_policy.trigger_policy.arn}ResultTrigger 까지 정상적으로 적용하고 테스트용으로 활용할 아무 Dockerfile을 CodeCommit에 Push합니다.다시 CodePipeline 콘솔 에 접속해 우상단에 위치한 변경사항 릴리스를 누르면,아래와 같이 정상적으로 코드 파이프라인이 작동하여 운영되는 것을 확인 할 수 있습니다.CleanupS3 bucket은 빈상태여야 제거가 가능하기에 S3 콘솔 에서 ecr-pipeline의 데이터를 모두 삭제합니다.이어서 terraform destory 명령어로 모든 리소스를 회수합니다.총 3편에 걸쳐서 테라폼으로 최소한의 리소스로 ECR Pipeline 구축법을 알아보았습니다. (CloudWatch 기능을 추가해 CodePipeline을 구축해보세요 👍)해당 과정을 통해 AWS 인프라 생성법과, IAM 활용법, Variable, Output, tfvars 등을 활용해 코드를 작성하는 법을 공부했습니다.다른 CI/CD 파이프라인 구축법도 이번 포스팅에서 다룬 방법과 크게 다르지 않으니, 해당 포스팅이 도움이 되면 좋겠습니다. 😁Terraform Courses    3-Tier VPC Architecture with Terraform    ECR Pipeline with Terraform Ⅰ (CodeCommit)    ECR Pipeline with Terraform Ⅱ (ECR, CodeBuild, IAM)    ECR Pipeline with Terraform Ⅲ (CodePipeline)    Provision Infrastructure with Packer",
        "url": "/CodePipeline"
    }
    ,
    
    "codebuild": {
        "title": "ECR CodePipeline with Terraform Ⅱ",
            "author": "HeuristicWave",
            "category": "",
            "content": "Terraform으로 ECR 파이프라인 구축하기 2 (ECR, CodeBuild, IAM)2편에서는 ECR과 CodeBuild를 생성하고 IAM 역할, 정책을 부여하는 법을 학습합니다.ECRECR 역시 공식 문서 에서 사용방법을 확인합니다.공식문서에서 image_scanning_configuration config를 사용하면 취약점 스캔이 가능하다 설명되어 있지만, 필요하지 않기 때문에 제외하겠습니다.더불어, output도 함께 작성하겠습니다.cat &lt;&lt;EOF &gt; ecr.tfresource \"aws_ecr_repository\" \"image_repo\" {  name                 = var.image_repo_name  image_tag_mutability = \"MUTABLE\"}output \"image_repo_url\" {  value = aws_ecr_repository.image_repo.repository_url}output \"image_repo_arn\" {  value = aws_ecr_repository.image_repo.arn}EOF이어서 ecr.tf에서 변수로 사용하기 위한 var.image_repo_name 부분이 작동하도록 1편에서 작성한 variables.tf 아래 값을 추가합니다.✅ 편의상 이번 단계에 필요한 variable을 함께 포함했습니다.variable \"image_repo_name\" {  description = \"Image repo name\"  type        = string}variable \"container_name\" {  description = \"Container Name\"  default     = \"my-container\"}variable \"source_repo_branch\" {  description = \"Source repo branch\"  type        = string}ecr 작성을 완료햇으니 plan, apply 명령어를 차례로 입력해 인프라를 생성하고 terraform state list명령어나 콘솔 에서 생성된 인프라를 확인합니다.CodeBuildCodeBuild를 사용하기 위해 Terraform 도큐먼트 에서 사용법을 확인합니다.기존까지의 작업과는 달리 상당히 어려워 보입니다. 그러나 쓱 훝어보면 크게 4가지(bucket, IAM Role과 Policy, Codebuild)로 정리됩니다.Bucket도큐먼트와 같이 우선적으로 S3를 생성합니다. bucket의 이름은 선택이지만, 여러개의 버킷을 가지고 있는 저는 식별을 위해 이름을 부여했습니다.cat &lt;&lt;EOF &gt; codebuild.tfresource \"aws_s3_bucket\" \"artifact_bucket\" {  bucket = \"ecr-pipeline\"}EOFIAM Role도큐먼트를 따라 AssumeRole을 사용합시다. ➕ S3을 만들때 사용한 codebuild.tf에 아래 코드를 추가합니다.resource \"aws_iam_role\" \"codebuild_role\" {  name = \"terraform-codebuild\",  assume_role_policy = &lt;&lt;EOF{   \"Version\": \"2012-10-17\",   \"Statement\": [      {         \"Effect\": \"Allow\",         \"Principal\": {            \"Service\": \"codebuild.amazonaws.com\"         },         \"Action\": \"sts:AssumeRole\"      }   ]}EOF}IAM Policy정책은 IAM 콘솔에서 기존에 만들어진 정책을 사용할 수도 있지만, 아래와 같이 직접 작성할 수도 있습니다.도큐먼트에서 EC2에 대한 정책을 사용하지만, 우리는 ECR을 사용하므로 아래와 같은 정책을 사용하겠습니다.  resource &quot;aws_iam_policy&quot; &quot;codebuild_policy&quot; {  description = &quot;CodeBuild Execution Policy&quot;  policy      = &lt;&lt;EOF{  &quot;Version&quot;: &quot;2012-10-17&quot;,  &quot;Statement&quot;: [    {      &quot;Action&quot;: [        &quot;logs:CreateLogGroup&quot;, &quot;logs:CreateLogStream&quot;, &quot;logs:PutLogEvents&quot;,        &quot;ecr:GetAuthorizationToken&quot;      ],      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Resource&quot;: &quot;*&quot;    },    {      &quot;Action&quot;: [        &quot;s3:GetObject&quot;, &quot;s3:GetObjectVersion&quot;, &quot;s3:PutObject&quot;      ],      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Resource&quot;: &quot;${aws_s3_bucket.artifact_bucket.arn}/*&quot;    },    {      &quot;Action&quot;: [        &quot;ecr:GetDownloadUrlForLayer&quot;, &quot;ecr:BatchGetImage&quot;,        &quot;ecr:BatchCheckLayerAvailability&quot;, &quot;ecr:PutImage&quot;,        &quot;ecr:InitiateLayerUpload&quot;, &quot;ecr:UploadLayerPart&quot;,        &quot;ecr:CompleteLayerUpload&quot;      ],      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Resource&quot;: &quot;${aws_ecr_repository.image_repo.arn}&quot;    }  ]}EOF}20, 30라인에서 앞서 생성한 리소스를 ${채움참조} 문법으로 유연한 코드를 작성합니다.🚩 이어서 생성한 Policy를 Role에 부여합니다. 이것 역시 codebuild.tf에 추가합니다.resource \"aws_iam_role_policy_attachment\" \"codebuild-attach\" {  role       = aws_iam_role.codebuild_role.name  policy_arn = aws_iam_policy.codebuild_policy.arn}CodeBuildTerraform 도큐먼트 를 보아도 어떻게 해야 ECR에 적용시킬 수 있는지 알기 어렵습니다.우선 CodeBuild를 이해하기 위해 AWS docs 를 읽어봅시다.대략 리소스 이름을 정하고, 환경을 구성하고 빌드를 하기 위한 방법을 정의해야 한다는 사실을 알 수 있습니다.CodeBuild가 정의된 아래 코드를 활용해 codebuild.tf에 추가합니다.  resource &quot;aws_codebuild_project&quot; &quot;codebuild&quot; {  name         = &quot;codebuild-${var.source_repo_name}-${var.source_repo_branch}&quot;  service_role = aws_iam_role.codebuild_role.arn  artifacts {    type = &quot;CODEPIPELINE&quot;  }    environment {    compute_type                = &quot;BUILD_GENERAL1_MEDIUM&quot;    image                       = &quot;aws/codebuild/standard:3.0&quot;    type                        = &quot;LINUX_CONTAINER&quot;    privileged_mode             = true    image_pull_credentials_type = &quot;CODEBUILD&quot;    environment_variable {      name  = &quot;REPOSITORY_URI&quot;      value = aws_ecr_repository.image_repo.repository_url    }    environment_variable {      name  = &quot;AWS_DEFAULT_REGION&quot;      value = var.aws_region    }    environment_variable {      name  = &quot;CONTAINER_NAME&quot;      value = var.container_name    }  }  source {    type      = &quot;CODEPIPELINE&quot;    buildspec = &lt;&lt;BUILDSPEC${file(&quot;buildspec.yml&quot;)}BUILDSPEC  }}31라인이 참조하는 buildspec.yml을 pre_build, build, post_build에 맞춰 작성합니다.cat &lt;&lt;EOF &gt; buildspec.ymlversion: 0.2phases:  install:    runtime-versions:      docker: 18  pre_build:    commands:      - echo Logging in to Amazon ECR...      - $(aws ecr get-login --region $AWS_DEFAULT_REGION --no-include-email)      - COMMIT_HASH=$(echo $CODEBUILD_RESOLVED_SOURCE_VERSION | cut -c 1-7)      - IMAGE_TAG=${COMMIT_HASH:=latest}  build:    commands:      - echo Build started on `date`      - echo Building the Docker image...      - docker build -t $REPOSITORY_URI:latest .      - docker tag $REPOSITORY_URI:latest $REPOSITORY_URI:$IMAGE_TAG  post_build:    commands:      - echo Build completed on `date`      - echo Pushing the Docker image...      - docker push $REPOSITORY_URI:latest      - docker push $REPOSITORY_URI:$IMAGE_TAG      - echo Writing image definitions file...      - printf '[{\"name\":\"%s\",\"imageUri\":\"%s\"}]' $CONTAINER_NAME $REPOSITORY_URI:$IMAGE_TAG &gt; imagedefinitions.jsonartifacts:  files: imagedefinitions.jsonEOF지금까지 작성된 인프라를 terraform state list명령어를 통해 확인하면 아래와 같습니다.❯ terraform state listaws_codebuild_project.codebuildaws_codecommit_repository.testaws_ecr_repository.image_repoaws_iam_policy.codebuild_policyaws_iam_role.codebuild_roleaws_iam_role_policy_attachment.codebuild-attachaws_s3_bucket.artifact_bucket  생성한 인프라가 위와 같지 않을 경우, 👉 Click  실수로 의도치 않은 인프라가 프로비저닝 되었다면 2가지 방법을 통해 원 상태로 복구 할 수 있습니다.      terraform destroy 명령어로 특정 인프라만 되돌리거나 프로비저닝 하고싶은 경우, -target 옵션과 함께 resource 명으로 명령어를 작성합니다. 예시) terraform destory -target aws_vpc.main    잘못 작성한 코드를 수정 후, terraform apply명령어를 적용하여 최신 상태의 인프라를 반영합니다.  Terraform Courses    3-Tier VPC Architecture with Terraform    ECR Pipeline with Terraform Ⅰ (CodeCommit)    ECR Pipeline with Terraform Ⅱ (ECR, CodeBuild, IAM)    ECR Pipeline with Terraform Ⅲ (CodePipeline)    Provision Infrastructure with Packer",
        "url": "/CodeBuild"
    }
    ,
    
    "codecommit": {
        "title": "ECR CodePipeline with Terraform Ⅰ",
            "author": "HeuristicWave",
            "category": "",
            "content": "Terraform으로 ECR 파이프라인 구축하기 1 (CodeCommit)Overview이번 포스팅에서는 커밋 후, 도커의 이미지를 자동으로 배포하는 ECR Pipeline을 테라폼으로 생성해보겠습니다.AWS에서 저장소 역할을 하는 CodeCommit, 코드를 빌드하는 CodeBuild, 파이프라인을 자동화 하는 CodePipeline, 컨테이너 이미지를 저장하는 ECR을 활용해 구축합니다.1편에서는 CodeCommit 구축과 terraform의 Output, Variables, tfvars 등을 배워 보겠습니다.준비 작업이번 포스팅의 작업공간(~/terraform)을 생성하고 해당 위치에서 아래 코드 블럭을 터미널에 복사합니다.cat &lt;&lt;EOF &gt; provider.tfprovider \"aws\" {  region  = var.aws_region}EOF이후, terraform init 명령어를 실행시켜주세요.CodeCommitCodeCommit을 사용하기 위해 Terraform 도큐먼트 에서 사용법을 확인합니다.링크의 Example Usage를 활용해 코드를 작성할 수도 있지만, 이번 포스팅에서는 제 방식대로아래 코드를 활용해 작성해보겠습니다.링크에서 소개하는 코드와 다른 부분은 output과 variable의 사용 여부입니다.✅ 아래 코드와 도큐먼트의 코드가 어떻게 다른지 꼭 확인해보세요!cat &lt;&lt;EOF &gt; codecommit.tfresource \"aws_codecommit_repository\" \"test\" {  repository_name = var.source_repo_name  description     = \"This is the Sample App Repository\"}output \"source_repo_clone_url_http\" {  value = aws_codecommit_repository.test.clone_url_http}EOFOutput은 향후 clone할 원격 저장소의 위치를 파악하기 위해 넣어줍니다. 또한 Variable을 사용해 보다 유연한 코드를 작성해 보겠습니다.준비 작업에 정의한 리전과 CodeCommit Repo 이름에 Variable을 사용하겠습니다.cat &lt;&lt;EOF &gt; variables.tfvariable \"aws_region\" {  description = \"The AWS region\"  default     = \"ap-northeast-2\"}variable \"source_repo_name\" {  description = \"Source repo name\"  type        = string}EOF위 코드를 복사한 후, terraform plan 명령어로 아래와 같은 화면을 확인 할 수 있습니다.앞서 작성한 variables.tf의 region은 default 값이 있지만, repository는 variable의 형식만 정의되어 있기 때문에 인프라를 생성할 때 필수적으로 이름을 입력받습니다.✅ variable의 input값을 수기로 작성하는 것을 피하고 싶으면 tfvars를 사용합니다. 편의상 이번 프로젝트에서 사용할 값들을 미리 작성하겠습니다.cat &lt;&lt;EOF &gt; terraform.tfvarsaws_ecr=\"my-image\"source_repo_name=\"my-pipeline\"source_repo_branch=\"master\"image_repo_name=\"my-pipeline\"EOFtfvars는 위와 같이 변수의 값을 지정하기도 하지만, .env처럼 외부로 노출하면 안되는 값을 넣어두고 git에 ignore시켜 사용하기도 합니다.위 작업을 진행 후, terraform apply명령어를 적용하면 “Apply complete”과 함께 Outputs 값이 나옵니다.terraform state list명령어 이외에도, 콘솔 로 이동하면 생성된 인프라를 확인 할 수 있습니다.생성된 원격저장소를 사용하기 위해 terraform output을 활용해 export 환경 변수를 지정합니다.export tf_source_repo_clone_url_http=$(terraform output source_repo_clone_url_http)echo $tf_source_repo_clone_url_http\t# 확인Git SettingCodeCommit의 Repo 활용법은 아래 2가지 방법이 있습니다.1. 로컬에 위치한 코드를 CodeCommit에 push하기 (원격저장소가 비어있음)로컬의 빈공간에서 CodeCommit Repo 사용을 위한 git remote 지정git initgit remote add origin $tf_source_repo_clone_url_httpgit remote -v   # 원격 저장소 확인코드를 작성하고 CodeCommit에 Push하기git add .git commit -m \"First commit\"git statusgit push origin # master branch로 push자격 증명 문제가 있다면 아래 명령어로 해결합니다. 자격 증명 헬퍼 및 AWS CodeCommit에 대한 HTTPS 연결 문제 해결git config --global credential.helper '!aws codecommit credential-helper $@'git config --global credential.UseHttpPath true2. 로컬에 원격저장소의 코드를 clone하기 (원격저장소가 비어있지 않음)git clone $tf_source_repo_clone_url_http지금까지 도큐먼트를 활용해 코드를 작성하고, variable, output, tfvars의 활용법을 배워보았습니다.앞서 작성된 작업들이 정상적으로 커밋과 clone이 가능하면, 다음 단계로 🚀Terraform Courses    3-Tier VPC Architecture with Terraform    ECR Pipeline with Terraform Ⅰ (CodeCommit)    ECR Pipeline with Terraform Ⅱ (ECR, CodeBuild, IAM)    ECR Pipeline with Terraform Ⅲ (CodePipeline)    Provision Infrastructure with Packer",
        "url": "/CodeCommit"
    }
    ,
    
    "3tier": {
        "title": "3-Tier VPC Architecture with Terraform",
            "author": "HeuristicWave",
            "category": "",
            "content": "본 글은 Configure and Deploying VPCs with Multiple Subnets 에서 다루는 Production-Ready: The 3-Tier VPC 강의를 바탕으로, 테라폼으로 구축하는 3계층 VPC 아키텍처에 대한 글입니다.  Multi-Tier VPC 란?  VPC를 구축할 때 단일 계층 VPC에 모든 자원을 넣는다면, 네트워크에 접근할 수 있는 잠재적 공격자에게 자원이 노출됩니다. 이를 보완하기 위해 서브넷으로다중 계층 VPC 아키텍처를 만들어 방어 계층을 이룰 수 있습니다.  디자인 패턴 : 3 Tier VPC Architecture테라폼 코드는 모듈로 관리하는 것을 권장하지만, 이번 포스팅에서는 3-tier 아키텍처 중 네트워크와 관련된 부분만을 다뤄 하나의 파일에서 코드를 관리합니다.  Step 0 (테라폼을 활용하실 줄 안다면 넘어가세요)  Step 0  아키텍처를 구성할 폴더를 만들고 provider를 주입합니다.  mkdir architecturecd architectureterraform inittouch threeTierVPC.tf    threeTierVPC.tf에 벤더 정보를 작성합니다.  provider \"aws\" {  region = \"ap-northeast-2\"}    명령어terraform plan, terraform apply를 통해, 오류 없이 통과하는 화면을 확인하고 다음 단계로 🚀      terraform apply로 인프라를 반영 할때, -auto-approve옵션을 주면 yes입력 없이 진행 할 수 있습니다. 그러나 yes를 입력하기 전, 한번 더 검토할 수 있는 기회가 있으므로 권장하지 않습니다.  💡각 소제목 링크에 첨부된 코드를 활용해 Step 0 에서 만든 threeTierVPC.tf에 이어서 작성하거나, 따로 새로운 파일을 만들어 terraform plan, terraform apply 명령어를 차례로 작성하며 계층을 쌓아 올립니다.Step 1Layer 1️⃣ : Public subnet하나의 VPC에 2개의 AZ를 만들고 각각의 Public 서브넷을 위치시킵니다.  퍼블릭 서브넷은 프라이빗 서브넷 보다 적은 수의 IP 예약하는 것이 좋습니다.Step 1 코드를 적용 후, 명령어로 인프라 상태를 점검해 아래와 같다면 다음 단계로 🚀$ terraform state listdata.aws_availability_zones.availableaws_internet_gateway.igwaws_subnet.pub_sub_1aws_subnet.pub_sub_2aws_vpc.main  생성한 인프라가 위와 같지 않을 경우  실수로 의도치 않은 인프라가 프로비저닝 되었다면 2가지 방법을 통해 원 상태로 복구 할 수 있습니다.      terraform destroy 명령어로 특정 인프라만 되돌리거나 프로비저닝 하고싶은 경우, -target 옵션과 함께 resource 명으로 명령어를 작성합니다. 예시) terraform destory -target aws_vpc.main    잘못 작성한 코드를 수정 후, terraform apply명령어를 적용하여 최신 상태의 인프라를 반영합니다.  Step 2Layer 1️⃣ : Internet access resources외부 인터넷과의 노출을 제한하고 나가는 트래픽을 위해 NAT Gateway를 활용합니다. 또한 들어오는 트래픽을 위해 ALB를 위치시켰습니다.로드밸런서와 NAT Gateway는 가용성이 높은 관리형 서비스로 병목 현상에 대해 걱정할 필요가 없습니다.  💡Nat Gateway 알아보기  NAT(네트워크 주소 변환) 게이트웨이를 사용하면 프라이빗 서브넷의 인스턴스를 인터넷 또는 기타 AWS 서비스에 연결하는 한편, 인터넷에서 해당 인스턴스와의 연결을 시작하지 못하게 할 수 있습니다.NAT 게이트웨이를 만들려면 NAT 게이트웨이가 속할 퍼블릭 서브넷을 지정해야 하기 때문에 Step2에서 우선적으로 생성합니다.  도큐먼트로 더 알아보기  그림에서는 보이지 않지만, VPC에는 암시적 라우터가 있으며 라우팅 테이블을 사용하여 네트워크 트래픽이 전달되는 위치를 제어합니다.VPC의 각 서브넷을 라우팅 테이블에 연결해야 합니다. 테이블에서는 서브넷에 대한 라우팅을 제어합니다.Step 2 코드를 적용 후, 명령어로 인프라 상태를 점검해 아래와 같다면 다음 단계로 🚀$ terraform state listdata.aws_availability_zones.availableaws_eip.nat_1aws_eip.nat_2aws_internet_gateway.igwaws_nat_gateway.nat_gateway_1aws_nat_gateway.nat_gateway_2aws_route_table.route_table_pubaws_route_table_association.route_table_association_1aws_route_table_association.route_table_association_2aws_subnet.pub_sub_1aws_subnet.pub_sub_2aws_vpc.main이번 포스팅에서 ALB와 인스턴스는 다루지 않습니다. 추후, 모듈로 테라폼을 관리하는 방법에서 학습하겠습니다. Step 3Layer 2️⃣️ : Apps in a private subnet2개의 프라이빗 서브넷에 각각의 인스턴스를 놓습니다. 이후, 두 퍼블릭 서브넷에 연결된 ALB는 프라이빗 서브넷 리소스 간의 트래픽을 분산시킵니다.  ❗️예제 그림에서는 Private subnet의 cidr block을 10.0.2.0/22로 가이드 하지만, 이는 앞서 만든 서브넷과 범위가 겹치므로 10.0.4.0/22로 바꿔 진행합니다.CIDR 계산기 에서 정확하게 확인해 볼 수 있습니다.  Step3에서도 Step2와 같이 그림에서는 보이지 않는 라우트 테이블을 만들고 NAT 게이트웨이와 프라이빗 서브넷을 연결해 줍니다.프라이빗 서브넷의 요청이 외부로 나갈때는 NAT 게이트웨이의 고정 IP를 사용합니다.(프라이빗 서브넷의 라우트 테이블은 퍼블릭과 달리 2개를 만들어 각각 연결해 주었습니다.)Step 3 코드를 적용 후, 명령어로 인프라 상태를 점검해 아래와 같다면 다음 단계로 🚀$ terraform state listdata.aws_availability_zones.availableaws_eip.nat_1aws_eip.nat_2aws_internet_gateway.igwaws_nat_gateway.nat_gateway_1aws_nat_gateway.nat_gateway_2aws_route.private_nat_1aws_route.private_nat_2aws_route_table.route_table_pri_1aws_route_table.route_table_pri_2aws_route_table.route_table_pubaws_route_table_association.route_table_association_1aws_route_table_association.route_table_association_2aws_route_table_association.route_table_pri_association_1aws_route_table_association.route_table_pri_association_2aws_subnet.pri_sub_1aws_subnet.pri_sub_2aws_subnet.pub_sub_1aws_subnet.pub_sub_2aws_vpc.main이번 포스팅에서 DB와 인스턴스 연결은 다루지 않습니다. 추후, 모듈로 테라폼을 관리하는 방법에서 학습하겠습니다. Step 4Layer 3️⃣ : Data in a second private subnet첫 번째 프라이빗 서브넷 뒤에 두 번째 프라이빗 서브넷을 배치합니다. (코드 생략) 장애가 발생할 경우를 대비해 read-replica 혹은 standby 구성으로 배치합니다.  ❗Step4의 서브넷 작성법은 Step3의 방법과 동일합니다. 아래 사진은 CIDR의 범위가 겹치므로, Private subnet의 cidr block을 10.0.8.0/23을 10.0.12.0/23로, 10.0.10.0/23을 10.0.14.0/23으로 바꿔 진행하세요.  데이터 리소스(/23)보다를 앱 리소스(/22)를 확장할 가능성이 커, 더 큰 서브넷 마스크를 할당합니다.Step 5Leave extra IPs available배포된 인프라가 확장되어 아키텍처가 변경될 때 사용할 수 있는 여유분의 IP를 예약을 할 수도 있습니다. (코드 생략)위와 같은 단계들을 통해 3 Tier VPC Architecture를 학습해보았습니다.Terraform으로 생성된 자원들은 terraform destory명령어를 통해 학습을 시작하기 전 상태로 되돌리세요.다음 포스팅에서는 이번 포스팅에서 생략했던 인스턴스, DB, LB 등을 모듈로 관리하며 다뤄 보겠습니다.Terraform Courses    3-Tier VPC Architecture with Terraform    ECR Pipeline with Terraform Ⅰ (CodeCommit)    ECR Pipeline with Terraform Ⅱ (ECR, CodeBuild, IAM)    ECR Pipeline with Terraform Ⅲ (CodePipeline)    Provision Infrastructure with Packer",
        "url": "/3Tier"
    }
    ,
    
    "pipenv01": {
        "title": "Pipenv, Nginx, Gunicorn 서버 운영하기",
            "author": "HeuristicWave",
            "category": "",
            "content": "Virtualenv 환경이 아닌 Pipenv를 사용하며 만난 에러 해결 과정 정리사전 작업자세한 방법은 하단 참고자료를 통해 확인 할 수 있다      gunicorn, nginx 설치          ec2에서 nginx 설치하기 : CentOS 7 Nginx 설치 방법            gunicorn 작동확인  Gunicorn 🦄서비스 등록 스크립트 생성/etc/systemd/system/gunicorn.service 파일을 아래와 같은 내용으로 생성.pipenv는 venv와 ExecStart 경로가 다르다는 점을 유념해 작성하자[Unit]Description=gunicorn daemonAfter=network.target[Service]User=ec2-userGroup=ec2-userWorkingDirectory=/home/ec2-user/django/repoExecStart=/usr/local/bin/pipenv run gunicorn --workers 3 \\        &lt;wsgi가 위치한 폴더&gt;.wsgi:application --bind 0.0.0.0:8000[Install]WantedBy=multi-user.target  본래 --bind부분에 unix:/home/ec2-user/django/gunicorn.sock 를 넣어 구동하면 repo의 상위 폴더에  gunicorn.sock가 생긴다.nginx의 proxy_pass 부분도 http://unix:/{$PATH}/gunicorn.sock을 기재해 sock로 구성하는 것이 맞는 방법 같은데… 이 부분에 대해서는 학습이 필요하다.서비스 등록sudo systemctl start gunicornsudo systemctl enable gunicorn서비스 구동 확인sudo systemctl status gunicornNginx사이트 설정 추가ec2에 nginx를 받았을 때, etc/nginx/sites-enabled 와 etc/nginx/sites-availabe 이 존재하지 않는다. 해당 경로에 없다면 만들어주고 있으면 default 파일을 삭제하자.server {        listen 80;        server_name &lt;IP or 도메인&gt;;        charset utf-8;        location / {                include proxy_params;                proxy_pass http://0.0.0.0:8000        }        location /static/ {                root /home/ec2-user/django/repo;        }                location /media/ {                root /home/ec2-user/django/repo;        }}  nginx 주요 개념, nginx : root vs aliasinclude proxy_params의 경우 /etc/nginx/proxy_params 에 프록시 헤더를 기재 해야 한다. (다음 링크 참고) nginx &amp; aws사이트 추가sudo ln -s /etc/nginx/sites-available/django_test /etc/nginx/sites-enabled기동sudo systemctl start nginx기타 도움이 되는 명령어sudo systemctl daemon-reloadsudo systemctl stop, restart nginxps -efpspkill gunicorn  dotenv 관련 에러 해결하기            gunicorn 을 활용해 연결 할 경우      $ pip uninstall dotenv$ pip install python-dotenv                     docker + nginx + gunicorn 을 활용할 경우      $ pip uninstall dotenv$ pip install python-dotenv             참고자료  gunicorn 사전작업  Nginx, Gunicorn, Django 연동하기",
        "url": "/pipenv01"
    }
    
    
    };
</script>
<script src="assets/js/lunr.js"></script>
<script src="assets/js/search.js"></script>

            </section>

        </article>

    </div>
</main>

<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->
<script>
$(function() {
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
});
</script>



        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="https://heuristicwave.github.io/">Heuristic Wave Blog</a> &copy; 2022</section>
                <section class="poweredby">Proudly published with <a href="https://jekyllrb.com/">Jekyll</a> &
                    <a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a> using
                    <a href="https://github.com/jekyller/jasper2" target="_blank" rel="noopener">Jasper2</a></section>
                <nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    
                    
                    <a href="https://ghost.org" target="_blank" rel="noopener">Ghost</a>
                </nav>
            </div>
        </footer>

    </div>

    <!-- The big email subscribe modal content -->
    
        <div id="subscribe" class="subscribe-overlay">
            <a class="subscribe-overlay-close" href="#"></a>
            <div class="subscribe-overlay-content">
                
                <h1 class="subscribe-overlay-title">Search on Heuristic Wave Blog</h1>
                <p class="subscribe-overlay-description">lunr.js를 이용한 posts 검색</p>
                <span id="searchform" method="post" action="/search/" class="">
    <input class="confirm" type="hidden" name="confirm"  />
    <input class="location" type="hidden" name="location"  />
    <input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()"
               id="searchtext" type="text" name="searchtext"
               placeholder="Search..." />
    </div>
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
</span>
                <br>
            </div>
        </div>
    

    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.10.0/components/prism-abap.min.js"></script>
    <script>$(document).ready(function() {
      $('pre code').each(function(i, block) {
        hljs.highlightBlock(block);
      });
    });</script>

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://demo.ghost.io/assets/js/jquery.fitvids.js?v=724281a32e"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google Analytics Tracking code -->
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'G-0FTXSPJZFY', 'auto');
  ga('send', 'pageview');

 </script>


    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>
$(function() {
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
});
</script>

    

    <!-- Ghost outputs important scripts and data with this tag - it should always be the very last thing before the closing body tag -->
    <!-- ghost_foot -->

</body>
</html>
