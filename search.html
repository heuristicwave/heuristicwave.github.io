<!DOCTYPE html>
<html>
<head>

    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>Search Result</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />

    <!-- Custom.css -->
    <link rel="stylesheet" type="text/css" href="/assets/built/custom.css" />

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">

    <!-- webfont -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/earlyaccess/nanumgothic.css">

    <!-- syntax.css -->
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />

    <!-- highlight.js -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>

    <!--[if IE]>
        <style>
            p, ol, ul{
                width: 100%;
            }
            blockquote{
                width: 100%;
            }
        </style>
    <![endif]-->
    
    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="Careful Writer" />
    <link rel="shortcut icon" href="https://heuristicwave.github.io/assets/built/images/water-wave-48.png" type="image/png" />
    <link rel="canonical" href="https://heuristicwave.github.io/search" />
    <meta name="referrer" content="no-referrer-when-downgrade" />

     <!--title below is coming from _includes/dynamic_title-->
    <meta property="og:site_name" content="Heuristic Wave Blog" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="Search Result" />
    <meta property="og:description" content="Careful Writer" />
    <meta property="og:url" content="https://heuristicwave.github.io/search" />
    <meta property="og:image" content="https://heuristicwave.github.io/assets/built/images/blog-cover.jpg" />
    <meta property="article:publisher" content="https://www.facebook.com/" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Search Result" />
    <meta name="twitter:description" content="Careful Writer" />
    <meta name="twitter:url" content="https://heuristicwave.github.io/" />
    <meta name="twitter:image" content="https://heuristicwave.github.io/assets/built/images/blog-cover.jpg" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Heuristic Wave Blog" />
    <meta name="twitter:site" content="@" />
    <meta name="twitter:creator" content="@" />
    <meta property="og:image:width" content="2000" />
    <meta property="og:image:height" content="666" />

    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "Heuristic Wave Blog",
        "logo": "https://heuristicwave.github.io/"
    },
    "url": "https://heuristicwave.github.io/search",
    "image": {
        "@type": "ImageObject",
        "url": "https://heuristicwave.github.io/assets/built/images/blog-cover.jpg",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://heuristicwave.github.io/search"
    },
    "description": "Careful Writer"
}
    </script>
    <script data-ad-client="ca-pub-6093187208665634" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <!-- <script type="text/javascript" src="https://demo.ghost.io/public/ghost-sdk.min.js?v=724281a32e"></script>
    <script type="text/javascript">
    ghost.init({
    	clientId: "ghost-frontend",
    	clientSecret: "f84a07a72b17"
    });
    </script> -->

    <meta name="generator" content="Jekyll 3.6.2" />
    <link rel="alternate" type="application/rss+xml" title="Search Result" href="https://heuristicwave.github.io/feed.xml" />


</head>
<body class="page-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- < default -->
<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<!-- The big featured header, it uses blog cover image as a BG if available -->
<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        
            
                <a class="site-nav-logo" href="https://heuristicwave.github.io/">Heuristic Wave Blog</a>
            
        
        
            <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-about" role="menuitem"><a href="/about/">About</a></li>
    <li class="nav-projects" role="menuitem"><a href="/tag/projects/">Projects</a></li>
    <li class="nav-devops" role="menuitem"><a href="/tag/devops/">DevOps</a></li>
    <li class="nav-aws" role="menuitem"><a href="/tag/aws/">AWS</a></li>
    <li class="nav-backend" role="menuitem"><a href="/tag/backend/">Back-end</a></li>
    <li class="nav-archive" role="menuitem">
        <a href="/archive">All Posts</a>
    </li>
    <li class="nav-archive" role="menuitem">
        <a href="/author_archive.html">Tag별 Posts</a>
    </li>
</ul>

        
    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
            
        </div>
        
            <a class="subscribe-button" href="#subscribe">Search</a>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full  post page no-image">

            <header class="post-full-header">
                <h1 class="post-full-title">Search Result</h1>
            </header>

            

            <section class="post-full-content">
                <form action="/search" method="get" hidden="hidden">
    <label for="search-box"></label>
    <input type="text" id="search-box" name="query">
</form>

<ul class="mylist" id="search-results"></ul>

<script>
    window.store = {
    
    "constructhub": {
        "title": "AWS CDK Library, Construct Hub",
            "author": "HeuristicWave",
            "category": "",
            "content": "본 글은 지난 AWS re:Invent 2021에서 발표된 Construct Hub를 둘러보며 느낀 첫인상에 대한 글입니다.IntroAWS re:Invent 2021, 아마존 CTO인 Dr. Werner Vogels 키노트 에서 AWS CDK 라이브러리들을 활용할 수 있는 Construct Hub 가 발표되었습니다.보통 AWS 리인벤트에서는 AWS 고유의 서비스들이 소개되는데요,이번에 말씀드릴 Construct Hub는 AWS의 솔루션이 아닌 오픈 소스 커뮤니티이자,CDK 라이브러리를 검색하고 공유할 수 있는 장이라고 할 수 있습니다.Construct Hub를 통해 AWS가 그리는 IaC(Infrastructure as Code)의 미래를 함께 만나보겠습니다.우선, Construct Hub를 소개하기 앞서 AWS의 IaC 도구들에 대하여 간략하게 알아보겠습니다.️📂 AWS CloudFormation2011년 AWS의 리소스를 JSON 또는 YAML 형식으로 인프라를 템플릿화 할 수 있는 CloudFormation을 발표했습니다.  예) AWS S3 버킷 리소스를 제어하는 CloudFormation 코드  Resources:  HelloBucket:    Type: AWS::S3::Bucket    Properties:      AccessControl: PublicRead      WebsiteConfiguration:        IndexDocument: index.html        ErrorDocument: error.html그러나, 인프라가 복잡해질수록 CloudFormation 코드의 길이도 길어지고 리소스 간의 관계도 파악하기 어려워 관리의 피로도가 증가하게 되었습니다.AWS는 이러한 CloudFormation의 약점을 보완하고 개발자들이 YAML 형식으로 인프라를 정의하는 게 아니라,선호하는 프로그래밍 언어로 클라우드 인프라를 정의할 수 있도록 Cloud Development Kit(CDK) 를 2019년에 정식으로 출시했습니다.⌨️ AWS Cloud Development KitCDK는 익숙한 프로그래밍 언어를 사용해 클라우드 애플리케이션 리소스를 정의할 수 있는 오픈 소스 소프트웨어 개발 프레임워크입니다.CDK는 어떤 방식으로, 앞서 언급된 CloudFormation의 약점들을 보완하는지 그림과 함께 알아보겠습니다.CDK는 Construct의 집합체라 말할 수 있습니다.Construct는 클라우드 서비스를 이루는 컴포넌트라 생각하셔도 좋습니다.CDK 프레임워크를 통해 개발자 혹은 클라우드 운영자는 타입스크립트, 파이썬, 닷넷, 자바 등의 익숙한 프로그래밍 언어 중하나를 선택하여, construct 라이브러리를 사용해 프로그래밍 합니다.하나의 Stack으로 엮어진 소스코드를 cdk synth 라는 명령어로 CloudFormation에서 사용되는 템플릿으로 변환하고 cdk deploy 명령어로 인프라를 배포합니다.CDK로 작성된 인프라 코드는 프로그래밍 언어의 이점을 그대로 채택하여,해당 언어에 익숙한 사람이라면 YAML로 작성된 CloudFormation 보다 인프라의 환경을 파악하기 쉽습니다.실제로 올해 키노트에 언급된 Liberty Mutual INSURANCE사의 CDK 도입 사례 에서1500라인의 CloudFormation 코드를 CDK에서 단 14줄로 구현하며 CDK의 뛰어난 가시성을 알렸습니다.🗂 Construct Hub대망의 Construct Hub를 소개하기 위해 먼 길을 돌아왔습니다.😓맨 처음에 Construct Hub를 오픈 소스 커뮤니티이자, CDK 라이브러리를 검색하고 공유할 수 있는 장이라고 소개한 말이 맞는지 그림과 함께 확인해 보겠습니다.Construct Hub 의 메인 홈페이지를 확인해 보면 CDK 라이브러리를 검색할 수 있는‘서치 바’와 현재 ‘지원하는 언어’와 ‘서비스 프로바이더’(AWS, Datadog, Mongo DB, Aqua Security 등)가 보입니다.이 중에서도 화면 왼쪽에 위치한 다양한 CDK들의 종류에 대해 궁금증이 생기실 것 같습니다.앞서 소개한 AWS CDK는 프로그래밍 언어로 작성한 인프라 코드를 Cloudformation으로 템플릿을 생성했습니다.이처럼 CDK가 생성하는 템플릿이 AWS Cloudformation으로 활용 가능하도록 하는 것을 AWS CDK,쿠버네티스로 활용 가능하도록 하는 것을 CDK8s, 테라폼으로 활용 가능하도록 하는 CDKtf라고 합니다.위와 같이 현재 3가지 Type을 지원하고 있으며, 향후 다른 도구들도 지원할 가능성이 있다고 합니다.다음으로는 Construct Hub에 등록된 다양한 Construct 검색 결과입니다. 현재는 대부분 Construct는 Hahicorp, Datadog과 같은 클라우드 서비스 Publisher 들이 참여했지만,개인도 JSII (CDK가 다중 언어 라이브러리를 제공할 수 있도록 하는 기술)기반의 construct를 만들고 aws-cdk, cdk8s, cdktf 등의 키워드와 함께 npm 레지스트리에 공개되어 있다면 약 30분 내에 Construct Hub에도 개시된다고 합니다.화면에 보이는 특이점으로는 HashiCorp가 제공하는 CDKtf를 통해,다른 클라우드 서비스들과 통합하여 AWS 서비스 만이 아닌 모든 클라우드를 CDK로 제어 가능하게 하려는 큰 그림을 그려나가고 있다는 것을 알 수 있습니다.Outro지금까지 Construct Hub를 간단하게 살펴보며, AWS IaC 도구들의 변천사와 AWS가 그리는 IaC의 미래를 엿볼 수 있었습니다.그중에서도 흥미로웠던 요소들은 다음 2가지로 말씀드릴 수 있습니다.  IaC를 도입한 조직의 경우, Terraform, Ansible 등 여러 IaC 도구를 각각의 IaC 도구들의 특성에 맞게 복합적으로 운영합니다. CDK가 다방면으로 IaC 도구를 지원(AWS CDK, CDK8s, CDKtf) 하게 만들어 관리 복잡도를 줄이려는 노력이 흥미롭습니다.  CDKtf를 통해, 타 클라우드 서비스를 CDK로 제어하게 된다는 점이 흥미롭습니다.사실 CDK8s, CDKtf 모두 Construct Hub가 나오기 이전부터 존재했지만, Construct Hub에 개시된 문서들을 보니 더 흥미롭게 다가옵니다.아직 세상에 알려진지 얼마 되지 않은 Construct Hub를 활용하기에는 어려움이 있지만, 누구나 Construct Hub에 기여할 수 있는 오픈소스 생태계를 구축한 만큼 빠른 성장이 기대됩니다. Construct Hub가 기여할 IaC 미래에 긍정적인 기대를 걸어봅니다.소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해주세요! 😃📚 References  AWS Construct Hub availability  Construct Hub",
        "url": "/ConstructHub"
    }
    ,
    
    "karpenter": {
        "title": "My first impression of AWS Karpenter",
            "author": "HeuristicWave",
            "category": "",
            "content": "본 글은 지난 11월 29일에 GA된 Karpenter를 가볍게 사용해 보며 느낀 첫인상에 대한 글입니다.IntroEKS 환경에서 더 빠르고 효율적인 Kubernetes Cluster Autoscaler Karpenter가 GA 되었습니다.오픈소스인 Karpenter는 모든 클라우드 벤더와의 통합도 목표하지만, 아직 타 클라우드와의 통합은 진행 중이라 v0.5으로 공개되었다고 합니다.즉 GA 된 v0.5만큼은 AWS 리소스와 완벽하게 통합되었기에 이번 업데이트에 공개되었다고 할 수 있습니다.사실 기존 EKS 환경에서도 EC2의 Autoscaling을 활용한 CA를 지원하였는데, Karpenter 출시가 어떤 영향을 미치게 될 것인지 알아보겠습니다.Kubernetes Autoscaling쿠버네티스에서는 다음과 같은 3가지 Autoscaling 방법이 있습니다.  HPA(Horizontal Pod Autoscaler)  VPA(Vertical Pod Autoscaler)  CA(Cluster Autoscaler)Pod Scaling의 경우 CPU 사용량, 메트릭 등을 관찰하여 스케일링하지만, EKS 클러스터 자체의 자원이 모자라는 경우 CA를 고려해야 합니다.⚙️ Cluster Autoscaler with EC2 Auto Scaling기존 EKS는 다음과 같이 EC2의 Autoscaler 기능을 활용해 탄력적인 환경을 구성했습니다.hpa와 vpa가 eks에서 내에서 scaling을 진행하는 것과는 달리,worker node를 확보하기 위해 EC2의 Auto Scaling Group을 사용하며 k8s와 ec2 별도의 Layer를 관리해야 하는 운영 복잡도가 발생하게 되었습니다.🪓 Cluster Autoscaler with Karpenter반면 Karpenter의 경우 k8s의 native method를 확장한 개념이기 때문에, 기존의 CA 방법과는 달리 효율적인 환경을 제공할 수 있습니다.(실제로 카펜터를 운영해 보면 기존의 Auto Scaling Group을 사용하지 않는 것은 아니지만,사용자 입장에서는 고려하지 않아도 되니 k8s layer에서만 관리된다고 해도 틀린 말은 아닌 것 같습니다.)karpenter.sh 의 홈 화면을 보면 간단한 동작 원리를 설명하는 그림이 있습니다.karpenter가 unscheduled pods를 관찰하고 있다가 즉시(just-in-time) 최적화된 capacity에 pods를 배포합니다.Overprovisioning과거 EC2의 스케일링을 사용하는 CA에 대한 공식 문서에서, 다음과 같은 고려 사항을 확인할 수 있습니다.  노드를 확장하기 전에 노드가 확장될 때까지 기다려야 하므로 배포 대기 시간에 큰 영향을 미칩니다.노드를 사용할 수 있게 되려면 몇 분 정도 걸릴 수 있으며, 이로 인해 포드 예약 지연 시간이 크게 늘어날 수 있습니다.예약 대기 시간이 늘어나는 것을 감수하고 오버프로비저닝을 사용하여 이를 완화할 수 있습니다.그러나 이제 karpenter를 도입한다면 1분 이내 최적화된 인스턴스를 바로 프로비저닝 할 수 있으므로,더 이상 미리 프로비저닝을 할 필요도 없고 워커 노드의 크기 조정도 고려하지 않아도 됩니다.어떻게 오버프로비저닝을 방지하고 컨테이너와 클라우드 환경의 이점을 더 누릴 수 있게 해주는지 제가 진행해본 테스트와 함께 알아보겠습니다.👀 LabTMI : 이 글을 보는 시점에는 수정되어 있을 수도 있겠습니다만,공식 문서에 기재된 Default Region과 Module의 azs Config 값이 통일되지 않았습니다. 수행 시, 참고하시기 바랍니다.공식 문서 Terraform으로 시작하기 의 가이드대로Terraform 코드를 실행시키면 EKS 내에 다음과 같은 karpenter-controller와 karpenter-webhook 포드가 올라온 것을 확인할 수 있습니다.우선, Karpenter가 정말 최적화된 capacity를 제공하는지 확인하기 위해 t3a 시리즈의 스펙을 첨부합니다.Test 1 : t3a.medium 인스턴스에 1cpu를 요구하는 5개의 pod 배포문서에서 제공하는 inflate manifest를 활용해 t3a.medium 인스턴스에 1cpu를 요구하는 5개의 pod를 배포하면,다음과 같이 t3a.2xlarge 인스턴스가 즉시 프로비저닝 됩니다.(1분 이내라고 소개되지만, 체감상 1분 보다 더 빠른 시간 안에 프로비저닝 되는 것 같습니다.)새롭게 생성된 t3a.2xlarge 노드를 확인하면 다음과 같이 5개의 pod가 배치된 것을 볼 수 있습니다.아키텍처로 보면 다음과 같습니다. 기존 t3a.medium에는 기본으로 있는 pod들 때문에 1cpu 조차 할당할 수 없습니다.inflate는 5cpu를 요구하므로, 이를 수용할 수 있는 t3a.2xlarge 인스턴스를 프로비저닝하고 pod들을 배치시켰습니다.요청 리소스를 기반으로 최적의 인스턴스를 할당한 것을 확인할 수 있었습니다.Test 2 : Test1환경에서 0.5cpu를 요구하는 5개의 pod 배포Test 1에서 Scalue out(worker node 1대 =&gt; 2대) &amp; Scale up(t3a.medium =&gt; t3a.2xlarge)를 동시에 경험해 봤다면, 이번에는 다음과 같이 필요한 리소스만 0.5 cpu로 줄여보겠습니다.기존 t3a.2xlarge 인스턴스가 사라지고, t3a.xlarge 인스턴스가 즉시 프로비저닝 되었습니다.⬆️ a minute ago에서 ⬇️ 2minutes ago로 변하는 것을 보니 정말 1분 이내로 동작하는 것 같습니다.아키텍처로 보면 다음과 같습니다.이번에는 t3a.medium에 0.5 cpu만큼의 capacity가 남아있으므로 1개의 inflate pod가 배포되었고,t3x.xlarge 나머지 4개의 inflate pod가 배포되었습니다.t3a.large(2cpu) &lt; inflate(0.5cpu * 4) + kube-proxy + aws-node &lt; t3a.xlarge(4cpu)정말 빠른 시간 내에 최적의 capacity를 할당하는 모습을 보니 유연하고 높은 성능을 제공한다는 소개가 맞는 것 같습니다.저는 위 실험에서 인스턴스에 관한 별도의 CRD 값들을 지정하지 않아 karpenter가 t시리즈 인스턴스들을 프로비저닝 하였지만,운영에서 Karpenter를 사용하기 위해서는 Provisioner API를 읽고 세밀한 manifest 값들을 조정해 주어야 합니다.Outro과거 AWS의 CA는 스케일링에 걸리는 시간도 상당할뿐더러,제한적인 스케일링으로 인해 리소스가 낭비되거나 운영환경에서 다운타임을 최소화하기 위해 오버프로비저닝 되는 경우도 많았습니다.기존의 방법보다 더 Kubernetes native 한 karpenter를 도입한다면, 아래 그림과 같은 효과를 기대할 수 있습니다.Karpenter의 빠른 프로비저닝과 유연한 스케일링 덕분에 클라우드를 더 클라우드답게 사용할 수 있게 된 만큼가까운 미래에 Karpenter가 기존의 CA를 대체할 것으로 예상됩니다.지금까지 아주 간단하게 Karpenter를 사용해 본 후기를 작성해 보았습니다.추후, Karpenter의 자세한 동작 원리와 제약 사항 혹은 더 많은 기능들에 대하여 다뤄보겠습니다.소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해주세요! 😃📚 References  AWS Whats new, aws karpenter  Introducing Karpenter  Karpenter Document",
        "url": "/Karpenter"
    }
    ,
    
    "terraformtips1": {
        "title": "Terraform Module Tips 1 - Output",
            "author": "HeuristicWave",
            "category": "",
            "content": "Terraform Module과 친해지기 1 - Output서문Terraform Module을 잘 활용하면 본인이 원하는 대로 인프라를 레고처럼 조립할 수 있습니다.레고처럼 인프라를 조립하기 위해서는 모듈 간의 연계가 중요한데요,이번 포스팅에서는 모듈 내에 작성된 Output value들을 활용하는 방법을 알아보겠습니다.해당 포스팅은 Output에 대한 이해가 있다는 가정하에 기술하였으므로, Output에 대한 설명이 필요하시면 아래 링크들을 참고해 주세요!🥲 사실, 아래 문서보다 더 잘 설명할 자신이 없어서… 언제나 가장 좋은 교재는 공식 문서입니다!  Terraform Docs  Tutorial : Output Data from TerraformOutput Command OptionOutput은 terraform apply 명령어를 수행하고 난 후, 맨 마지막에 Value 들이 렌더링 되어 나옵니다.그러나 테라폼 코드를 작성하는 중간중간 인프라의 value 값들이 필요할 때가 있습니다.그럴 때, output이 정의된 상황에서 terraform output {label} 명령어로 Ouput 값을 확인합니다.-raw 옵션을 함께 주면 문자열 형태가 아닌 raw한 텍스트 값만을 얻을 수 있습니다.$ terraform output -raw security_group_idAccessing Child Module Outputs하위 모듈의 아웃풋에 접근할 경우도 종종 있는데요, 이때는 module.&lt;모듈 명&gt;.&lt;Output 명&gt; 이런 형식으로 조회가 가능합니다.하위 모듈 출력값에 접근하는 것이, 모듈과 모듈은 연계하는 방법이기에 아래 예시에서 알아보겠습니다.module \"vpc\" {  source = \"terraform-aws-modules/vpc/aws\"  name   = \"sample_vpc\"  cidr = \"10.10.0.0/16\"  azs            = [\"us-west-2a\"]  public_subnets = [\"10.10.1.0/24\"]  tags = {    Owner       = \"me\"    Environment = \"stage\"  }}위와 같은 vpc 모듈은 security group 모듈과 거의 단짝 수준으로 함께 움직이는데요,security group 모듈을 활용하려면 다음과 같이 vpc_id 값이 필요합니다.이 경우, vpc를 먼저 생성하고 vpc_id 값을 알아내어 사용할 수 있지만, 다음과 같은 방법으로 모듈을 연계합니다.module \"security_group\" {  source      = \"terraform-aws-modules/security-group/aws\"  name        = \"ssh\"  description = \"ssh from workstation\"  vpc_id      = module.vpc.vpc_id  ingress_cidr_blocks = [\"0.0.0.0/0\"]  ingress_rules       = [\"ssh-tcp\"]}Find Module Output Label방금 전, 모듈을 연계하는 방법을 배워 보았습니다. 그런데, 모듈을 연계하기 위해서는 미리 사전에 작성된 모듈의 Output Label을 알아야 합니다.우선 에디터의 Explorer 탭에서 .terraform 폴더를 열어봅시다.apply를 적용한 security-group, vpc 모듈이 내 로컬 머신에 숨어 있습니다.해당 모듈 폴더 안에 들어가면 outputs.tf 가 정의되어 있으므로 해당 파일을 참고하여 Label 값을 얻어오면 됩니다!글을 마치며이렇게 Module의 Output 값을 활용하는 방법을 알게 되니, 테라폼 모듈 조립에 대한 자신감이 생겼습니다. 앞으로도 Module을 활용해 다양한 인프라를 구축하고 오픈소스 툴들을 연계하는 방법을 시리즈로 연재할 계획인데, 언제 끝날지 모르겠습니다. 😑지금까지 테라폼 모듈과 친해지기 Output 편을 읽어주셔서 감사합니다! 잘못된 내용은 지적해 주세요! 😃",
        "url": "/TerraformTips1"
    }
    ,
    
    "ec2-clocksource": {
        "title": "EC2 Clocksource",
            "author": "HeuristicWave",
            "category": "",
            "content": "몰라도 되지만 알면 알수록 더 신비한 EC2 🙃Preview이번 포스팅에서는 AWS Well-Architected Labs - Performance Efficiency 에 개재된 Calculating differences in clock source를 읽고 궁금증이 생겨 구글링을 하다 알게 된 사실들을 의식의 흐름대로 작성한 포스팅입니다.Performance Efficiency Summary일단 Performance Efficiency에 나오는 실험 내용을 요약하자면 다음과 같습니다.AWS의 5세대 가상머신 Nitro와 non-nitro 인스턴스 2개를 올리고 시간을 반환하는 테스트 코드를 돌려 성능 테스트를 진행합니다.당연히 5세대 Nitro가 기존 세대보다 월등한 결과를 보여 주지만,non-nitro 기반의 인스턴스에서 ‘리눅스 클럭 소스를 교체하면 유의미한 성능 향상의 결과를 얻을 수 있다’ 라는 실험 결과를 보여줍니다.  Nitro 기반 인스턴스의 default clocksource : kvm-clock(권장)  Non-nitro 인스턴스의 default clocksource : xen  실험에서 교체한 Non-nitro 인스턴스의 clocksource : tsc마지막으로 첨부된 How do I manage the clock source for EC2 instances running Linux?게시물에서 클럭 소스를 교체하는 방법(xen에서 tsc로 교체)을 소개하며 실험 내용을 마칩니다.궁금한 건 못 참아 ❓위에 소개한 Lab을 진행하다 보니 ‘왜 tsc로 교체하여 성능 향상 효과를 얻을 수 있는지’ 알 수가 없었습니다.궁금증을 해소하기 위해 구글링을 하다 보니 이해를 돕는 다음 3가지 자료를 찾을 수 있었습니다.⏱ TimestampingRed Hat Reference Guide 에서 어느 정도 제 가려운 부분을 긁어 주었던 포스팅이 있습니다.기본적으로 멀티프로세서 시스템인 NUMA와 SMP 아키텍처에서는 여러 개의 clock source가 탑재되어 있습니다.멀티프로세서 기반의 EC2 인스턴스에서도 아래 명령어로 사용 가능한 clocksource를 확인하면 다음과 같은 결과를 확인할 수 있습니다.cat /sys/devices/system/clocksource/clocksource0/available_clocksourcexen tsc hpet acpi_pmRed Hat의 실험 결과에 따르면 tsc &gt; hpet &gt; acpi_pm 순으로 오버헤드가 적은데,tsc는 register에서 hpet은 memory area에서 읽기 때문에 수십만 개의 타임스탬프를 지정할 때 상당한 성능 이점을 제공한다고 합니다.⚙️ Heap Engineering PostRunning a database on EC2? Your clock could be slowing you down. 을 보면 더 정확한 분석이 있습니다.내용이 어려워 저는 완벽하게 이해하지 못했지만, 읽어보시면 굉장히 좋은 자료인 것 같습니다.Heap Engineering 해당 포스팅에서 밀당을 시도하는데…‘tsc에서는 낮은 가능성으로 clock drift 현상이 있어 프로덕션에서는 수행하지 말라’ 고 했다가,실제로는 clock drfit가 발생하지 않는다며 AWS가 tsc를 권장했던 슬라이드 자료 를 함께 보여줍니다.그냥 맘놓고 kvm-clock이 탑재된 인스턴스를 사용하는게 좋을 것 같습니다.🎥 Tudum~ 또! Netflix클라우드를 공부하다 보면 Netflix 가 클라우드에 지대한 영향을 끼친 것 같다고 느낄 때가 많은데, 이번에도 그랬습니다.AWS re:Invent 2014에서 Netflix의 Senior Performance Architect, Brendan Gregg의 발표 자료 를 보면xen에서 tsc로 교체하여 CPU 사용량은 30%, 평균 앱 레이턴시는 43%가 줄었다고 합니다.Result이번에도 구글링으로 딴짓을 하다 보니 많은 사실들을 알게 되었습니다. 사실 Current generation instances 를사용하면 대부분 위에서 언급한 최적화는 T2 시리즈, Gravition 계열을 제외한 대부분의 인스턴스에서는 기본적으로 적용되어 있습니다.그래서 포스팅의 첫 포문을 ‘몰라도 되지만 ~’이라 지었습니다.clocksource와는 별도로 이번 포스팅을 준비하다 거의 주말 하루를 소비했는데,비교적 최근의 인스턴스가 과거 인스턴스들과 어떻게 다른지(Hypervisor, Jumbo Frame 등등)를 알 수 있었습니다.새롭게 알게 된 사실들 역시 그냥 Nitro 기반의 Amazon Linux 2를 사용하면, 운영하는데 몰라도 지장 없이 최고의 성능을 보장받는 것 같습니다.아직 알음알음 아는 지식이라 포스팅하기 어렵지만, 훗날 더 정확히 알게 되면 성능과 관련된 다른 튜닝 요소들도 적어보겠습니다.소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해주세요! 😃📚 포스팅과 직접적인 연관도는 떨어지지만 함께 보면 좋은 자료  AWS EC2 Virtualization 2017: Introducing Nitro  Linux AMI virtualization types  Reinventing virtualization with the AWS Nitro System",
        "url": "/EC2_Clocksource"
    }
    ,
    
    "selfservice": {
        "title": "Self-service Infrastructure",
            "author": "HeuristicWave",
            "category": "",
            "content": "본 글은 Kief Morris의 Infrastructure as Code 와 HashiCorp 백서 를 읽고 학습한 내용을 기반으로 작성한 글 입니다.서문IaC(Infrastructure as Code)에 관심을 갖고 공부를 하다보면 Self-service Infra라는 말을 자주 만나게 됩니다.몇 개월째 와닿지 않는 개념이였지만, 최근 키프 모리스의 책을 다시 읽고 조금은 알게 된 거 같아 그동안 공부한 Self-service Infra에 대한 자료들을 바탕으로 작성해 보았습니다.Self-service Infra에 관한 배경지식을 넓히는 데 도움이 되었으면 좋겠습니다.Dynamic Infrastructure동적 인프라는 서버, 스토리지, 네트워크와 같은 인프라 자원을 관리할 수 있는 시스템을 말합니다.동적 인프라의 종류로는 Public/Private 클라우드, 오픈스택을 활용하는 사설 클라우드, 베어메탈 등이 있습니다.앞선 정의만 보면 동적 인프라는 클라우드와 굉장히 유사하지만, 키프 모리스는 동적 인프라가 클라우드보다 범위가 더 넓다고 합니다.📣 동적 인프라를 소개하며 알려드리고 싶은 문장이 있습니다.클라우드로의 전환이라는 의미에 대해 많은 사람들이 여러 측면에서 설명을 하지만,저는 HashiCorp의 Unlocking the Cloud Operating Model: Provisioning백서에 소개된 다음 표현에 참 공감이 갑니다.  클라우드로의 전환의 본질적인 의미는 “정적” 인프라에서 “동적” 인프라로의 전환입니다.  The essential implications of the transition to the cloud is the shift from “static” infrastructure to “dynamic” infrastructure동적 인프라 플랫폼 요구 사항동적 인프라 플랫폼은 다음과 같은 특성이 있습니다.  Programmable  On-Demand  Self-Service💻 Programmable동적 인프라 플랫폼은 프로그래밍을 쉽게 할 수 있어야 합니다. 유저 인터페이스 외에도 스크립트, CLI와 같은 도구들과도 상호 작용 할 수 있도록 프로그래밍 API가 필요합니다.아래 각 플랫폼 별 SDK를 사용하면 클라우드 내 자원을 생성하고 관리하는 코드를 작성할 수 있습니다.  AWS SDK  Azure SDK  GCP SDK  Openstack SDK⏰ On-Demand동적 인프라 플랫폼에서 자원을 즉시 생성하고 삭제하는 기능은 필수입니다.또한 전통적인 인프라의 과금 정책이 일정 기간 동안의 계약을 기반으로 한다면, 동적 인프라에서는 시간당 과금 체계를 지원합니다.🎊 드디어 대망의 셀프 서비스가 처음 소개 됩니다!🏃🏻 Self-Service셀프서비스는 온디맨드 요구 사항을 좀 더 발전시킨 개념입니다. 전통적인 방법에서 인프라를 요구하기 위해서는 세부 요청 양식, 설계 및 명세 문서, 구현 계획 수립 등을 필요로 했습니다.셀프서비스는 전통적인 방법에서 더 진화하여 필요한 인프라를 즉시 프로비저닝하고 쉽게 수정할 수 있는 자동화된 절차를 의미합니다.🛠 셀프 서비스는 인프라 템플릿을 운용할 수 있는 IaC 도구를 기반으로 구현합니다.예를 들어 개발자가 로드밸런서로 ALB를 사용하고 있다가 NLB로 바꾸고 싶은 경우, 다음과 같이 정의된 인프라 코드를 재배포 하면 됩니다.resource \"aws_lb\" \"test\" {  name               = \"test-lb-tf\"  internal           = false    # load_balancer_type = \"application\"  load_balancer_type = \"network\"    # Leave out other config ...}HashiCorp가 정의한 Self-Service Infrastructure 게시물을 보면,Self-Service Infra가 어떤 의미인지 더 쉽게 다가옵니다. (첨부된 링크를 통해 셀프서비스의 장점을 꼭 한번 읽어보세요!)기존 방법개발자는 인프라 담당자가 인프라를 할당할 때까지 기다려야 합니다.셀프 서비스 적용작성된 인프라 템플릿을 활용해 온디맨드로 프로비저닝 할 수 있습니다.글을 마치며저는 테라폼을 공부한 이후, 간단한 웹서비스를 운영하는 토이프로젝트를 진행할 때 다음과 같은 인프라 환경을 자주 사용합니다.미리 코드로 정의한 인프라 덕분에 개발에만 집중할 수 있는 환경과 생산성 향상을 경험했습니다.또한 테라폼 모듈 덕분에 제가 원하는 대로 인프라 스펙을 변경하고 각종 클라우드 서비스 추가 혹은 제거가 가능했습니다.셀프서비스 인프라가 기존 승인 체계(리소스 요청 ➡️ 담당자 승인)를 부정하는 것은 아니라고 생각합니다.기존 체계가 갖고 있는 보안적 이점을 포함한 장점들을 유지하며, 유연하고 신속하게 인프라를 운용하는 Self-service 환경이 불러올 장점을 고민해 봐야겠습니다.마지막으로, DevOps 문화가 정착해가며 CI/CD 를 통해 지속적 배포가 가능해지며 더 잦은 서비스 출시가 가능해졌습니다.또한, IaC를 통해 Immutable Infra를 추구하며 인프라의 일관성과 안정성을 보장하게 되었습니다.앞선 두 개의 개념에 더해 Self-service를 추구한다면 조직의 민첩성과 생산성 향상에 도움이 될 것이라고 생각합니다.소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해주세요! 😃",
        "url": "/SelfService"
    }
    ,
    
    "packer": {
        "title": "Provision Infrastructure with Packer",
            "author": "HeuristicWave",
            "category": "",
            "content": "본 글은 HashiCorp Learn - Provision Infrastructure with Packer 에서 다루는 내용을 기반으로 작성한 글 입니다.  Packer를 사용해 AWS AMI를 만들고 Terraform과 연계하여 활용하는 방법에 약간의 설명과 팁을 담아 한국어로 재작성해 보았습니다. (설치와 관련된 준비사항은 생략되어 있으므로 원문을 확인해주세요.)프로젝트 구조먼저, 해당 튜토리얼을 진행하기 위한 프로젝트 구조는 아래와 같습니다. 아래와 폴더와 파일을 준비해주세요..└── provision-infra-with-packer    ├── images    │   └── image.pkr.hcl    ├── instances    │   ├── main.tf    │   └── variables.tf    └── scripts        └── setup.shLocal SSH key 생성하기provision-infra-with-packer 폴더 안에서 AWS AMI로 만들 인스턴스에 접속하기 위한 SSH 키를 생성합니다.필자는 사용할 공개키를 Mac OS에서 ssh-keygen으로 생성하였습니다. 각자 환경에 맞는 방법으로 SSH 공개키를 생성하세요.암호화 타입(-t)을 RSA로 주석(-C)을 이메일로 생성되는 공개키의 위치(-f)를 현재 directory로 설정하고 공개키의 이름을 tf-packer로 설정했습니다.명령어를 입력하고 비밀번호를 입력하면 되지만 편의상 공백으로 두겠습니다.$ ssh-keygen -t rsa -C \"your_email@example.com\" -f ./tf-packer이후 tf-packer와 tf-packer.pub 2가지 파일이 생성되었다면 다음 단계 🚀Packer 코드 작성하기image.pkr.hcl 파일에서 진행합니다. 1️⃣ AMI 구축에 필요한 config 작성packer가 빌드한 이미지가 저장될 리전의 정보와 AMI에 timestamp 정보를 넣기 위한 config를 차례로 작성합니다.variable \"region\" {  type    = string  default = \"us-east-1\"}locals { timestamp = regex_replace(timestamp(), \"[- TZ:]\", \"\") }2️⃣️ Base AMI에 대한 Source AMI config 작성packer가 이미지를 만들기 위해 기본으로 사용되는 Base AMI에 대한 정보를 source 블록에 정의합니다.1단계에서 작성한 config 값을 활용해 리전과 timestamp를 넣어주는 코드와 Packer Builder로 사용할 인스턴스 타입을 지정합니다.이후, AWS에 존재하는 수많은 AMI 중에서 source로 활용할 이미지를 filter 코드로 작성합니다.(name 부분에 직접 ami 번호를 명시적으로 기재 할 수도 있습니다.)source \"amazon-ebs\" \"example\" {  ami_name      = \"learn-terraform-packer-${local.timestamp}\"  instance_type = \"t2.micro\"  region        = var.region  source_ami_filter {    filters = {      name                = \"ubuntu/images/*ubuntu-xenial-16.04-amd64-server-*\"      root-device-type    = \"ebs\"      virtualization-type = \"hvm\"    }    most_recent = true    owners      = [\"099720109477\"]  }  ssh_username = \"ubuntu\"}3️⃣ Packer build config 작성build 부분에서는 환경변수 세팅이나 명령어를 inline 형태로 기입 할 수 있지만,아래와 같은 간단한 기능만 수행하도록 코드를 작성합니다. hcl 문법에 따라 source를 지정하고, 프로비저닝을 위한 공개키의 위치를 명세합니다. source는 로컬 머신, destination은 원격 머신 입니다.마지막으로 Packer로 빌드한 이미지에서 Application Setup이 담긴 script를 지정합니다.build {  sources = [\"source.amazon-ebs.example\"]  provisioner \"file\" {    source      = \"../tf-packer.pub\"    destination = \"/tmp/tf-packer.pub\"  }  provisioner \"shell\" {    script = \"../scripts/setup.sh\"  }}4️⃣ Shell script 작성해당 작업은 scripts 폴더의 setup.sh에서 진행합니다. 이 부분은 Terraform으로 프로비저닝 한 인프라를 웹페이지에서 확인하기 위해 간단한 샘플을 띄우는 코드가 담겨있습니다.아래 Script에 필요한 종속성 설치, terraform을 user에 추가, 생성한 SSH키 설치, 샘플 Go App 설치가 단계가 작성되어 있습니다.  #!/bin/bashset -e# Install necessary dependenciessudo DEBIAN_FRONTEND=noninteractive apt-get -y -o Dpkg::Options::=&quot;--force-confdef&quot; -o Dpkg::Options::=&quot;--force-confold&quot; dist-upgradesudo apt-get -y -qq install curl wget git vim apt-transport-https ca-certificatessudo add-apt-repository ppa:longsleep/golang-backports -ysudo apt -y -qq install golang-go# Setup sudo to allow no-password sudo for &quot;hashicorp&quot; group and adding &quot;terraform&quot; usersudo groupadd -r hashicorpsudo useradd -m -s /bin/bash terraformsudo usermod -a -G hashicorp terraformsudo cp /etc/sudoers /etc/sudoers.origecho &quot;terraform ALL=(ALL) NOPASSWD:ALL&quot; | sudo tee /etc/sudoers.d/terraform# Installing SSH keysudo mkdir -p /home/terraform/.sshsudo chmod 700 /home/terraform/.sshsudo cp /tmp/tf-packer.pub /home/terraform/.ssh/authorized_keyssudo chmod 600 /home/terraform/.ssh/authorized_keyssudo chown -R terraform /home/terraform/.sshsudo usermod --shell /bin/bash terraform# Create GOPATH for Terraform user &amp; download the webapp from GitHubsudo -H -i -u terraform -- env bash &lt;&lt; EOFwhoamiecho ~terraformcd /home/terraformexport GOROOT=/usr/lib/goexport GOPATH=/home/terraform/goexport PATH=$PATH:$GOROOT/bin:$GOPATH/bingo get -d github.com/hashicorp/learn-go-webapp-demoEOF1 ~ 4단계를 마쳤다면 images 폴더 위치에 packer build image.pkr.hcl 명령어로 이미지를 빌드합니다.이미지 빌드 후, 콘솔 Images 탭의 AMIs을 확인하면 빌드한 이미지가 존재합니다.또한, 인스턴스 탭을 확인하면 아래와 같이 Base AMI를 만들기 위한 Packer Builder의 흔적을 볼 수 있습니다.해당 화면의 Instance ID를 눌러 설정들을 확인해보면 source 블록에 정의한 값을 바탕으로 만들어진 모습을 확인할 수 있습니다.Terraform으로 Packer 이미지 배포해당 작업은 instances 폴더에서 진행합니다. Sample App Infra Code 작성하기아래 🛠 이모티콘을 클릭하여 main.tf와 varaiables.tf를 작성합니다.  🔨 main.tf 🔨    terraform {  required_providers {    aws = {      source  = \"hashicorp/aws\"      version = \"~&gt; 3.26.0\"    }  }  required_version = \"~&gt; 1.0.2\"}provider \"aws\" {  region = var.region}resource \"aws_vpc\" \"vpc\" {  cidr_block           = var.cidr_vpc  enable_dns_support   = true  enable_dns_hostnames = true}resource \"aws_internet_gateway\" \"igw\" {  vpc_id = aws_vpc.vpc.id}resource \"aws_subnet\" \"subnet_public\" {  vpc_id     = aws_vpc.vpc.id  cidr_block = var.cidr_subnet}resource \"aws_route_table\" \"rtb_public\" {  vpc_id = aws_vpc.vpc.id  route {    cidr_block = \"0.0.0.0/0\"    gateway_id = aws_internet_gateway.igw.id  }}resource \"aws_route_table_association\" \"rta_subnet_public\" {  subnet_id      = aws_subnet.subnet_public.id  route_table_id = aws_route_table.rtb_public.id}resource \"aws_security_group\" \"sg_22_80\" {  name   = \"sg_22\"  vpc_id = aws_vpc.vpc.id  # SSH access from the VPC  ingress {    from_port   = 22    to_port     = 22    protocol    = \"tcp\"    cidr_blocks = [\"0.0.0.0/0\"]  }  ingress {    from_port   = 8080    to_port     = 8080    protocol    = \"tcp\"    cidr_blocks = [\"0.0.0.0/0\"]  }  ingress {    from_port   = 80    to_port     = 80    protocol    = \"tcp\"    cidr_blocks = [\"0.0.0.0/0\"]  }  egress {    from_port   = 0    to_port     = 0    protocol    = \"-1\"    cidr_blocks = [\"0.0.0.0/0\"]  }}resource \"aws_instance\" \"web\" {  ami                         = \"ami-YOUR-AMI-ID\"  instance_type               = \"t2.micro\"  subnet_id                   = aws_subnet.subnet_public.id  vpc_security_group_ids      = [aws_security_group.sg_22_80.id]  associate_public_ip_address = true  tags = {    Name = \"Learn-Packer\"  }}output \"public_ip\" {  value = aws_instance.web.public_ip}    🔧 varaiables.tf 🔧    variable \"cidr_vpc\" {  description = \"CIDR block for the VPC\"  default     = \"10.1.0.0/16\"}variable \"cidr_subnet\" {  description = \"CIDR block for the subnet\"  default     = \"10.1.0.0/24\"}variable \"environment_tag\" {  description = \"Environment tag\"  default     = \"Learn\"}variable \"region\"{  description = \"The region Terraform deploys your instance\"  default     = \"us-east-1\"}    인프라 코드에 대한 설명은 생략하겠습니다. 인프라에 대한 테라폼 코드가 궁금하시다면 다른 게시글이나 제 포스팅을 읽어보세요!AMI Query 하기본문에는 소개되지 않았지만, Packer와 Terraform의 통합을 위해 필자가 작성한 부분입니다.Terraform의 Data Sources를 활용해 빌드된 AMI를 Query하여 샘플 앱을 프로비저닝 해봅시다.선행 작업에서 진행한 main.tf 하단에 아래 코드를 추가합니다.data \"aws_ami_ids\" \"myami\" {  owners = [\"YOUR Account ID\"]  # sort_ascending = true    filter {    name   = \"name\"    values = [\"learn-terraform-packer-*\"]  }}Packer로 이미지를 빌드했으므로, ami owner가 되었습니다!owners 부분에 자신의 계정 ID를 작성하고 filter에 packer build를 할 떄 사용한 ami_name을 value 값으로 넣습니다.  sort_ascending default 값이 false  sort_ascending = false : list의 0번째 요소가 latest sort_ascending = true : 만들어진 순서대로 리스트 생성 (가장 먼저 생성된 ami가 0번)이어서 main.tf에 인스턴스 리소스를 정의한 부분의 ami = \"ami-YOUR-AMI-ID\" 코드를 아래와 같이 대체합니다.data 객체에 필터링한 결과 값들이 빌드된 AMI들이 리스트 형식으로 들어가는데 latest 버전을 사용하기 위해 0번째 이미지를 명시합니다.편의상 사용된 ami 번호를 터미널에서 확인 할 수 있도록 output에 대한 코드도 함께 작성합니다.resource \"aws_instance\" \"web\" {  ami = data.aws_ami_ids.myami.ids[0]   # \"ami-YOUR-AMI-ID\"  # Skip Other Config}output \"my_ami\" {  value = aws_instance.web.ami}코드를 작성하고 테라폼 코드가 위치한 폴더에서(instances 폴더 하위) 다음 명령어를 실행합니다.terraform init &amp;&amp; terraform apply 인스턴스를 생성하기 위해 yes를 기입합니다. 명령어로 인프라 상태를 점검해 아래와 같다면 다음 단계로 🚀$ terraform state listdata.aws_ami_ids.myamiaws_instance.webaws_internet_gateway.igwaws_route_table.rtb_publicaws_route_table_association.rta_subnet_publicaws_security_group.sg_22_80aws_subnet.subnet_publicaws_vpc.vpc  data object에 담긴 정보가 확인하고 싶다면? $ terraform state show data.aws_ami_ids.myami.ids인스턴스 확인하기SSH를 통해 인스턴스에 연결합니다. ssh terraform@$(terraform output -raw public_ip) -i ../tf-packerGo 디렉토리로 이동하세요. cd go/src/github.com/hashicorp/learn-go-webapp-demo데모 앱을 실행합니다. go run webapp.go배포한 앱 확인을 위해 terraform output public_ip로 얻은 IP에 8080 포트로 접속하면 간단한 테트리스 게임 앱을 확인할 수 있습니다.인스턴스 리소스 회수terraform destroy 명령어로 상기 프로젝트에서 사용한 인프라를 리소스를 회수합니다. Packer로 작성한 이미지는 파괴되지 않습니다.위와 같은 단계들을 통해 패커로 이미지를 만들고 테라폼과 통합하는 방법을 학습해보았습니다.이번 포스팅에서 다뤘던 내용은 Immutable Servers를 유지하기 위한 방법 중 하나입니다.오늘 포스팅에 추가로 Ansible을 통합한다면, Immutable Infrastructure를 구축할 수도 있습니다.Terraform Courses    3-Tier VPC Architecture with Terraform    ECR Pipeline with Terraform Ⅰ (CodeCommit)    ECR Pipeline with Terraform Ⅱ (ECR, CodeBuild, IAM)    ECR Pipeline with Terraform Ⅲ (CodePipeline)    Provision Infrastructure with Packer",
        "url": "/Packer"
    }
    ,
    
    "eks-max-pods": {
        "title": "Look into EKS max pods",
            "author": "HeuristicWave",
            "category": "",
            "content": "EKS 노드에서 사용 가능한 Pod의 개수는 몇 개일까?Preview이번 포스팅에서는 EKS의 노드 그룹에서는 최대 몇 개의 포드(Private IP)가 할당 가능하고 어떠한 방법으로 최대 포드의 개수를 제어할 수 있는지 알아보자.Build Up쿠버네티스 도규먼트 에 따르면 노드당 110개의 포드를 생성할 수 있으며, 노드는 5000개까지 생성 가능해 총 15만 개의 포드가 생성 가능하다고 한다. GCP의 GKE 가이드 에 따르면 기본 클러스터 노드 하나에 최대 110개의 포드가 생성 가능하다고 한다. 구글링을 통해 확인하니, 노드에서 포드의 갯수가 증가할수록 kubelet, cAdvisor 등과 같은 K8s 에이전트에 오버헤드를 발생시키므로 110개 정도를 권장한다고 한다.그래서 그런지 kubelet docs 에서도 아래와 같은 max-pods가 110을 기본값으로 가진다.--max-pods int32     Default: 110EKS eni max podsIP addresses per network interface per instance type 을 확인해보면 AWS의 인스턴스 타입별 ENI 개수를 파악할 수 있다.EKS 설명서 를 보면 다음과 같은 공식을 확인할 수 있다. (# of network interfaces for the instance type × (# of IPv4 per network interface - 1)) + 2ENI의 첫 번째 IP는 포드가 사용할 수 없으므로 1을 빼고 AWS CNI와 kube-proxy가 차지하는 2개의 IP를 마지막 수식에 더해 최종 사용 가능한 max-pod 값을 알 수 있다. 설명한 공식 이외에도 kubectl 명령어를 통해 Maximum Pods를 파악할 수 있다.❯ kubectl get nodes -ANAME                                            STATUS   ROLES    AGE   VERSIONip-10-0-0-178.ap-northeast-2.compute.internal   Ready    &lt;none&gt;   18h   v1.19.6-eks-49a6c0ip-10-0-1-143.ap-northeast-2.compute.internal   Ready    &lt;none&gt;   18h   v1.19.6-eks-49a6c0❯ kubectl describe nodes ip-10-0-0-178.ap-northeast-2.compute.internal | grep -i pods  pods:                        17  pods:                        17Non-terminated Pods:          (17 in total)👀 Labs 1kubectl의 명령어에서 노드(t3.medium)당 17개의 포드를 사용할 수 있다고 했는데, 직접 노드그룹 내에서 포드를 최대로 띄워 확인해보자. 노드그룹 내에 nginx 32개를 올려보았다.K9s 쉘을 통해 총 38개의 포드가 확인되었고 38개 중 aws-node, coredns, kube-proxy가 각각 2개의 노드에 위치하고 4개의 nginx 포드가 Pending 상태라는 것을 파악했다.즉, 38(Total) - 4(Pending) = 17(t3.medium Maximum Pods) * 2(# of Node) 실험 결과와 앞서 알아본 Maximum Pods가 동일하다.Maximum Pods 변경하기EKS에서 Maximum Pods를 결정 짓는 요소는 ENI다. 그러나 클러스터의 노드그룹을 생성할 때 kubelet의 max-pods 값을 변경해 커스터마이징 할 수 있다. AWS Docs 에서 다음과 같은 (불친절한?) 설명을 통해 ENI와 별개로 max-pods를 제어할 수 있는 힌트를 얻었다.공식 문서에서 설명이 굉장히 빈약하지만, EKS에서 노드 그룹을 커스텀으로 생성할 때 Launch templates의 UserData를 아래와 같이 정의하면 Maximum Pods가 변경된다.MIME-Version: 1.0Content-Type: multipart/mixed; boundary=\"==MYBOUNDARY==\"--==MYBOUNDARY==Content-Type: text/x-shellscript; charset=\"us-ascii\"#!/bin/bash/etc/eks/bootstrap.sh {Cluster Name} --use-max-pods false --kubelet-extra-args '--max-pods=10'--==MYBOUNDARY==--\\  GCP에서 Max Pods를 제어하는 방법 👈 Click!   GCP에서는 클러스터를 생성할 때아래 명령어의 --default-max-pods-per-node 파라미터를 통해 max-pods(👆 Build Up 단계에서 default 110 👆)를 조절할 수 있다.  gcloud container clusters create CLUSTER_NAME \\  # 생략  --default-max-pods-per-node MAXIMUM_PODS \\  # 생략  👀 Labs 2아래 사진의 왼쪽은 Labs 1의 Pure한 t3.medium, 오른쪽은 max-pods를 지정한 Custom t3.medium이다. 콘솔화면에서 스펙은 같지만 할당된 포드의 수가 다르다.Result지금까지 다양한 방법을 통해 EKS에서 Maximum Pods를 파악하는 방법과 변경하는 방법 배웠다.AWS에서는 ENI라는 가상 네트워크 카드를 나타내는 논리적 네트워크 구성 요소 덕분에 인스턴스 타입마다 생성될 수 있는 포드의 수가 달랐다.그 밖에도 GCP에 쿠버네티스의 설계 철학을 그대로 이어받아 kubelet의 max-pods 값이 동일하고 클러스터 생성 순간에도 max-pods 설정에 대한 자유도가 높다는 사실을 알 수 있었다.어떠한 방법이 더 우위에 있는지 결론짓기 어렵지만, CSP의 쿠버네티스 max-pods 생성 원리를 파악하여 최적의 IP 할당에 도움이 되면 좋겠다.소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해주세요! 😃EKS Series    Look into EKS max pods",
        "url": "/EKS_Max_Pods"
    }
    ,
    
    "codepipeline": {
        "title": "ECR CodePipeline with Terraform Ⅲ",
            "author": "HeuristicWave",
            "category": "",
            "content": "Terraform으로 ECR 파이프라인 구축하기 3 (CodePipeline)Preview3편에서는 CodePipeline을 생성하고 IAM 정책과 역할을 부여하는 법을 배워보겠습니다.문서를 확인하면 artifact가 담기는 버킷, pipeline을 생성하는 리소스, 관련된 IAM Role과 Policy가 보입니다.테라폼은 선언형 언어이므로 Role과 Resource의 작성 순서가 바뀌어도 상관이 없지만, 콘솔에서 작업할 경우 Role을 먼저 작성하고 리소스를 생성하니 3편에서는 IAM을 먼저 작성하겠습니다.IAM Role아래 Role을 방금전 생성한 codepipeline.tf에 작성합니다.resource \"aws_iam_role\" \"codepipeline_role\" {  name = \"terraform-codepipeline\"  assume_role_policy = &lt;&lt;EOF{  \"Version\": \"2012-10-17\",  \"Statement\": [    {      \"Action\": \"sts:AssumeRole\",      \"Principal\": {        \"Service\": \"codepipeline.amazonaws.com\"      },      \"Effect\": \"Allow\"    }  ]}EOF}IAM Policy본래 필요한 정책만을 골라 정책생성기 에서 생생된 정책을 활용하는 방법이 있지만, 어떤 정책이 필요한지 한번에 맞추기는 너무 어렵습니다.(저의 경우 인터넷에서 타인이 작성한 정책과 에러메시지를 맞아가며 정책을 작성하고 있습니다 😅)➕ 아래 Policy를 방금전 생성한 codepipeline.tf에 아래 코드를 추가합니다.resource \"aws_iam_policy\" \"codepipeline_policy\" {  description = \"Codepipeline Execution Policy\"  policy      = &lt;&lt;EOF{  \"Version\": \"2012-10-17\",  \"Statement\": [    {      \"Action\": [        \"s3:GetObject\", \"s3:GetObjectVersion\", \"s3:PutObject\",        \"s3:GetBucketVersioning\"      ],      \"Effect\": \"Allow\",      \"Resource\": \"${aws_s3_bucket.artifact_bucket.arn}/*\"    },    {      \"Action\" : [        \"codebuild:StartBuild\", \"codebuild:BatchGetBuilds\",        \"iam:PassRole\"      ],      \"Effect\": \"Allow\",      \"Resource\": \"*\"    },    {      \"Action\" : [        \"codecommit:CancelUploadArchive\",        \"codecommit:GetBranch\",        \"codecommit:GetCommit\",        \"codecommit:GetUploadArchiveStatus\",        \"codecommit:UploadArchive\"      ],      \"Effect\": \"Allow\",      \"Resource\": \"${aws_codecommit_repository.test.arn}\"    }  ]}EOF}🚩 이어서 생성한 Policy를 Role에 부여합니다. 이것 역시 codebuild.tf에 추가합니다.resource \"aws_iam_role_policy_attachment\" \"codepipeline-attach\" {  role       = aws_iam_role.codepipeline_role.name  policy_arn = aws_iam_policy.codepipeline_policy.arn}CodePipelineaws_codepipeline리소스의 config에는 artifact store와 암호화 키, Source-Build-Deploy로 이어지는 각 Stage가 선언되어 있습니다.리소스 안에 기재된 설정들은 필수가 아니므로 선택하여 사용할 수 있습니다. 이번 포스팅에서는 deploy stage와 암호화 config는 제외하고 진행하겠습니다.resource \"aws_codepipeline\" \"pipeline\" {  name     = \"${var.source_repo_name}-${var.source_repo_branch}-Pipeline\"  role_arn = aws_iam_role.codepipeline_role.arn  artifact_store {    location = aws_s3_bucket.artifact_bucket.bucket    type     = \"S3\"  }  stage {    name = \"Source\"    action {      name             = \"Source\"      category         = \"Source\"      owner            = \"AWS\"      version          = \"1\"      provider         = \"CodeCommit\"      output_artifacts = [\"SourceOutput\"]      run_order        = 1      configuration = {        RepositoryName       = var.source_repo_name        BranchName           = var.source_repo_branch        PollForSourceChanges = \"false\"      }    }  }  stage {    name = \"Build\"    action {      name             = \"Build\"      category         = \"Build\"      owner            = \"AWS\"      version          = \"1\"      provider         = \"CodeBuild\"      input_artifacts  = [\"SourceOutput\"]      output_artifacts = [\"BuildOutput\"]      run_order        = 1      configuration = {        ProjectName = aws_codebuild_project.codebuild.id      }    }  }}1편에서 작성한 CodeCommit을 Stage의 Source, 2편에서 작성한 CodeBuild를 Build단계 지정했습니다.terraform apply, plan 명령어를 차례로 반영해 오류가 없는지 확인합니다.지금까지 작성된 인프라를 terraform state list명령어를 통해 확인하면 아래와 같습니다.❯ terraform state listaws_codebuild_project.codebuildaws_codecommit_repository.testaws_codepipeline.pipelineaws_ecr_repository.image_repoaws_iam_policy.codebuild_policyaws_iam_policy.codepipeline_policyaws_iam_role.codebuild_roleaws_iam_role.codepipeline_roleaws_iam_role_policy_attachment.codebuild-attachaws_iam_role_policy_attachment.codepipeline-attachaws_s3_bucket.artifact_bucketCodePipe line 콘솔 에서 확인하면 권한이 없어 실패한 화면이 나올 것 입니다.이를 해결하기 위해 또 다른 권한이 필요합니다.CodePipeline TriggerCodeCommit에서 발생한 이벤트가 CodePipeline으로 트리거되기 위해서는 아래 정의된 권한이 필요합니다.➕ 아래 코드를 codepipeline.tf에 추가하고 인프라를 생성해주세요.  resource &quot;aws_iam_role&quot; &quot;trigger_role&quot; {  name               = &quot;terraform-trigger&quot;  assume_role_policy = &lt;&lt;EOF{  &quot;Version&quot;: &quot;2012-10-17&quot;,  &quot;Statement&quot;: [    {      &quot;Action&quot;: &quot;sts:AssumeRole&quot;,      &quot;Principal&quot;: {        &quot;Service&quot;: &quot;events.amazonaws.com&quot;      },      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Sid&quot;: &quot;&quot;    }  ]}EOF}resource &quot;aws_iam_policy&quot; &quot;trigger_policy&quot; {  description = &quot;CodePipeline Trigger Execution Policy&quot;  policy      = &lt;&lt;EOF{  &quot;Version&quot;: &quot;2012-10-17&quot;,  &quot;Statement&quot;: [    {      &quot;Action&quot;: [        &quot;codepipeline:StartPipelineExecution&quot;      ],      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Resource&quot;: &quot;${aws_codepipeline.pipeline.arn}&quot;    }  ]}EOF}resource &quot;aws_iam_role_policy_attachment&quot; &quot;trigger-attach&quot; {  role       = aws_iam_role.trigger_role.name  policy_arn = aws_iam_policy.trigger_policy.arn}ResultTrigger 까지 정상적으로 적용하고 테스트용으로 활용할 아무 Dockerfile을 CodeCommit에 Push합니다.다시 CodePipeline 콘솔 에 접속해 우상단에 위치한 변경사항 릴리스를 누르면,아래와 같이 정상적으로 코드 파이프라인이 작동하여 운영되는 것을 확인 할 수 있습니다.CleanupS3 bucket은 빈상태여야 제거가 가능하기에 S3 콘솔 에서 ecr-pipeline의 데이터를 모두 삭제합니다.이어서 terraform destory 명령어로 모든 리소스를 회수합니다.총 3편에 걸쳐서 테라폼으로 최소한의 리소스로 ECR Pipeline 구축법을 알아보았습니다. (CloudWatch 기능을 추가해 CodePipeline을 구축해보세요 👍)해당 과정을 통해 AWS 인프라 생성법과, IAM 활용법, Variable, Output, tfvars 등을 활용해 코드를 작성하는 법을 공부했습니다.다른 CI/CD 파이프라인 구축법도 이번 포스팅에서 다룬 방법과 크게 다르지 않으니, 해당 포스팅이 도움이 되면 좋겠습니다. 😁Terraform Courses    3-Tier VPC Architecture with Terraform    ECR Pipeline with Terraform Ⅰ (CodeCommit)    ECR Pipeline with Terraform Ⅱ (ECR, CodeBuild, IAM)    ECR Pipeline with Terraform Ⅲ (CodePipeline)    Provision Infrastructure with Packer",
        "url": "/CodePipeline"
    }
    ,
    
    "codebuild": {
        "title": "ECR CodePipeline with Terraform Ⅱ",
            "author": "HeuristicWave",
            "category": "",
            "content": "Terraform으로 ECR 파이프라인 구축하기 2 (ECR, CodeBuild, IAM)2편에서는 ECR과 CodeBuild를 생성하고 IAM 역할, 정책을 부여하는 법을 학습합니다.ECRECR 역시 공식 문서 에서 사용방법을 확인합니다.공식문서에서 image_scanning_configuration config를 사용하면 취약점 스캔이 가능하다 설명되어 있지만, 필요하지 않기 때문에 제외하겠습니다.더불어, output도 함께 작성하겠습니다.cat &lt;&lt;EOF &gt; ecr.tfresource \"aws_ecr_repository\" \"image_repo\" {  name                 = var.image_repo_name  image_tag_mutability = \"MUTABLE\"}output \"image_repo_url\" {  value = aws_ecr_repository.image_repo.repository_url}output \"image_repo_arn\" {  value = aws_ecr_repository.image_repo.arn}EOF이어서 ecr.tf에서 변수로 사용하기 위한 var.image_repo_name 부분이 작동하도록 1편에서 작성한 variables.tf 아래 값을 추가합니다.✅ 편의상 이번 단계에 필요한 variable을 함께 포함했습니다.variable \"image_repo_name\" {  description = \"Image repo name\"  type        = string}variable \"container_name\" {  description = \"Container Name\"  default     = \"my-container\"}variable \"source_repo_branch\" {  description = \"Source repo branch\"  type        = string}ecr 작성을 완료햇으니 plan, apply 명령어를 차례로 입력해 인프라를 생성하고 terraform state list명령어나 콘솔 에서 생성된 인프라를 확인합니다.CodeBuildCodeBuild를 사용하기 위해 Terraform 도큐먼트 에서 사용법을 확인합니다.기존까지의 작업과는 달리 상당히 어려워 보입니다. 그러나 쓱 훝어보면 크게 4가지(bucket, IAM Role과 Policy, Codebuild)로 정리됩니다.Bucket도큐먼트와 같이 우선적으로 S3를 생성합니다. bucket의 이름은 선택이지만, 여러개의 버킷을 가지고 있는 저는 식별을 위해 이름을 부여했습니다.cat &lt;&lt;EOF &gt; codebuild.tfresource \"aws_s3_bucket\" \"artifact_bucket\" {  bucket = \"ecr-pipeline\"}EOFIAM Role도큐먼트를 따라 AssumeRole을 사용합시다. ➕ S3을 만들때 사용한 codebuild.tf에 아래 코드를 추가합니다.resource \"aws_iam_role\" \"codebuild_role\" {  name = \"terraform-codebuild\",  assume_role_policy = &lt;&lt;EOF{   \"Version\": \"2012-10-17\",   \"Statement\": [      {         \"Effect\": \"Allow\",         \"Principal\": {            \"Service\": \"codebuild.amazonaws.com\"         },         \"Action\": \"sts:AssumeRole\"      }   ]}EOF}IAM Policy정책은 IAM 콘솔에서 기존에 만들어진 정책을 사용할 수도 있지만, 아래와 같이 직접 작성할 수도 있습니다.도큐먼트에서 EC2에 대한 정책을 사용하지만, 우리는 ECR을 사용하므로 아래와 같은 정책을 사용하겠습니다.  resource &quot;aws_iam_policy&quot; &quot;codebuild_policy&quot; {  description = &quot;CodeBuild Execution Policy&quot;  policy      = &lt;&lt;EOF{  &quot;Version&quot;: &quot;2012-10-17&quot;,  &quot;Statement&quot;: [    {      &quot;Action&quot;: [        &quot;logs:CreateLogGroup&quot;, &quot;logs:CreateLogStream&quot;, &quot;logs:PutLogEvents&quot;,        &quot;ecr:GetAuthorizationToken&quot;      ],      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Resource&quot;: &quot;*&quot;    },    {      &quot;Action&quot;: [        &quot;s3:GetObject&quot;, &quot;s3:GetObjectVersion&quot;, &quot;s3:PutObject&quot;      ],      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Resource&quot;: &quot;${aws_s3_bucket.artifact_bucket.arn}/*&quot;    },    {      &quot;Action&quot;: [        &quot;ecr:GetDownloadUrlForLayer&quot;, &quot;ecr:BatchGetImage&quot;,        &quot;ecr:BatchCheckLayerAvailability&quot;, &quot;ecr:PutImage&quot;,        &quot;ecr:InitiateLayerUpload&quot;, &quot;ecr:UploadLayerPart&quot;,        &quot;ecr:CompleteLayerUpload&quot;      ],      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Resource&quot;: &quot;${aws_ecr_repository.image_repo.arn}&quot;    }  ]}EOF}20, 30라인에서 앞서 생성한 리소스를 ${채움참조} 문법으로 유연한 코드를 작성합니다.🚩 이어서 생성한 Policy를 Role에 부여합니다. 이것 역시 codebuild.tf에 추가합니다.resource \"aws_iam_role_policy_attachment\" \"codebuild-attach\" {  role       = aws_iam_role.codebuild_role.name  policy_arn = aws_iam_policy.codebuild_policy.arn}CodeBuildTerraform 도큐먼트 를 보아도 어떻게 해야 ECR에 적용시킬 수 있는지 알기 어렵습니다.우선 CodeBuild를 이해하기 위해 AWS docs 를 읽어봅시다.대략 리소스 이름을 정하고, 환경을 구성하고 빌드를 하기 위한 방법을 정의해야 한다는 사실을 알 수 있습니다.CodeBuild가 정의된 아래 코드를 활용해 codebuild.tf에 추가합니다.  resource &quot;aws_codebuild_project&quot; &quot;codebuild&quot; {  name         = &quot;codebuild-${var.source_repo_name}-${var.source_repo_branch}&quot;  service_role = aws_iam_role.codebuild_role.arn  artifacts {    type = &quot;CODEPIPELINE&quot;  }    environment {    compute_type                = &quot;BUILD_GENERAL1_MEDIUM&quot;    image                       = &quot;aws/codebuild/standard:3.0&quot;    type                        = &quot;LINUX_CONTAINER&quot;    privileged_mode             = true    image_pull_credentials_type = &quot;CODEBUILD&quot;    environment_variable {      name  = &quot;REPOSITORY_URI&quot;      value = aws_ecr_repository.image_repo.repository_url    }    environment_variable {      name  = &quot;AWS_DEFAULT_REGION&quot;      value = var.aws_region    }    environment_variable {      name  = &quot;CONTAINER_NAME&quot;      value = var.container_name    }  }  source {    type      = &quot;CODEPIPELINE&quot;    buildspec = &lt;&lt;BUILDSPEC${file(&quot;buildspec.yml&quot;)}BUILDSPEC  }}31라인이 참조하는 buildspec.yml을 pre_build, build, post_build에 맞춰 작성합니다.cat &lt;&lt;EOF &gt; buildspec.ymlversion: 0.2phases:  install:    runtime-versions:      docker: 18  pre_build:    commands:      - echo Logging in to Amazon ECR...      - $(aws ecr get-login --region $AWS_DEFAULT_REGION --no-include-email)      - COMMIT_HASH=$(echo $CODEBUILD_RESOLVED_SOURCE_VERSION | cut -c 1-7)      - IMAGE_TAG=${COMMIT_HASH:=latest}  build:    commands:      - echo Build started on `date`      - echo Building the Docker image...      - docker build -t $REPOSITORY_URI:latest .      - docker tag $REPOSITORY_URI:latest $REPOSITORY_URI:$IMAGE_TAG  post_build:    commands:      - echo Build completed on `date`      - echo Pushing the Docker image...      - docker push $REPOSITORY_URI:latest      - docker push $REPOSITORY_URI:$IMAGE_TAG      - echo Writing image definitions file...      - printf '[{\"name\":\"%s\",\"imageUri\":\"%s\"}]' $CONTAINER_NAME $REPOSITORY_URI:$IMAGE_TAG &gt; imagedefinitions.jsonartifacts:  files: imagedefinitions.jsonEOF지금까지 작성된 인프라를 terraform state list명령어를 통해 확인하면 아래와 같습니다.❯ terraform state listaws_codebuild_project.codebuildaws_codecommit_repository.testaws_ecr_repository.image_repoaws_iam_policy.codebuild_policyaws_iam_role.codebuild_roleaws_iam_role_policy_attachment.codebuild-attachaws_s3_bucket.artifact_bucket  생성한 인프라가 위와 같지 않을 경우, 👉 Click  실수로 의도치 않은 인프라가 프로비저닝 되었다면 2가지 방법을 통해 원 상태로 복구 할 수 있습니다.      terraform destroy 명령어로 특정 인프라만 되돌리거나 프로비저닝 하고싶은 경우, -target 옵션과 함께 resource 명으로 명령어를 작성합니다. 예시) terraform destory -target aws_vpc.main    잘못 작성한 코드를 수정 후, terraform apply명령어를 적용하여 최신 상태의 인프라를 반영합니다.  Terraform Courses    3-Tier VPC Architecture with Terraform    ECR Pipeline with Terraform Ⅰ (CodeCommit)    ECR Pipeline with Terraform Ⅱ (ECR, CodeBuild, IAM)    ECR Pipeline with Terraform Ⅲ (CodePipeline)    Provision Infrastructure with Packer",
        "url": "/CodeBuild"
    }
    ,
    
    "codecommit": {
        "title": "ECR CodePipeline with Terraform Ⅰ",
            "author": "HeuristicWave",
            "category": "",
            "content": "Terraform으로 ECR 파이프라인 구축하기 1 (CodeCommit)Overview이번 포스팅에서는 커밋 후, 도커의 이미지를 자동으로 배포하는 ECR Pipeline을 테라폼으로 생성해보겠습니다.AWS에서 저장소 역할을 하는 CodeCommit, 코드를 빌드하는 CodeBuild, 파이프라인을 자동화 하는 CodePipeline, 컨테이너 이미지를 저장하는 ECR을 활용해 구축합니다.1편에서는 CodeCommit 구축과 terraform의 Output, Variables, tfvars 등을 배워 보겠습니다.준비 작업이번 포스팅의 작업공간(~/terraform)을 생성하고 해당 위치에서 아래 코드 블럭을 터미널에 복사합니다.cat &lt;&lt;EOF &gt; provider.tfprovider \"aws\" {  region  = var.aws_region}EOF이후, terraform init 명령어를 실행시켜주세요.CodeCommitCodeCommit을 사용하기 위해 Terraform 도큐먼트 에서 사용법을 확인합니다.링크의 Example Usage를 활용해 코드를 작성할 수도 있지만, 이번 포스팅에서는 제 방식대로아래 코드를 활용해 작성해보겠습니다.링크에서 소개하는 코드와 다른 부분은 output과 variable의 사용 여부입니다.✅ 아래 코드와 도큐먼트의 코드가 어떻게 다른지 꼭 확인해보세요!cat &lt;&lt;EOF &gt; codecommit.tfresource \"aws_codecommit_repository\" \"test\" {  repository_name = var.source_repo_name  description     = \"This is the Sample App Repository\"}output \"source_repo_clone_url_http\" {  value = aws_codecommit_repository.test.clone_url_http}EOFOutput은 향후 clone할 원격 저장소의 위치를 파악하기 위해 넣어줍니다. 또한 Variable을 사용해 보다 유연한 코드를 작성해 보겠습니다.준비 작업에 정의한 리전과 CodeCommit Repo 이름에 Variable을 사용하겠습니다.cat &lt;&lt;EOF &gt; variables.tfvariable \"aws_region\" {  description = \"The AWS region\"  default     = \"ap-northeast-2\"}variable \"source_repo_name\" {  description = \"Source repo name\"  type        = string}EOF위 코드를 복사한 후, terraform plan 명령어로 아래와 같은 화면을 확인 할 수 있습니다.앞서 작성한 variables.tf의 region은 default 값이 있지만, repository는 variable의 형식만 정의되어 있기 때문에 인프라를 생성할 때 필수적으로 이름을 입력받습니다.✅ variable의 input값을 수기로 작성하는 것을 피하고 싶으면 tfvars를 사용합니다. 편의상 이번 프로젝트에서 사용할 값들을 미리 작성하겠습니다.cat &lt;&lt;EOF &gt; terraform.tfvarsaws_ecr=\"my-image\"source_repo_name=\"my-pipeline\"source_repo_branch=\"master\"image_repo_name=\"my-pipeline\"EOFtfvars는 위와 같이 변수의 값을 지정하기도 하지만, .env처럼 외부로 노출하면 안되는 값을 넣어두고 git에 ignore시켜 사용하기도 합니다.위 작업을 진행 후, terraform apply명령어를 적용하면 “Apply complete”과 함께 Outputs 값이 나옵니다.terraform state list명령어 이외에도, 콘솔 로 이동하면 생성된 인프라를 확인 할 수 있습니다.생성된 원격저장소를 사용하기 위해 terraform output을 활용해 export 환경 변수를 지정합니다.export tf_source_repo_clone_url_http=$(terraform output source_repo_clone_url_http)echo $tf_source_repo_clone_url_http\t# 확인Git SettingCodeCommit의 Repo 활용법은 아래 2가지 방법이 있습니다.1. 로컬에 위치한 코드를 CodeCommit에 push하기 (원격저장소가 비어있음)로컬의 빈공간에서 CodeCommit Repo 사용을 위한 git remote 지정git initgit remote add origin $tf_source_repo_clone_url_httpgit remote -v   # 원격 저장소 확인코드를 작성하고 CodeCommit에 Push하기git add .git commit -m \"First commit\"git statusgit push origin # master branch로 push자격 증명 문제가 있다면 아래 명령어로 해결합니다. 자격 증명 헬퍼 및 AWS CodeCommit에 대한 HTTPS 연결 문제 해결git config --global credential.helper '!aws codecommit credential-helper $@'git config --global credential.UseHttpPath true2. 로컬에 원격저장소의 코드를 clone하기 (원격저장소가 비어있지 않음)git clone $tf_source_repo_clone_url_http지금까지 도큐먼트를 활용해 코드를 작성하고, variable, output, tfvars의 활용법을 배워보았습니다.앞서 작성된 작업들이 정상적으로 커밋과 clone이 가능하면, 다음 단계로 🚀Terraform Courses    3-Tier VPC Architecture with Terraform    ECR Pipeline with Terraform Ⅰ (CodeCommit)    ECR Pipeline with Terraform Ⅱ (ECR, CodeBuild, IAM)    ECR Pipeline with Terraform Ⅲ (CodePipeline)    Provision Infrastructure with Packer",
        "url": "/CodeCommit"
    }
    ,
    
    "3tier": {
        "title": "3-Tier VPC Architecture with Terraform",
            "author": "HeuristicWave",
            "category": "",
            "content": "본 글은 Configure and Deploying VPCs with Multiple Subnets 에서 다루는 Production-Ready: The 3-Tier VPC 강의를 바탕으로, 테라폼으로 구축하는 3계층 VPC 아키텍처에 대한 글입니다.  Multi-Tier VPC 란?  VPC를 구축할 때 단일 계층 VPC에 모든 자원을 넣는다면, 네트워크에 접근할 수 있는 잠재적 공격자에게 자원이 노출됩니다. 이를 보완하기 위해 서브넷으로다중 계층 VPC 아키텍처를 만들어 방어 계층을 이룰 수 있습니다.  디자인 패턴 : 3 Tier VPC Architecture테라폼 코드는 모듈로 관리하는 것을 권장하지만, 이번 포스팅에서는 3-tier 아키텍처 중 네트워크와 관련된 부분만을 다뤄 하나의 파일에서 코드를 관리합니다.  Step 0 (테라폼을 활용하실 줄 안다면 넘어가세요)  Step 0  아키텍처를 구성할 폴더를 만들고 provider를 주입합니다.  mkdir architecturecd architectureterraform inittouch threeTierVPC.tf    threeTierVPC.tf에 벤더 정보를 작성합니다.  provider \"aws\" {  region = \"ap-northeast-2\"}    명령어terraform plan, terraform apply를 통해, 오류 없이 통과하는 화면을 확인하고 다음 단계로 🚀      terraform apply로 인프라를 반영 할때, -auto-approve옵션을 주면 yes입력 없이 진행 할 수 있습니다. 그러나 yes를 입력하기 전, 한번 더 검토할 수 있는 기회가 있으므로 권장하지 않습니다.  💡각 소제목 링크에 첨부된 코드를 활용해 Step 0 에서 만든 threeTierVPC.tf에 이어서 작성하거나, 따로 새로운 파일을 만들어 terraform plan, terraform apply 명령어를 차례로 작성하며 계층을 쌓아 올립니다.Step 1Layer 1️⃣ : Public subnet하나의 VPC에 2개의 AZ를 만들고 각각의 Public 서브넷을 위치시킵니다.  퍼블릭 서브넷은 프라이빗 서브넷 보다 적은 수의 IP 예약하는 것이 좋습니다.Step 1 코드를 적용 후, 명령어로 인프라 상태를 점검해 아래와 같다면 다음 단계로 🚀$ terraform state listdata.aws_availability_zones.availableaws_internet_gateway.igwaws_subnet.pub_sub_1aws_subnet.pub_sub_2aws_vpc.main  생성한 인프라가 위와 같지 않을 경우  실수로 의도치 않은 인프라가 프로비저닝 되었다면 2가지 방법을 통해 원 상태로 복구 할 수 있습니다.      terraform destroy 명령어로 특정 인프라만 되돌리거나 프로비저닝 하고싶은 경우, -target 옵션과 함께 resource 명으로 명령어를 작성합니다. 예시) terraform destory -target aws_vpc.main    잘못 작성한 코드를 수정 후, terraform apply명령어를 적용하여 최신 상태의 인프라를 반영합니다.  Step 2Layer 1️⃣ : Internet access resources외부 인터넷과의 노출을 제한하고 나가는 트래픽을 위해 NAT Gateway를 활용합니다. 또한 들어오는 트래픽을 위해 ALB를 위치시켰습니다.로드밸런서와 NAT Gateway는 가용성이 높은 관리형 서비스로 병목 현상에 대해 걱정할 필요가 없습니다.  💡Nat Gateway 알아보기  NAT(네트워크 주소 변환) 게이트웨이를 사용하면 프라이빗 서브넷의 인스턴스를 인터넷 또는 기타 AWS 서비스에 연결하는 한편, 인터넷에서 해당 인스턴스와의 연결을 시작하지 못하게 할 수 있습니다.NAT 게이트웨이를 만들려면 NAT 게이트웨이가 속할 퍼블릭 서브넷을 지정해야 하기 때문에 Step2에서 우선적으로 생성합니다.  도큐먼트로 더 알아보기  그림에서는 보이지 않지만, VPC에는 암시적 라우터가 있으며 라우팅 테이블을 사용하여 네트워크 트래픽이 전달되는 위치를 제어합니다.VPC의 각 서브넷을 라우팅 테이블에 연결해야 합니다. 테이블에서는 서브넷에 대한 라우팅을 제어합니다.Step 2 코드를 적용 후, 명령어로 인프라 상태를 점검해 아래와 같다면 다음 단계로 🚀$ terraform state listdata.aws_availability_zones.availableaws_eip.nat_1aws_eip.nat_2aws_internet_gateway.igwaws_nat_gateway.nat_gateway_1aws_nat_gateway.nat_gateway_2aws_route_table.route_table_pubaws_route_table_association.route_table_association_1aws_route_table_association.route_table_association_2aws_subnet.pub_sub_1aws_subnet.pub_sub_2aws_vpc.main이번 포스팅에서 ALB와 인스턴스는 다루지 않습니다. 추후, 모듈로 테라폼을 관리하는 방법에서 학습하겠습니다. Step 3Layer 2️⃣️ : Apps in a private subnet2개의 프라이빗 서브넷에 각각의 인스턴스를 놓습니다. 이후, 두 퍼블릭 서브넷에 연결된 ALB는 프라이빗 서브넷 리소스 간의 트래픽을 분산시킵니다.  ❗️예제 그림에서는 Private subnet의 cidr block을 10.0.2.0/22로 가이드 하지만, 이는 앞서 만든 서브넷과 범위가 겹치므로 10.0.4.0/22로 바꿔 진행합니다.CIDR 계산기 에서 정확하게 확인해 볼 수 있습니다.  Step3에서도 Step2와 같이 그림에서는 보이지 않는 라우트 테이블을 만들고 NAT 게이트웨이와 프라이빗 서브넷을 연결해 줍니다.프라이빗 서브넷의 요청이 외부로 나갈때는 NAT 게이트웨이의 고정 IP를 사용합니다.(프라이빗 서브넷의 라우트 테이블은 퍼블릭과 달리 2개를 만들어 각각 연결해 주었습니다.)Step 3 코드를 적용 후, 명령어로 인프라 상태를 점검해 아래와 같다면 다음 단계로 🚀$ terraform state listdata.aws_availability_zones.availableaws_eip.nat_1aws_eip.nat_2aws_internet_gateway.igwaws_nat_gateway.nat_gateway_1aws_nat_gateway.nat_gateway_2aws_route.private_nat_1aws_route.private_nat_2aws_route_table.route_table_pri_1aws_route_table.route_table_pri_2aws_route_table.route_table_pubaws_route_table_association.route_table_association_1aws_route_table_association.route_table_association_2aws_route_table_association.route_table_pri_association_1aws_route_table_association.route_table_pri_association_2aws_subnet.pri_sub_1aws_subnet.pri_sub_2aws_subnet.pub_sub_1aws_subnet.pub_sub_2aws_vpc.main이번 포스팅에서 DB와 인스턴스 연결은 다루지 않습니다. 추후, 모듈로 테라폼을 관리하는 방법에서 학습하겠습니다. Step 4Layer 3️⃣ : Data in a second private subnet첫 번째 프라이빗 서브넷 뒤에 두 번째 프라이빗 서브넷을 배치합니다. (코드 생략) 장애가 발생할 경우를 대비해 read-replica 혹은 standby 구성으로 배치합니다.  ❗Step4의 서브넷 작성법은 Step3의 방법과 동일합니다. 아래 사진은 CIDR의 범위가 겹치므로, Private subnet의 cidr block을 10.0.8.0/23을 10.0.12.0/23로, 10.0.10.0/23을 10.0.14.0/23으로 바꿔 진행하세요.  데이터 리소스(/23)보다를 앱 리소스(/22)를 확장할 가능성이 커, 더 큰 서브넷 마스크를 할당합니다.Step 5Leave extra IPs available배포된 인프라가 확장되어 아키텍처가 변경될 때 사용할 수 있는 여유분의 IP를 예약을 할 수도 있습니다. (코드 생략)위와 같은 단계들을 통해 3 Tier VPC Architecture를 학습해보았습니다.Terraform으로 생성된 자원들은 terraform destory명령어를 통해 학습을 시작하기 전 상태로 되돌리세요.다음 포스팅에서는 이번 포스팅에서 생략했던 인스턴스, DB, LB 등을 모듈로 관리하며 다뤄 보겠습니다.Terraform Courses    3-Tier VPC Architecture with Terraform    ECR Pipeline with Terraform Ⅰ (CodeCommit)    ECR Pipeline with Terraform Ⅱ (ECR, CodeBuild, IAM)    ECR Pipeline with Terraform Ⅲ (CodePipeline)    Provision Infrastructure with Packer",
        "url": "/3Tier"
    }
    ,
    
    "pipenv01": {
        "title": "Pipenv, Nginx, Gunicorn 서버 운영하기",
            "author": "HeuristicWave",
            "category": "",
            "content": "Virtualenv 환경이 아닌 Pipenv를 사용하며 만난 에러 해결 과정 정리사전 작업자세한 방법은 하단 참고자료를 통해 확인 할 수 있다      gunicorn, nginx 설치          ec2에서 nginx 설치하기 : CentOS 7 Nginx 설치 방법            gunicorn 작동확인  Gunicorn 🦄서비스 등록 스크립트 생성/etc/systemd/system/gunicorn.service 파일을 아래와 같은 내용으로 생성.pipenv는 venv와 ExecStart 경로가 다르다는 점을 유념해 작성하자[Unit]Description=gunicorn daemonAfter=network.target[Service]User=ec2-userGroup=ec2-userWorkingDirectory=/home/ec2-user/django/repoExecStart=/usr/local/bin/pipenv run gunicorn --workers 3 \\        &lt;wsgi가 위치한 폴더&gt;.wsgi:application --bind 0.0.0.0:8000[Install]WantedBy=multi-user.target  본래 --bind부분에 unix:/home/ec2-user/django/gunicorn.sock 를 넣어 구동하면 repo의 상위 폴더에  gunicorn.sock가 생긴다.nginx의 proxy_pass 부분도 http://unix:/{$PATH}/gunicorn.sock을 기재해 sock로 구성하는 것이 맞는 방법 같은데… 이 부분에 대해서는 학습이 필요하다.서비스 등록sudo systemctl start gunicornsudo systemctl enable gunicorn서비스 구동 확인sudo systemctl status gunicornNginx사이트 설정 추가ec2에 nginx를 받았을 때, etc/nginx/sites-enabled 와 etc/nginx/sites-availabe 이 존재하지 않는다. 해당 경로에 없다면 만들어주고 있으면 default 파일을 삭제하자.server {        listen 80;        server_name &lt;IP or 도메인&gt;;        charset utf-8;        location / {                include proxy_params;                proxy_pass http://0.0.0.0:8000        }        location /static/ {                root /home/ec2-user/django/repo;        }                location /media/ {                root /home/ec2-user/django/repo;        }}  nginx 주요 개념, nginx : root vs aliasinclude proxy_params의 경우 /etc/nginx/proxy_params 에 프록시 헤더를 기재 해야 한다. (다음 링크 참고) nginx &amp; aws사이트 추가sudo ln -s /etc/nginx/sites-available/django_test /etc/nginx/sites-enabled기동sudo systemctl start nginx기타 도움이 되는 명령어sudo systemctl daemon-reloadsudo systemctl stop, restart nginxps -efpspkill gunicorn  dotenv 관련 에러 해결하기            gunicorn 을 활용해 연결 할 경우      $ pip uninstall dotenv$ pip install python-dotenv                     docker + nginx + gunicorn 을 활용할 경우      $ pip uninstall dotenv$ pip install python-dotenv             참고자료  gunicorn 사전작업  Nginx, Gunicorn, Django 연동하기",
        "url": "/pipenv01"
    }
    
    
    };
</script>
<script src="assets/js/lunr.js"></script>
<script src="assets/js/search.js"></script>

            </section>

        </article>

    </div>
</main>

<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->
<script>
$(function() {
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
});
</script>



        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="https://heuristicwave.github.io/">Heuristic Wave Blog</a> &copy; 2021</section>
                <section class="poweredby">Proudly published with <a href="https://jekyllrb.com/">Jekyll</a> &
                    <a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a> using
                    <a href="https://github.com/jekyller/jasper2" target="_blank" rel="noopener">Jasper2</a></section>
                <nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    
                    
                    <a href="https://ghost.org" target="_blank" rel="noopener">Ghost</a>
                </nav>
            </div>
        </footer>

    </div>

    <!-- The big email subscribe modal content -->
    
        <div id="subscribe" class="subscribe-overlay">
            <a class="subscribe-overlay-close" href="#"></a>
            <div class="subscribe-overlay-content">
                
                <h1 class="subscribe-overlay-title">Search on Heuristic Wave Blog</h1>
                <p class="subscribe-overlay-description">lunr.js를 이용한 posts 검색</p>
                <span id="searchform" method="post" action="/search/" class="">
    <input class="confirm" type="hidden" name="confirm"  />
    <input class="location" type="hidden" name="location"  />
    <input class="referrer" type="hidden" name="referrer"  />

    <div class="form-group">
        <input class="subscribe-email" onkeyup="myFunc()"
               id="searchtext" type="text" name="searchtext"
               placeholder="Search..." />
    </div>
    <script type="text/javascript">
        function myFunc() {
            if(event.keyCode == 13) {
                var url = encodeURIComponent($("#searchtext").val());
                location.href = "/search.html?query=" + url;
            }
        }
    </script>
</span>
                <br>
            </div>
        </div>
    

    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.10.0/components/prism-abap.min.js"></script>
    <script>$(document).ready(function() {
      $('pre code').each(function(i, block) {
        hljs.highlightBlock(block);
      });
    });</script>

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://demo.ghost.io/assets/js/jquery.fitvids.js?v=724281a32e"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google Analytics Tracking code -->
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-xxxxxxxx-x', 'auto');
  ga('send', 'pageview');

 </script>


    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>
$(function() {
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
});
</script>

    

    <!-- Ghost outputs important scripts and data with this tag - it should always be the very last thing before the closing body tag -->
    <!-- ghost_foot -->

</body>
</html>
