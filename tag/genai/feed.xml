<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator>
  <link href="https://heuristicwave.github.io/tag/genai/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://heuristicwave.github.io/" rel="alternate" type="text/html" />
  <updated>2025-09-30T09:12:29+00:00</updated>
  <id>https://heuristicwave.github.io/tag/genai/feed.xml</id>

  
  
  

  
    <title type="html">Heuristic Wave Blog | </title>
  

  
    <subtitle>Careful Writer</subtitle>
  

  

  
    
      
    
  

  
  

  
    <entry>
      <title type="html">Amazon Nova Vision 기능 모델 증류 하기: Fine-Tuning Text-Image-to-Text</title>
      <link href="https://heuristicwave.github.io/Fine-tuning-Nova" rel="alternate" type="text/html" title="Amazon Nova Vision 기능 모델 증류 하기: Fine-Tuning Text-Image-to-Text" />
      <published>2025-05-05T00:00:00+00:00</published>
      <updated>2025-05-05T00:00:00+00:00</updated>
      <id>https://heuristicwave.github.io/Fine-tuning-Nova</id>
      <content type="html" xml:base="https://heuristicwave.github.io/Fine-tuning-Nova">&lt;p&gt;이 포스팅에서는 Amazon Nova Pro Model의 Vision 성능을 Lite Model에 효과적으로 전달하기 위한 Text-Image-to-Text 미세 조정 방법을 소개합니다.&lt;/p&gt;

&lt;p&gt;본격적인 내용에 들어가기에 앞서, 처음에는 Vision 분야의 Model Distillation(이하, 모델 증류) 기법을 직접 다루고자 했으나, 현재 Amazon Bedrock에서 이에 대한 직접적인 지원이 제한적이라는 점을 말씀드립니다. 이에 대안으로 “Fine-Tuning: Text-Image-to-Text” 방식을 활용하여 Vision 모델 증류를 우회적으로 구현하는 방법을 공유하고자 합니다.&lt;/p&gt;

&lt;h2 id=&quot;️-model-distillation&quot;&gt;&lt;a href=&quot;#distillation&quot;&gt;⚗️ Model Distillation&lt;/a&gt;&lt;a id=&quot;distillation&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;지난 2024년 re:Invent에서 Amazon Bedrock 생태계는 Fine-tuning과 Continued pre-training 외에도 &lt;strong&gt;Distillation&lt;/strong&gt;이라는 새로운 모델 커스터마이제이션 기능을 제공하기 시작했습니다. 또 &lt;a href=&quot;https://aws.amazon.com/ko/about-aws/whats-new/2025/04/amazon-nova-premier-complex-tasks-model-distillation/&quot;&gt;최근(4월 30일)&lt;/a&gt;에는 복잡한 작업들에 대한 모델 증류를 위한 교사 모델로 Amazon Nova Premier를 출시하기도 했습니다.&lt;/p&gt;

&lt;p&gt;모델 증류는 대규모 교사 모델(Teacher Model)의 지식을 더 작은 학생 모델(Student Model)로 전달하는 기술로, 모델 크기와 연산 비용을 줄이면서도 성능을 최대한 유지할 수 있습니다.
Amazon Bedrock Model Distillation은 두 가지 주요 단계로 구성됩니다. 첫째, 훈련에 필요한 학습 데이터를 생성하는 과정과 둘째, 이렇게 만들어진 학습 데이터를 활용하여 교사 모델을 미세 조정함으로써 증류 모델을 생성하는 과정입니다.
&lt;img src=&quot;../../assets/built/images/post/ai/Bedrock/modelDistillation.png&quot; alt=&quot;Model Distillation&quot; /&gt;
Bedrock은 현재 이미지 작업에 대한 모델 증류를 공식적으로 지원하지 않습니다. 그러나 증류 과정의 기본 원리를 이해하고 있다면, 교사 모델을 활용해 직접 훈련용 데이터를 생성하고 별도로 미세 조정을 수행함으로써 이미지 작업에 대한 모델 증류를 자체적으로 구현할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;-과제-설정---image-labeling-작업-비교&quot;&gt;&lt;a href=&quot;#task&quot;&gt;📸 과제 설정 - Image Labeling 작업 비교&lt;/a&gt;&lt;a id=&quot;task&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Vision Understanding 기능이 있는 멀티모달 모델은 주어진 이미지를 설명하는 &lt;a href=&quot;https://huggingface.co/docs/transformers/en/tasks/image_captioning&quot;&gt;Image Captioning&lt;/a&gt; 기능을 탑재하고 있습니다.
이미지를 제공하고 원하는 스타일(촬영 기법, 분위기, 객체 등)에 대한 키워드 추출을 요청하면, 해당 이미지에 대한 관련 키워드를 받을 수 있습니다.&lt;/p&gt;

&lt;details&gt;
  &lt;summary&gt;👉 Image Labeling Example Prompt 👈&lt;/summary&gt;
  &lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;당신은 이미지 키워드 추출 전문가입니다. 이미지를 분석하여 검색에 최적화된 간결한 키워드를 추출해주세요.

다음 5가지 카테고리에 따라 키워드를 추출하되, 최종 결과는 카테고리 구분 없이 쉼표로 구분된 단일 목록으로 제공해주세요:

1. 주요 객체/인물: 인물(성별, 연령대, 민족성), 동물, 사물 등 핵심 요소
2. 위치/배경: 장소, 풍경, 환경(실내/외), 시간, 계절
3. 행동/감정: 활동을 설명하는 동사, 분위기를 나타내는 형용사
4. 시각적 특성: 주요 색상, 구도, 촬영 기법, 이미지 스타일
5. 맥락적 요소: 패션, 랜드마크, 문화적 맥락, 행사/축제 관련 정보

각 카테고리당 2-5개, 총 15-25개의 검색 최적화된 키워드를 제공해주세요. 중복을 피하고 간결하게 작성하세요.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/details&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/post/ai/Bedrock/auto-labeling.png&quot; alt=&quot;Image Labeling&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 표 이미지는 &lt;a href=&quot;https://huggingface.co/datasets/ShutterstockInc/high_resolution_images&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ShutterstockInc/high_resolution_images&lt;/code&gt; 데이터셋&lt;/a&gt; 중 하나의 사진에 대해 Nova Pro와 Lite 모델로 Image Labeling을 진행한 결과입니다.
동일한 프롬프트를 사용하더라도 각 모델의 응답 결과가 매우 상이함을 확인할 수 있습니다. &lt;strong&gt;본 포스팅에서는 어느 모델이 Image Labeling 작업에 우위에 있는지를 판단하기보다는, Lite 모델이 Pro 모델과 유사한 답변을 도출하도록 하는 작업에 초점을 맞추었다는 점을 기억해주세요!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;두 모델의 답변 유사도를 파악하기 위해 공통적으로 제시한 교집합 부분의 자카드 지수(Jaccard index)를 측정한 결과, 0.129로 나타났습니다. 이제 Lite 모델을 Pro의 데이터로 미세 조정하여 두 모델 간의 응답값이 얼마나 유사해질 수 있는지 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;-vlm-모델-증류-자체-구현&quot;&gt;&lt;a href=&quot;#self-diatillation&quot;&gt;🧑‍🔬 VLM 모델 증류 자체 구현&lt;/a&gt;&lt;a id=&quot;self-diatillation&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&quot;데이터셋-준비-프로세스&quot;&gt;데이터셋 준비 프로세스&lt;/h3&gt;

&lt;p&gt;VLM 모델을 자체적으로 증류하기 위해 Text-Image-to-Text 방식의 미세 조정을 진행하겠습니다. 이를 위해 다음과 같은 4가지 단계로 미세 조정용 데이터 셋을 준비해야 합니다.
&lt;img src=&quot;../../assets/built/images/post/ai/Bedrock/create_datasets.png&quot; alt=&quot;Image Labeling&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;본 글에서는 VLM 모델 증류를 자체적으로 구현하기 위해 Hugging Face에 올라온 &lt;a href=&quot;https://huggingface.co/datasets/ShutterstockInc/high_resolution_images&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ShutterstockInc/high_resolution_images&lt;/code&gt;의 medium 데이터셋&lt;/a&gt;을 활용하였습니다.&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;1-이미지-전처리&quot;&gt;1. 이미지 전처리&lt;/h4&gt;

&lt;p&gt;이미지 전처리의 범주는 매우 광범위합니다. 여기서는 특정 작업에 적합한 분류가 완료되었다고 가정하고, 이미지 크기 조정에 관한 전처리만 다루겠습니다.
작업별로 요구되는 해상도는 다르지만, 대부분의 경우 굳이 고해상도 이미지가 필요하지 않습니다.&lt;/p&gt;

&lt;p&gt;예를 들어, Claude 모델은 다음 공식으로 이미지의 토큰량을 계산합니다: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;토큰 수 = (너비 px × 높이 px) ÷ 750&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;300 × 199 이미지의 경우&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;총 픽셀 수: 300 × 199 = 59,700 픽셀&lt;/li&gt;
  &lt;li&gt;소요 토큰 수: 59,700 ÷ 750 = 79.6 ≈ &lt;strong&gt;80 토큰&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;1000 × 665 이미지의 경우&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;총 픽셀 수: 1000 × 665 = 665,000 픽셀&lt;/li&gt;
  &lt;li&gt;소요 토큰 수: 665,000 ÷ 750 = 886.67 ≈ &lt;strong&gt;887 토큰&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이처럼 이미지 해상도에 따라 토큰 소모량이 크게 달라지므로, 훈련용 데이터셋을 구축하기 전에 고해상도 이미지의 크기를 적절히 축소하는 것이 중요합니다. 
이는 모델 훈련 비용을 절감할 뿐만 아니라, 처리 속도 향상에도 기여하면서 대부분의 작업에서 성능 저하 없이 효율적인 학습을 가능하게 합니다.&lt;/p&gt;

&lt;h4 id=&quot;2-레퍼런스-데이터-구성&quot;&gt;2. 레퍼런스 데이터 구성&lt;/h4&gt;

&lt;p&gt;이 과정에서는 교사 모델을 호출하여 프롬프트-응답(Prompt-Response) 쌍의 데이터를 생성합니다. 교사 모델이 생성한 응답은 이후 학생 모델의 미세 조정 데이터로 활용됩니다.
멀티모달 기능을 지원하는 Converse API를 통해 교사 모델을 호출하고, 모델의 응답과 해당 이미지 파일명을 미세 조정용 데이터셋 구축을 위해 JSONL 형식으로 저장했습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;system_prompts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;system_prompt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;conversation&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;role&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;user&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;content&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;user_prompts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;s&quot;&gt;&quot;image&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                    &lt;span class=&quot;s&quot;&gt;&quot;format&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;jpeg&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;s&quot;&gt;&quot;source&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;bytes&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image_bytes&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;converse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;modelId&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;teacher_model_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;system&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;system_prompts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;messages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conversation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;inferenceConfig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;maxTokens&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;temperature&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;topP&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;reponse_text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;output&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;message&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;content&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;jsonl_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;image&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;label&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reponse_text&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;3-훈련-데이터셋-생성&quot;&gt;3. 훈련 데이터셋 생성&lt;/h4&gt;

&lt;p&gt;Bedrock의 미세 조정 요구사항에 맞추어, &lt;a href=&quot;https://docs.aws.amazon.com/nova/latest/userguide/fine-tune-prepare-data-understanding.html#custom-fine-tune-constraints&quot;&gt;Preparing data for fine-tuning Understanding models&lt;/a&gt; 가이드라인을 참고하여 모델 학습에 필요한 데이터셋을 JSONL 형식으로 생성합니다.&lt;/p&gt;

&lt;p&gt;본 포스팅에서는 &lt;a href=&quot;https://docs.aws.amazon.com/nova/latest/userguide/fine-tune-prepare-data-understanding.html#customize-fine-tune-examples&quot;&gt;Single image custom fine tuning format&lt;/a&gt; 형식으로 데이터를 준비합니다.
이 과정에서 앞서 &lt;strong&gt;두 번째 단계에서 생성한 데이터를 활용&lt;/strong&gt;하여 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;system&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;messages&lt;/code&gt;의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text&lt;/code&gt; 값과 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;image&lt;/code&gt;의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uri&lt;/code&gt; 필드에 적절히 배치함으로써 완성된 데이터셋을 구성합니다.&lt;/p&gt;

&lt;h4 id=&quot;4-데이터셋-검증&quot;&gt;4. 데이터셋 검증&lt;/h4&gt;

&lt;p&gt;미세 조정 작업을 시작하기 전에, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aws-samples&lt;/code&gt; GitHub 저장소에서 제공하는 &lt;a href=&quot;https://github.com/aws-samples/amazon-bedrock-samples/tree/main/custom-models/bedrock-fine-tuning/nova/understanding/dataset_validation#dataset-validation-for-fine-tuning-nova-understanding-models&quot;&gt;Dataset Validation for Fine-tuning Nova Understanding models&lt;/a&gt; 도구를 활용하여 데이터셋의 유효성을 먼저 점검하세요.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python3 nova_ft_dataset_validator.py -i &amp;lt;file path&amp;gt; -m &amp;lt;model name&amp;gt;&lt;/code&gt; 명령어를 실행하면 검사가 진행되며, 모든 샘플이 검증을 통과할 경우 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Validation successful, all samples passed&lt;/code&gt; 메시지가 표시됩니다.&lt;/p&gt;

&lt;h3 id=&quot;미세-조정&quot;&gt;미세 조정&lt;/h3&gt;

&lt;p&gt;데이터셋 준비가 완료되면, 미세 조정 과정은 매우 간단하게 진행됩니다. Amazon Bedrock 콘솔에서 데이터셋이 저장된 S3 위치를 지정하고 필요한 하이퍼파라미터 값만 설정하면 됩니다.
이번 훈련에서는 Nova Lite 모델의 기본 에포크(Epoch) 값인 2 대신 5로 상향 조정하였으며, 그 외 파라미터는 기본값을 유지하여 진행했습니다.
&lt;img src=&quot;../../assets/built/images/post/ai/Bedrock/hyperparams.png&quot; alt=&quot;Hyperparams&quot; /&gt;
훈련이 완료되면, 미세 조정 과정에서 지정한 S3 위치에 훈련 결과 지표들이 저장됩니다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;step_wise_training_metrics.csv&lt;/code&gt; 파일을 통해 각 단계(step)와 에포크(epoch)별 훈련 손실(training_loss) 값을 확인할 수 있어, 모델의 학습 진행 결과를 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;️-fine-tuning-text-image-to-text-결과&quot;&gt;&lt;a href=&quot;#results&quot;&gt;🖍️ Fine-Tuning Text-Image-to-Text 결과&lt;/a&gt;&lt;a id=&quot;results&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;본 포스팅에서는 &lt;a href=&quot;https://huggingface.co/datasets/ShutterstockInc/high_resolution_images&quot;&gt;🤗 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ShutterstockInc/high_resolution_images&lt;/code&gt;&lt;/a&gt;의 medium 데이터셋을 활용했으며, 이 데이터셋은 총 1,000장의 이미지로 구성되어 있습니다.
데이터 활용 방식으로는 900장을 훈련용 데이터로 사용하고, 나머지 100장은 미세 조정 완료 후 모델 성능 검증에 활용했습니다. 데이터의 제한적인 특성을 고려하여, 300장과 900장을 각각 사용한 두 가지 훈련 세션을 진행했습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Nova Pro &amp;amp; Nova Lite 비교&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;먼저 미세 조정을 거치지 않은 상태에서 Nova Pro와 Lite 모델 간의 성능 차이를 확인하기 위해, 100장의 이미지에 대한 분석 결과를 비교했습니다. 두 모델 간의 Jaccard 유사도는 대부분 0.1에서 0.4 사이에 분포하는 것으로 나타났습니다.
&lt;img src=&quot;../../assets/built/images/post/ai/Bedrock/jaccard-case1.png&quot; alt=&quot;case1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Nova Pro &amp;amp; Nova Lite (300장)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;300개의 샘플 데이터로 훈련을 진행한 결과, Jaccard 유사도가 0.2에서 0.6 사이로 향상되었습니다. 이는 비교적 적은 양의 데이터만으로도 Lite 모델이 Pro 모델의 성능에 근접할 수 있음을 보여주는 결과입니다.
&lt;img src=&quot;../../assets/built/images/post/ai/Bedrock/jaccard-case2.png&quot; alt=&quot;case2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Nova Pro &amp;amp; Nova Lite (900장)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;900개의 샘플 데이터로 훈련을 진행한 결과, Jaccard 유사도가 0.2에서 0.6 사이로 향상되었으며, 300장으로 훈련한 모델(빨간색)과 비교했을 때 900장으로 훈련한 모델(보라색)이 소폭 더 높은 성능을 보여주었습니다.
이번 실험에서는 이미지 데이터의 제한으로 900장만 활용했지만, Amazon Bedrock의 이미지 미세 조정 기능은 최대 20,000개의 데이터를 지원합니다. 따라서 더 많은 데이터로 미세 조정을 진행할 경우, 성능이 더욱 향상될 것으로 기대됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/post/ai/Bedrock/jaccard-case3.png&quot; alt=&quot;case2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;-모델-customization-비용&quot;&gt;&lt;a href=&quot;#cost&quot;&gt;💸 모델 Customization 비용&lt;/a&gt;&lt;a id=&quot;cost&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;실험에 소요된 비용 내역을 기재했으니, 향후 미세 조정 작업 계획 시 예상 비용 산출에 도움이 되길 바랍니다. 🙃&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Nova Lite 미세 조정 소모 비용&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;사용량 유형&lt;/th&gt;
      &lt;th&gt;데이터 수&lt;/th&gt;
      &lt;th&gt;비용&lt;/th&gt;
      &lt;th&gt;훈련 시간&lt;/th&gt;
      &lt;th&gt;전용 처리량 비용 (무약정)&lt;/th&gt;
      &lt;th&gt;모델 보관 비용&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;USE1-NovaLite-Customization-Training&lt;/td&gt;
      &lt;td&gt;300개&lt;/td&gt;
      &lt;td&gt;약 $2.1&lt;/td&gt;
      &lt;td&gt;약 1시간&lt;/td&gt;
      &lt;td&gt;시간당 $108.15&lt;/td&gt;
      &lt;td&gt;월 $1.95&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;USE1-NovaLite-Customization-Training&lt;/td&gt;
      &lt;td&gt;900개&lt;/td&gt;
      &lt;td&gt;약 $7.5&lt;/td&gt;
      &lt;td&gt;약 2시간&lt;/td&gt;
      &lt;td&gt;시간당 $108.15&lt;/td&gt;
      &lt;td&gt;월 $1.95&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;이 비용 내역에는 교사 모델을 활용하여 프롬프트-응답 쌍 데이터를 생성하는 데 소모된 비용은 포함되지 않았습니다.
해당 비용을 산출하려면, 작업을 1회 수행한 후 소모된 토큰량을 측정하여 별도로 계산하시기 바랍니다.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;-마치며&quot;&gt;&lt;a href=&quot;#outro&quot;&gt;🌟 마치며&lt;/a&gt;&lt;a id=&quot;outro&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;이번 포스팅에서는 Amazon Bedrock이 Vision 작업에 대한 모델 증류를 공식적으로 지원하지 않는 상황에서, Text-Image-to-Text 미세 조정을 통해 우회적으로 모델 증류를 구현하는 방법을 살펴보았습니다.
성공적인 VLM 모델 증류를 위해서는 체계적인 데이터셋 준비 과정이 필수적입니다. 이미지 전처리를 통한 토큰 소모량 최적화, 교사 모델을 활용한 레퍼런스 데이터 구축, Bedrock 요구사항에 맞는 훈련 데이터셋 생성, 그리고 미세 조정 전 데이터셋 검증까지의 단계가 모델 성능에 직접적인 영향을 미칩니다.
또한 미세 조정 완료 후에는 반드시 검증 과정을 통해 모델의 성능 향상을 확인해야 합니다. 본 글에서는 Jaccard 유사도를 통해 모델 간 응답 일치도를 측정했으며, 데이터양이 증가할수록 Lite 모델이 Pro 모델의 응답에 더 가까워지는 것을 확인할 수 있었습니다.&lt;/p&gt;

&lt;p&gt;이러한 우회적 증류 방법은 비록 공식 지원 기능은 아니지만, 적절한 데이터셋 구성과 미세 조정을 통해 경량 모델에서도 고성능 모델과 유사한 결과를 얻을 수 있음을 보여줍니다. 앞으로 Amazon Bedrock에서 Vision 모델 증류에 대한 공식 지원이 확대되기를 기대하며, 그때까지는 이러한 접근법이 실무에서 유용하게 활용될 수 있을 것입니다. 여러분의 프로젝트에도 이러한 방법론이 도움이 되길 바랍니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;🤣 사실 이번 포스팅은 제가 AWS Seoul Summit 2025 발표를 준비하면서 실험했던 내용 중 일부랍니다. 발표 영상이 나오면 여기에 함께 공유해 드릴게요!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;🚨 광고 : Bedrock 내에서 나만의 모델을 만드는 다른 방법(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;지속적인 사전 훈련&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;미세 조정&lt;/code&gt;)들은 &lt;br /&gt; &lt;strong&gt;“Amazon Bedrock으로 시작하는 실전 생성형 AI 개발”&lt;/strong&gt;를 참고하세요! &lt;br /&gt;
판매 링크 : &lt;a href=&quot;https://www.yes24.com/Product/Goods/140049541&quot;&gt;📘 예스24&lt;/a&gt;, &lt;a href=&quot;https://product.kyobobook.co.kr/detail/S000214962344&quot;&gt;📗 교보문고&lt;/a&gt;, &lt;a href=&quot;https://www.aladin.co.kr/shop/wproduct.aspx?ItemId=353662061&amp;amp;start=slayer&quot;&gt;📕 알라딘&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;</content>

      
      
      
      
      

      <author>
          <name>Jihun Lim</name>
        
        
      </author>

      

      
        <category term="aws" />
      
        <category term="genai" />
      

      
        <summary type="html">이 포스팅에서는 Amazon Nova Pro Model의 Vision 성능을 Lite Model에 효과적으로 전달하기 위한 Text-Image-to-Text 미세 조정 방법을 소개합니다.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Exploring Amazon Kendra GenAI Index with Amazon Bedrock Knowledge Bases</title>
      <link href="https://heuristicwave.github.io/Kendra2" rel="alternate" type="text/html" title="Exploring Amazon Kendra GenAI Index with Amazon Bedrock Knowledge Bases" />
      <published>2025-02-01T00:00:00+00:00</published>
      <updated>2025-02-01T00:00:00+00:00</updated>
      <id>https://heuristicwave.github.io/Kendra2</id>
      <content type="html" xml:base="https://heuristicwave.github.io/Kendra2">&lt;p&gt;Amazon Bedrock Knowledge Bases로 알아보는 Amazon Kendra GenAI Index&lt;/p&gt;

&lt;h2 id=&quot;-kendra-genai-index&quot;&gt;&lt;a href=&quot;#kendra&quot;&gt;🔍 Kendra GenAI Index&lt;/a&gt;&lt;a id=&quot;kendra&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;2023년 12월 12일 Amazon Bedrock Knowledge Bases(이하 지식 기반)가 정식 출시된 이후, AWS 생태계에서 유일한 Retriever 역할을 담당하던 Kendra는 한동안 주목받지 못했습니다.
하지만 &lt;a href=&quot;https://aws.amazon.com/ko/about-aws/whats-new/2024/12/genai-index-amazon-kendra/&quot;&gt;2024년 re:Invent에서 Amazon Kendra GenAI Index&lt;/a&gt;라는 새로운 기능이 추가되었습니다.
이는 &lt;strong&gt;Bedrock 지식 기반의 관리형 리트리버로 제공&lt;/strong&gt;되어, 사용자들이 보다 손쉽게 관리형 서비스를 통해 RAG를 구축할 수 있게 되었습니다.&lt;/p&gt;

&lt;p&gt;새로운 기능의 도입과 함께, AWS는 Amazon Kendra GenAI Index에 대한 새로운 가격 정책을 발표하였습니다.
이 가격 구조는 최신 기능의 가치를 반영하면서도, 서비스의 접근성을 높여 더 많은 사용자들이 고급 검색 및 RAG 기능을 활용할 수 있도록 설계되었습니다.
그럼 이제 Amazon Kendra GenAI Index의 새로운 가격 정책과 그 특징들을 자세히 살펴보겠습니다.&lt;/p&gt;

&lt;h3 id=&quot;가격-정책&quot;&gt;&lt;a href=&quot;https://aws.amazon.com/ko/kendra/pricing/&quot;&gt;가격 정책&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;새로운 GenAI 에디션은 커넥터를 제외한 기본 인덱스(스토리지 및 쿼리 요금)의 시간당 비용이 $0.32로, &lt;strong&gt;기존 베이직 에디션 대비 1/4 수준으로 낮아져 더 경제적&lt;/strong&gt;으로 서비스를 이용할 수 있게 되었습니다.
커넥터 요금의 경우, 데이터 동기화가 빈번한 서비스에서는 GenAI 에디션이 비용 효율적일 수 있습니다. 다만, 기본 인덱스 요금만큼 큰 폭의 가격 인하가 이루어지지 않은 점은 아쉬움으로 남습니다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;구성 요소&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;GenAI 엔터프라이즈 에디션&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;베이직 엔터프라이즈 에디션&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;기본 개발자 에디션&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;기본&lt;br /&gt; 인덱스&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;시간당 0.32 USD &lt;br /&gt;&lt;br /&gt; 포함 항목 &lt;br /&gt;- 최대 문서 20,000개 or 추출된 텍스트 200MB &lt;br /&gt;- 0.1 QPS(하루 약 8,000개의 쿼리)&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;시간당 1.4 USD &lt;br /&gt;&lt;br /&gt; 포함 항목 &lt;br /&gt;- 최대 문서 100,000개 or 추출된 텍스트 30GB &lt;br /&gt;- 0.1 QPS(하루 약 8,000개의 쿼리)&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;시간당 1.125 USD &lt;br /&gt;&lt;br /&gt; 포함 항목 &lt;br /&gt;- 최대 문서 10,000개 or 추출된 텍스트 3GB&lt;br /&gt;- 0.05 QPS(하루 약 4,000개의 쿼리)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;커넥터&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;인덱스당 월 30 USD 고정 요금&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;- 동기화 시 시간당 0.35 USD &lt;br /&gt; - 동기화 중 스캔한 문서 100만 개당 1 USD&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;- 동기화 시 시간당 0.35 USD &lt;br /&gt; - 동기화 중 스캔한 문서 100만 개당 1 USD&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;-kendra로-지식-기반-구축하기&quot;&gt;&lt;a href=&quot;#setting&quot;&gt;📑 Kendra로 지식 기반 구축하기&lt;/a&gt;&lt;a id=&quot;setting&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;구축 방법은 &lt;a href=&quot;https://aws.amazon.com/ko/blogs/machine-learning/introducing-amazon-kendra-genai-index-enhanced-semantic-search-and-retrieval-capabilities/&quot;&gt;AWS Blog&lt;/a&gt;에 상세히 설명되어 있으므로, 이 글에서는 주요 포인트만 간략히 살펴보도록 하겠습니다.&lt;/p&gt;

&lt;h3 id=&quot;1-knowledge-base-with-kendra-genai-index-생성&quot;&gt;1. Knowledge Base with Kendra GenAI Index 생성&lt;/h3&gt;

&lt;p&gt;Bedrock 지식 기반에서 GenAI Index를 선택하고 필요한 권한을 부여하면, Kendra GenAI Index를 지식 기반으로 활용하기 위한 기본 인프라 구성이 완료됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/post/ai/Kendra/genaiIndex.png&quot; alt=&quot;genaiIndex&quot; /&gt;
&lt;img src=&quot;../../assets/built/images/post/ai/Kendra/genaiIndexRole.png&quot; alt=&quot;genaiIndexRole&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;2-kendra에서-amazon-s3-connector-생성&quot;&gt;2. Kendra에서 Amazon S3 connector 생성&lt;/h3&gt;

&lt;p&gt;Kendra GenAI Index가 생성되면 Bedrock 지식 기반 화면에서 확인할 수 있습니다. 이후 화면 하단의 &lt;strong&gt;데이터 소스&lt;/strong&gt; 메뉴에서 S3 커넥터를 선택하여 데이터 소스 연결을 완료하면 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/post/ai/Kendra/kendraConnector.png&quot; alt=&quot;kendraConnector&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3-데이터-업로드-및-데이터-소스-동기화&quot;&gt;3. 데이터 업로드 및 데이터 소스 동기화&lt;/h3&gt;

&lt;p&gt;S3 커넥터로 지정한 버킷에 테스트용 데이터&lt;em&gt;(최근 🔥한 arXiv에 게재된 &lt;a href=&quot;https://arxiv.org/pdf/2501.12948&quot;&gt;DeepSeek-R1 논문&lt;/a&gt;)&lt;/em&gt;를 업로드하고 데이터 동기화를 실행합니다.
데이터 싱크(인덱싱) 주기는 필요에 따라 자유롭게 설정할 수 있습니다. 이번 테스트에서는 그림과 같이 ‘Sync now’ 버튼을 사용하여 업로드한 문서를 Kendra에 즉시 인덱싱했습니다.
인덱싱이 완료되면 몇 분 후부터 Bedrock 지식 기반에서 해당 데이터를 활용한 LLM과의 대화가 가능합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/post/ai/Kendra/kendraSync.png&quot; alt=&quot;kendraSync&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;-bedrock에서-지원하는-다양한-문서-채팅-기능-알아보기&quot;&gt;&lt;a href=&quot;#chat&quot;&gt;💬 Bedrock에서 지원하는 다양한 문서 채팅 기능 알아보기&lt;/a&gt;&lt;a id=&quot;chat&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Bedrock에서는 Kendra GenAI Index 없이도 문서 기반 채팅이 가능합니다. 이제 Bedrock에서 제공하는 다양한 문서 채팅 방식들을 살펴보고, 이들과 Kendra GenAI Index 방식의 차이점을 비교해 보도록 하겠습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;🔫 3가지 방식을 동일하게 비교 테스트하였으며, 결과는 마지막 부분에서 확인하실 수 있습니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;모델 구성 정보&lt;/strong&gt;&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;모델 : Claude 3.5 Sonnet v2&lt;/li&gt;
    &lt;li&gt;Temperature / Top P / Top K : 0 / 1 / 250&lt;/li&gt;
    &lt;li&gt;Maximum length : 2048&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;playground의-기본-기능으로-대화하기&quot;&gt;Playground의 기본 기능으로 대화하기&lt;/h3&gt;

&lt;p&gt;Playground는 ChatGPT와 유사한 방식으로, Bedrock에서 가장 간단하게 LLM과 대화할 수 있는 기능입니다.
PDF를 포함한 다양한 형식의 문서를 텍스트와 함께 업로드하여 질의할 수 있습니다. 테스트를 위해 다음과 같은 질문을 진행해 보았습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/post/ai/Kendra/playground_with_pdf.png&quot; alt=&quot;playground&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;‘DeepSeek R1 Zero는 R1과는 어떤 훈련방식의 차이가 있나요?’&lt;/em&gt;라는 두 번째 질문을 진행하고, 이에 대한 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;modelinvocations&lt;/code&gt; 로그를 자세히 분석해 보도록 하겠습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;🚨 광고 : &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;modelinvocations&lt;/code&gt; 로그를 자세히 분석하는 방법은 &lt;br /&gt; &lt;strong&gt;“Amazon Bedrock으로 시작하는 실전 생성형 AI 개발”&lt;/strong&gt;를 참고하세요! &lt;br /&gt;
판매 링크 : &lt;a href=&quot;https://www.yes24.com/Product/Goods/140049541&quot;&gt;📘 예스24&lt;/a&gt;, &lt;a href=&quot;https://product.kyobobook.co.kr/detail/S000214962344&quot;&gt;📗 교보문고&lt;/a&gt;, &lt;a href=&quot;https://www.aladin.co.kr/shop/wproduct.aspx?ItemId=353662061&amp;amp;start=slayer&quot;&gt;📕 알라딘&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/post/ai/Kendra/playground_log.png&quot; alt=&quot;playground_log&quot; /&gt;&lt;/p&gt;

&lt;p&gt;답변의 품질은 우수했으나, 토큰 사용량이 상당히 많았습니다. 첫 번째 질문에서는 &lt;strong&gt;입력 토큰 20,192개와 출력 토큰 716개가 사용&lt;/strong&gt;되었고, 두 번째 질문에서는 &lt;strong&gt;입력 토큰 20,945개와 출력 토큰 660개가 소모&lt;/strong&gt;되었습니다. 이는 모든 문서를 참조하여 답변을 생성하는 방식 때문에 발생한 높은 토큰 소비량입니다.&lt;/p&gt;

&lt;h3 id=&quot;bedrock-지식-기반-기능으로-대화하기&quot;&gt;Bedrock 지식 기반 기능으로 대화하기&lt;/h3&gt;

&lt;p&gt;두 번째 방법으로, Amazon Bedrock 지식 기반의 ‘&lt;a href=&quot;https://docs.aws.amazon.com/ko_kr/bedrock/latest/userguide/knowledge-base-chatdoc.html&quot;&gt;문서와의 채팅&lt;/a&gt;’ 기능을 사용하여 테스트를 진행해 보겠습니다.
이 기능은 별도의 지식 기반 설정 없이도 신속하게 문서 기반 대화를 테스트할 수 있는 환경을 제공합니다.
Bedrock 지식 기반 콘솔의 ‘문서와의 채팅’ 기능에서 PDF 파일을 업로드하면 다음과 같이 대화를 진행할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/post/ai/Kendra/chatdoc.png&quot; alt=&quot;chatdoc&quot; /&gt;
&lt;img src=&quot;../../assets/built/images/post/ai/Kendra/naive_kb_with_pdf.png&quot; alt=&quot;naive&quot; /&gt;&lt;/p&gt;

&lt;p&gt;‘문서와의 채팅’ 기능은 답변 생성 시 참고한 소스 정보를 함께 제공합니다. Playground에서와 동일한 조건으로 테스트했음에도 불구하고 답변 결과가 다르게 나왔습니다. 이러한 차이의 원인을 파악하기 위해 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;modelinvocations&lt;/code&gt; 로그를 분석해 보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/post/ai/Kendra/chatdoc_log.png&quot; alt=&quot;chatdoc&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Playground에서는 질문과 PDF의 모든 내용이 하나의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;input&lt;/code&gt;으로 처리되었습니다. 반면 ‘문서와의 채팅’ 기능에서는 질문이 messages의 content에 포함되고, RAG를 통해 얻은 문맥 정보는 특정 명령어와 함께 시스템 프롬프트에 삽입되었음을 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;‘문서와의 채팅’ 시스템 프롬프트&lt;/strong&gt;&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/heuristicwave/2a17662da3357f178b73e7edc8a596a1.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;‘문서와의 채팅’ 기능은 현재 페이지를 벗어나면 대화 내용이 저장되지 않으므로 사용 시 주의가 필요합니다.
이제 본 포스팅의 주요 주제인 Kendra GenAI Index를 지식 기반으로 활용했을 때의 결과를 살펴보겠습니다.&lt;/p&gt;

&lt;h3 id=&quot;kendra-genai-index로-대화하기&quot;&gt;Kendra GenAI Index로 대화하기&lt;/h3&gt;

&lt;p&gt;Kendra를 지식 기반으로 활용할 경우, ‘문서와의 채팅’ 기능과 마찬가지로 참고한 데이터 소스를 제공한다는 점에서 유사합니다. 그러나 Kendra는 전문적인 검색 엔진으로서, &lt;strong&gt;RAG에 대해 아래와 같이 다양한 설정(검색 결과 수, 유형 등)&lt;/strong&gt;을 할 수 있다는 점에서 차이가 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/post/ai/Kendra/kendra_with_pdf.png&quot; alt=&quot;kendraQA&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번 테스트에서도 앞선 경우들과 동일한 조건을 적용했음에도 불구하고 답변이 다르게 나왔습니다. 로그 분석을 통해 그 원인을 파악해 보았습니다. 첫 번째 질문은 ‘문서와의 채팅’ 기능과 동일한 프롬프트로 처리되었습니다. 그러나 두 번째 질문에서는 최적화된 답변 생성을 위해 시스템이 &lt;strong&gt;자체적으로 쿼리의 의도를 분석하고 재작성하여 질문을 처리&lt;/strong&gt;했음을 확인했습니다. 결과적으로, &lt;strong&gt;하나의 질문에 대해 실제로는 언어 모델이 두 번 호출&lt;/strong&gt;되었습니다.&lt;/p&gt;

&lt;p&gt;다음은 사용자의 질문을 최적화하기 위해 쿼리 재작성을 시도하는 Bedrock 호출 로그입니다. 이 호출이 Stream 방식이 아닌 Converse 방식으로 이루어진다는 것도 차이점입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/post/ai/Kendra/kb_create_query.png&quot; alt=&quot;kendraRewriteQuery&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;사용자 쿼리 최적화 프롬프트&lt;/strong&gt;&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/heuristicwave/d9aea418105d8806132a6a1005a68e88.js&quot;&gt;&lt;/script&gt;

&lt;h2 id=&quot;-실험-결과&quot;&gt;&lt;a href=&quot;#result&quot;&gt;🧪 실험 결과&lt;/a&gt;&lt;a id=&quot;result&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&quot;답변-비교&quot;&gt;답변 비교&lt;/h3&gt;

&lt;p&gt;Q. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning에 대하여 요약해 알려주세요.&lt;/p&gt;

&lt;details&gt;
  &lt;summary&gt;Playground 답변, 👉 Click&lt;/summary&gt;

  &lt;p&gt;DeepSeek-R1에 대한 주요 내용을 다음과 같이 요약해드리겠습니다:&lt;/p&gt;

  &lt;p&gt;핵심 성과:&lt;/p&gt;

  &lt;ol&gt;
    &lt;li&gt;DeepSeek-R1-Zero&lt;/li&gt;
  &lt;/ol&gt;

  &lt;ul&gt;
    &lt;li&gt;지도학습 없이 순수 강화학습(RL)만으로 추론 능력을 향상시킨 최초의 모델&lt;/li&gt;
    &lt;li&gt;AIME 2024에서 71% 정확도 달성 (다수결 투표 시 86.7%)&lt;/li&gt;
    &lt;li&gt;자가 검증, 반성, 긴 사고 체인 생성 등의 능력 보유&lt;/li&gt;
  &lt;/ul&gt;

  &lt;ol&gt;
    &lt;li&gt;DeepSeek-R1&lt;/li&gt;
  &lt;/ol&gt;

  &lt;ul&gt;
    &lt;li&gt;Cold-start 데이터와 다단계 훈련 파이프라인 도입&lt;/li&gt;
    &lt;li&gt;OpenAI-o1-1217과 비슷한 수준의 성능 달성&lt;/li&gt;
    &lt;li&gt;수학, 코딩, 과학적 추론 등 다양한 분야에서 우수한 성능&lt;/li&gt;
  &lt;/ul&gt;

  &lt;ol&gt;
    &lt;li&gt;지식 증류(Distillation)&lt;/li&gt;
  &lt;/ol&gt;

  &lt;ul&gt;
    &lt;li&gt;DeepSeek-R1의 추론 능력을 더 작은 모델로 전달&lt;/li&gt;
    &lt;li&gt;1.5B부터 70B까지 다양한 크기의 모델 공개&lt;/li&gt;
    &lt;li&gt;작은 모델들도 기존 오픈소스 모델들보다 우수한 성능 달성&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;훈련 방법:&lt;/p&gt;

  &lt;ol&gt;
    &lt;li&gt;Cold Start: 소량의 고품질 데이터로 초기 훈련&lt;/li&gt;
    &lt;li&gt;추론 중심 강화학습 적용&lt;/li&gt;
    &lt;li&gt;거부 샘플링과 지도학습 미세조정&lt;/li&gt;
    &lt;li&gt;모든 시나리오에 대한 강화학습 수행&lt;/li&gt;
  &lt;/ol&gt;

  &lt;p&gt;한계점:&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;함수 호출, 다중 턴 대화, 복잡한 역할극 등에서는 DeepSeek-V3보다 성능이 낮음&lt;/li&gt;
    &lt;li&gt;중국어와 영어 외 다른 언어에서 언어 혼합 문제 발생&lt;/li&gt;
    &lt;li&gt;프롬프트에 민감하며 few-shot 프롬프팅에서 성능 저하&lt;/li&gt;
    &lt;li&gt;소프트웨어 엔지니어링 태스크에서 제한적인 성능 향상&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;이 연구는 순수 강화학습만으로도 언어 모델의 추론 능력을 크게 향상시킬 수 있음을 보여주었으며, 이는 AI 연구 분야에 중요한 이정표가 되었습니다.&lt;/p&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;문서와의 채팅 답변, 👉 Click&lt;/summary&gt;
  &lt;p&gt;DeepSeek-R1은 대규모 강화학습(RL)을 통해 LLM의 추론 능력을 향상시키는 연구입니다. 이 연구는 두 가지 주요 모델을 소개합니다:&lt;/p&gt;

  &lt;ol&gt;
    &lt;li&gt;
      &lt;p&gt;DeepSeek-R1-Zero: 지도학습 없이 순수 RL만으로 학습된 모델로, 뛰어난 추론 능력을 보여주었지만 가독성이 떨어지고 언어 혼합 등의 문제가 있었습니다.&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;DeepSeek-R1: cold-start 데이터와 다단계 훈련을 도입하여 R1-Zero의 문제를 해결하고 성능을 더욱 향상시켰습니다. OpenAI-o1-1217과 비슷한 수준의 성능을 달성했습니다.&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;

  &lt;p&gt;주요 성과:&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;AIME 2024에서 79.8% Pass@1 달성&lt;/li&gt;
    &lt;li&gt;MATH-500에서 97.3% 달성&lt;/li&gt;
    &lt;li&gt;Codeforces에서 96.3% 상위권 달성&lt;/li&gt;
    &lt;li&gt;MMLU에서 90.8% 달성&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;또한 연구팀은 DeepSeek-R1의 추론 능력을 더 작은 모델로 전달하기 위해 distillation을 수행했으며, 1.5B부터 70B까지 다양한 크기의 모델을 오픈소스로 공개했습니다.&lt;/p&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;Kendra GenAI Index 답변, 👉 Click&lt;/summary&gt;
  &lt;p&gt;DeepSeek-R1은 두 가지 주요 모델을 소개합니다:&lt;/p&gt;

  &lt;ol&gt;
    &lt;li&gt;
      &lt;p&gt;DeepSeek-R1-Zero: 감독학습 없이 대규모 강화학습만을 통해 개발된 모델로, 추론 능력을 자연스럽게 발현시켰습니다. 다만 가독성이 떨어지고 언어 혼합 등의 문제가 있었습니다.&lt;/p&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;DeepSeek-R1: R1-Zero의 문제점을 해결하기 위해 개발된 모델로, 다단계 학습과 사전 데이터를 활용했습니다. OpenAI-o1-1217와 비슷한 수준의 추론 성능을 달성했습니다.&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;

  &lt;p&gt;이 프로젝트의 주요 특징과 기여점은 다음과 같습니다:&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;감독 학습 없이도 강화학습만으로 언어 모델의 추론 능력을 향상시킬 수 있다는 것을 최초로 입증했습니다.&lt;/li&gt;
    &lt;li&gt;추론 패턴 발견과 인간 선호도 조정을 위한 두 단계의 강화학습과, 추론 및 비추론 능력을 위한 두 단계의 감독학습을 포함하는 파이프라인을 도입했습니다.&lt;/li&gt;
    &lt;li&gt;큰 모델의 추론 패턴을 작은 모델로 증류할 수 있음을 보여주었으며, 연구 커뮤니티를 위해 DeepSeek-R1과 관련 API를 공개했습니다.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/details&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Q. DeepSeek R1 Zero는 R1과는 어떤 훈련방식의 차이가 있나요?&lt;/p&gt;

&lt;details&gt;
  &lt;summary&gt;Playground 답변, 👉 Click&lt;/summary&gt;
  &lt;p&gt;DeepSeek-R1-Zero와 DeepSeek-R1의 주요 훈련방식 차이는 다음과 같습니다:&lt;/p&gt;

  &lt;p&gt;DeepSeek-R1-Zero:&lt;/p&gt;

  &lt;ol&gt;
    &lt;li&gt;순수 강화학습만 사용&lt;/li&gt;
  &lt;/ol&gt;

  &lt;ul&gt;
    &lt;li&gt;지도학습(SFT) 없이 바로 강화학습 적용&lt;/li&gt;
    &lt;li&gt;기본 템플릿만 사용하여 추론 과정과 답변을 생성하도록 함&lt;/li&gt;
    &lt;li&gt;규칙 기반 보상 시스템만 사용 (정확도 보상, 형식 보상)&lt;/li&gt;
  &lt;/ul&gt;

  &lt;ol&gt;
    &lt;li&gt;장단점
장점:&lt;/li&gt;
  &lt;/ol&gt;

  &lt;ul&gt;
    &lt;li&gt;자연스러운 추론 능력 발현&lt;/li&gt;
    &lt;li&gt;모델이 자체적으로 진화하는 과정 관찰 가능&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;단점:&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;가독성이 떨어짐&lt;/li&gt;
    &lt;li&gt;언어 혼합 문제 발생&lt;/li&gt;
    &lt;li&gt;출력 형식이 일관적이지 않음&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;DeepSeek-R1:&lt;/p&gt;

  &lt;ol&gt;
    &lt;li&gt;다단계 훈련 파이프라인 사용&lt;/li&gt;
  &lt;/ol&gt;

  &lt;ul&gt;
    &lt;li&gt;Cold Start: 수천 개의 고품질 Chain-of-Thought 데이터로 초기 미세조정&lt;/li&gt;
    &lt;li&gt;추론 중심 강화학습 적용&lt;/li&gt;
    &lt;li&gt;거부 샘플링과 지도학습 미세조정 수행&lt;/li&gt;
    &lt;li&gt;모든 시나리오에 대한 강화학습 진행&lt;/li&gt;
  &lt;/ul&gt;

  &lt;ol&gt;
    &lt;li&gt;장점:&lt;/li&gt;
  &lt;/ol&gt;

  &lt;ul&gt;
    &lt;li&gt;더 나은 가독성&lt;/li&gt;
    &lt;li&gt;일관된 언어 사용&lt;/li&gt;
    &lt;li&gt;더 넓은 범위의 태스크 수행 가능&lt;/li&gt;
    &lt;li&gt;전반적으로 더 높은 성능&lt;/li&gt;
  &lt;/ul&gt;

  &lt;p&gt;핵심 차이:&lt;/p&gt;

  &lt;ol&gt;
    &lt;li&gt;초기 훈련 데이터&lt;/li&gt;
  &lt;/ol&gt;

  &lt;ul&gt;
    &lt;li&gt;R1-Zero: 완전히 백지 상태에서 시작&lt;/li&gt;
    &lt;li&gt;R1: Cold Start 데이터로 초기 지식 주입&lt;/li&gt;
  &lt;/ul&gt;

  &lt;ol&gt;
    &lt;li&gt;훈련 단계&lt;/li&gt;
  &lt;/ol&gt;

  &lt;ul&gt;
    &lt;li&gt;R1-Zero: 단일 강화학습 단계&lt;/li&gt;
    &lt;li&gt;R1: 여러 단계의 훈련 과정 (Cold Start → RL → SFT → RL)&lt;/li&gt;
  &lt;/ul&gt;

  &lt;ol&gt;
    &lt;li&gt;최적화 목표&lt;/li&gt;
  &lt;/ol&gt;

  &lt;ul&gt;
    &lt;li&gt;R1-Zero: 순수 추론 능력 향상에 집중&lt;/li&gt;
    &lt;li&gt;R1: 추론 능력과 함께 실용성, 가독성, 다목적성 고려&lt;/li&gt;
  &lt;/ul&gt;
&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;문서와의 채팅 답변, 👉 Click&lt;/summary&gt;
  &lt;p&gt;DeepSeek R1-Zero는 감독학습(supervised fine-tuning) 없이 순수하게 강화학습만을 사용하여 훈련된 모델입니다. 반면 DeepSeek R1은 다단계 훈련 파이프라인을 사용합니다:&lt;/p&gt;

  &lt;ol&gt;
    &lt;li&gt;Cold Start: 수천 개의 Chain-of-Thought 예제로 초기 파인튜닝&lt;/li&gt;
    &lt;li&gt;추론 중심의 강화학습 진행&lt;/li&gt;
    &lt;li&gt;강화학습 체크포인트에서 rejection sampling을 통한 새로운 SFT 데이터 생성&lt;/li&gt;
    &lt;li&gt;모든 시나리오를 고려한 추가 강화학습 진행&lt;/li&gt;
  &lt;/ol&gt;

  &lt;p&gt;R1-Zero는 순수 강화학습만으로도 뛰어난 추론 능력을 보여주었지만, 가독성이 떨어지고 언어가 혼합되는 등의 문제가 있었습니다. R1은 이러한 문제들을 해결하고 더 나은 성능을 달성하기 위해 cold-start 데이터와 다단계 훈련을 도입했습니다.&lt;/p&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;Kendra GenAI Index 답변, 👉 Click&lt;/summary&gt;
  &lt;p&gt;DeepSeek-R1-Zero는 감독학습(SFT) 없이 순수하게 강화학습(RL)만을 사용하여 훈련된 모델입니다. 기본 모델에 직접 RL을 적용하는 방식을 사용했습니다.&lt;/p&gt;

  &lt;p&gt;반면 DeepSeek-R1은 수천 개의 Chain-of-Thought(CoT) 예제로 사전 미세조정된 체크포인트에서 시작하여 RL을 적용했습니다. 즉, 콜드 스타트 데이터와 반복적인 RL 미세조정을 함께 활용하는 방식으로, 이는 R1-Zero보다 더 강력한 성능을 보여줍니다.&lt;/p&gt;

&lt;/details&gt;

&lt;h3 id=&quot;토큰-소모량-비교&quot;&gt;토큰 소모량 비교&lt;/h3&gt;

&lt;p&gt;Q. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning에 대하여 요약해 알려주세요.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;토큰 소모량&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Playground&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;문서와의 채팅&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Kendra GenAI Index&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Input&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;20,192&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;22,114&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1,904&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Output&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;716&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;553&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;576&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Q. DeepSeek R1 Zero는 R1과는 어떤 훈련방식의 차이가 있나요?&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;토큰 소모량&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Playground&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;문서와의 채팅&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Kendra GenAI Index&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Input&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;20,945&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;22,107&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3,062(1,091+1,971)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Output&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;660&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;434&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;362(36+326)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;세 가지 방식 모두 우수한 품질의 답변을 제공했습니다. Playground와 ‘문서와의 채팅’ 기능의 경우, 전체 문서를 참고하여 답변을 생성하므로 긴 문맥(long context)을 지원하는 모델에서 정확한 답변을 얻을 수 있다는 점이 매력적입니다.
&lt;em&gt;(문서를 기반으로 신속한 테스트를 수행하고자 할 때, ‘문서와의 채팅’ 기능보다는 Playground를 활용하는 것이 더 상세한 답변을 얻을 수 있는 것으로 보입니다.)&lt;/em&gt;
Kendra의 경우, 더 많은 분석이 필요하겠지만, 관리형 RAG를 통해 필요한 정보를 효과적으로 찾아내어 답변을 생성한다는 점이 매력적으로 느껴집니다.&lt;/p&gt;

&lt;p&gt;이번 포스팅에서 다룬 논문 리뷰와는 달리, 수백 개의 Zendesk 티켓에 대한 질의응답이나 S3에 산재된 데이터에 대한 질의응답 같은 작업에는 Kendra GenAI Index가 특히 유용할 것으로 예상됩니다.&lt;/p&gt;

&lt;h2 id=&quot;-마치며&quot;&gt;&lt;a href=&quot;#outro&quot;&gt;🌟 마치며&lt;/a&gt;&lt;a id=&quot;outro&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;2023년 6월, &lt;a href=&quot;https://heuristicwave.github.io/Kendra&quot;&gt;flan-t5 모델과 Amazon Kendra(엔터프라이즈 검색 엔진)를 활용한 RAG 관련 글&lt;/a&gt;을 작성한 지 2년이 지나 다시 Kendra에 대해 글을 쓰게 되니 감회가 새롭습니다.
이번 포스팅을 준비하며 과거 작성했던 글을 다시 보니, 부족했던 제 모습이 부끄러우면서도 그 사이 놀랍게 발전한 AI 생태계에 새삼 감탄하게 됩니다.&lt;/p&gt;

&lt;p&gt;이번에 Kendra GenAI Index를 테스트해보며, 테이블 데이터에 대한 정확한 이해는 아직 부족하지만, 향후 이 부분까지 지원된다면 정말 매력적인 관계형 RAG 서비스가 될 것 같습니다.&lt;/p&gt;

&lt;p&gt;글을 작성하다 보니 내용이 방대해져, 1편과 2편으로 나눌지 고민이 많았습니다. 이번 기회에 Kendra를 다시 테스트하며, ‘지식 기반의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stopSequences&lt;/code&gt;에 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\nObservation&lt;/code&gt;을 지정한 이유’나 ‘Data source 설정 시, Default Language의 제약 사항’ 등에 대해 다루지 못한 부분이 있습니다. 이러한 내용들은 다음 기회에 다루도록 하고, 이번 글은 여기서 마무리하겠습니다.&lt;/p&gt;

&lt;p&gt;소중한 시간을 내어 긴 글 읽어주셔서 감사합니다! 잘못된 내용은 지적해주세요! 😃&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;</content>

      
      
      
      
      

      <author>
          <name>Jihun Lim</name>
        
        
      </author>

      

      
        <category term="aws" />
      
        <category term="genai" />
      

      
        <summary type="html">Amazon Bedrock Knowledge Bases로 알아보는 Amazon Kendra GenAI Index</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">OpenAI-Compatible RESTful APIs</title>
      <link href="https://heuristicwave.github.io/BedrockOpenAI" rel="alternate" type="text/html" title="OpenAI-Compatible RESTful APIs" />
      <published>2025-01-18T00:00:00+00:00</published>
      <updated>2025-01-18T00:00:00+00:00</updated>
      <id>https://heuristicwave.github.io/%08BedrockOpenAI</id>
      <content type="html" xml:base="https://heuristicwave.github.io/BedrockOpenAI">&lt;p&gt;Amazon Bedrock Access Gateway로 OpenAI 호환 API 구현하기&lt;/p&gt;

&lt;h2 id=&quot;-openai-provider&quot;&gt;&lt;a href=&quot;#openai&quot;&gt;🤖 OpenAI Provider&lt;/a&gt;&lt;a id=&quot;openai&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;2024년 12월 19일 발행된 &lt;a href=&quot;https://blog.langchain.dev/langchain-state-of-ai-2024/&quot;&gt;LangChain State of AI 2024 Report&lt;/a&gt;에 따르면,
OpenAI Provider는 LangSmith 사용자들 사이에서 가장 많이 사용되는 LLM 공급업체 입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://blog.langchain.dev/content/images/size/w1600/2024/12/Top-10-LLM-Providers-bar-chart.png&quot; alt=&quot;Top 10 LLM Providers 2024&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 데이터는 LangSmith 사용자들의 사용량만을 기반으로 하고 있어, 전체 LLM 시장의 실제 점유율과는 차이가 있습니다.
&lt;em&gt;(당장 저만 해도 LangChain을 통해 모델을 호출하지 않고 각 CSP가 권장하는 호출 방법으로 사용하고 있습니다. 😆)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;그러나, OpenAI는 59.2%의 점유율로 2위인 Ollama(8.5%)보다 약 7배 높은 사용률을 보여주며 LLM 시장에서 OpenAI가 압도적인 지배력을 가지고 있음을 보여줍니다.&lt;/p&gt;

&lt;p&gt;각 Provider가 공식적으로 발표한 사용량 데이터를 기반으로 직접적인 비교자료는 없지만, OpenAI가 시장에서 가지는 영향력이 매우 크기 때문에 Google(Gemini), xAI(Grok) 등의 Provider들은 OpenAI와 호환되는 라이브러리를 출시하고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://developers.googleblog.com/en/gemini-is-now-accessible-from-the-openai-library/&quot;&gt;Gemini is now accessible from the OpenAI Library&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;openai&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OpenAI&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;client&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OpenAI&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;api_key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;gemini_api_key&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;base_url&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;https://generativelanguage.googleapis.com/v1beta/&quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;completions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;gemini-1.5-flash&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;messages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;role&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;system&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;content&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;You are a helpful assistant.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;&quot;role&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;user&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;&quot;content&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Explain to me how AI works&quot;&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://docs.x.ai/docs/overview#migrating-from-another-llm-provider&quot;&gt;xAI, Migrating from another LLM provider?&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Grok의 경우, OpenAI 뿐만 아니라 Anthropic SDK와도 호환성을 제공합니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;openai&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OpenAI&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;client&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OpenAI&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;api_key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;XAI_API_KEY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base_url&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;https://api.x.ai/v1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;anthropic&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Anthropic&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;client&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Anthropic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;api_key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;XAI_API_KEY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base_url&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;https://api.x.ai&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;️-openai-호환-bedrock-사용하기&quot;&gt;&lt;a href=&quot;#bedrock&quot;&gt;🖇️ OpenAI 호환 Bedrock 사용하기&lt;/a&gt;&lt;a id=&quot;bedrock&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;AWS의 공식 서비스는 아니지만, AWS의 Application Load Balancer와 Lambda 등을 활용해 Amazon Bedrock의 다양한 AI 모델들을 OpenAI API 형식으로 손쉽게 사용할 수 있게 해주는 ‘&lt;a href=&quot;https://github.com/aws-samples/bedrock-access-gateway&quot;&gt;Bedrock Access Gateway&lt;/a&gt;‘에 대해 알아보겠습니다.&lt;/p&gt;

&lt;h3 id=&quot;아키텍처&quot;&gt;아키텍처&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/aws-samples/bedrock-access-gateway/raw/main/assets/arch.png&quot; alt=&quot;아키텍처&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Bedrock Access Gateway는 API Key 생성을 위한 Parameter Store, 스트리밍 응답과 API URL을 제공하는 Application Load Balancer(ALB), 그리고 Bedrock API를 OpenAI 형식으로 변환해주는 Proxy 서버 역할의 Lambda로 구성되어 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;지원-기능&quot;&gt;지원 기능&lt;/h3&gt;

&lt;p&gt;이 서비스는 Bedrock의 Guardrails와 같은 고급(편의) 기능은 지원하지 않지만, 다음과 같은 기본적인 기능들을 제공합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;스트리밍 응답&lt;/li&gt;
  &lt;li&gt;Model, Chat Completion API&lt;/li&gt;
  &lt;li&gt;Tool 호출&lt;/li&gt;
  &lt;li&gt;Embedding/Multimodal API&lt;/li&gt;
  &lt;li&gt;교차 리전 호출&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;구축하기&quot;&gt;구축하기&lt;/h3&gt;

&lt;h4 id=&quot;1-api-키-설정&quot;&gt;1. API 키 설정&lt;/h4&gt;

&lt;p&gt;Parameter Store를 사용하는 방법과 컨테이너에 직접 환경 변수를 설정하는 2가지 방법이 있지만, 이번 포스팅에서는 Parameter Store를 사용하겠습니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;AWS Secrets Manager &amp;gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Store a new secret&lt;/code&gt; 버튼 클릭&lt;/li&gt;
  &lt;li&gt;‘Other type of secret’을 누르고 차례대로 &lt;strong&gt;Secrets&lt;/strong&gt; 생성&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;2-cloudformation-스택-배포&quot;&gt;2. CloudFormation 스택 배포&lt;/h4&gt;

&lt;p&gt;AWS 콘솔에서 원하는 리전과 CloudFormation 템플릿을 선택합니다. 이번 포스팅에서는 서울리전과 ALB + Fargate 조합을 선택했습니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;제공된 &lt;a href=&quot;https://github.com/aws-samples/bedrock-access-gateway/tree/main?tab=readme-ov-file#store-api-key-in-env-variable&quot;&gt;Github&lt;/a&gt;에서 CloudFormation 템플릿 선택
    &lt;ul&gt;
      &lt;li&gt;ALB + Lambda&lt;/li&gt;
      &lt;li&gt;ALB + Fargate&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;스택 세부정보 입력
    &lt;ul&gt;
      &lt;li&gt;1단계에서 API 키를 설정한 경우, API 키를 저장하는데 사용한 Secret ARN을 입력 (예: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;arn:aws:secretsmanager&lt;/code&gt; 으로 시작함.)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;IAM 리소스 생성 권한 확인
    &lt;ul&gt;
      &lt;li&gt;CloudFormation이 필요한 IAM 리소스를 자동으로 생성할 수 있도록, 3단계 하단의 권한 부여 체크박스를 선택합니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;스택 생성 약 10분 내외 소요&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;사용하기&quot;&gt;사용하기&lt;/h3&gt;

&lt;p&gt;배포 후, 다음 환경 변수를 설정합니다.&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;OPENAI_API_KEY&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&amp;lt;API key&amp;gt;
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;OPENAI_BASE_URL&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&amp;lt;API base url&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;사용-가능한-모델-확인&quot;&gt;사용 가능한 모델 확인&lt;/h4&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl &lt;span class=&quot;nt&quot;&gt;-s&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$OPENAI_BASE_URL&lt;/span&gt;/models &lt;span class=&quot;nt&quot;&gt;-H&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Authorization: Bearer &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$OPENAI_API_KEY&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; | jq .data
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;sdk에서-사용하기&quot;&gt;SDK에서 사용하기&lt;/h4&gt;

&lt;p&gt;이제 다음 세 가지 정보만 있으면 OpenAI SDK 형식으로 모델을 사용할 수 있습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;API Key (Parameter Store 생성 시 설정한 값)&lt;/li&gt;
  &lt;li&gt;Base URL (CloudFormation 출력 값)&lt;/li&gt;
  &lt;li&gt;Bedrock에서 사용할 Model ID&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;openai&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OpenAI&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;client&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;OpenAI&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;api_key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;{API Key}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;base_url&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;{API Base URL}&quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;client&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;completions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;anthropic.claude-3-5-sonnet-20240620-v1:0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;messages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;role&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;system&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;content&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;You are a helpful assistant.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;&quot;role&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;user&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;&quot;content&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;너는 무슨 모델이니?&quot;&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 저는 Anthropic에서 개발한 AI 어시스턴트인 Claude입니다. 정확한 모델 버전은 잘 모르겠습니다.
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;api-사용하기&quot;&gt;API 사용하기&lt;/h4&gt;

&lt;p&gt;다음과 같이 터미널에서 직접 API를 호출하여 사용할 수 도 있습니다.&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl &lt;span class=&quot;nv&quot;&gt;$OPENAI_BASE_URL&lt;/span&gt;/chat/completions &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;-H&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Content-Type: application/json&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;-H&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Authorization: Bearer &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$OPENAI_API_KEY&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;{
    &quot;model&quot;: &quot;anthropic.claude-3-5-sonnet-20240620-v1:0&quot;,
    &quot;messages&quot;: [
      {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: &quot;너는 무슨 모델이니?&quot;
      }
    ]
  }&apos;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;{“id”:”chatcmpl-39fd0f67”,”created”:1737187206,”model”:”anthropic.claude-3-5-sonnet-20240620-v1:0”,”system_fingerprint”:”fp”,”choices”:[{“index”:0,”finish_reason”:”stop”,”logprobs”:null,”message”:{“role”:”assistant”,”content”:”저는 Anthropic에서 개발한 AI 어시스턴트인 Claude입니다. 정확한 모델 버전은 잘 모르겠습니다.”}}],”object”:”chat.completion”,”usage”:{“prompt_tokens”:20,”completion_tokens”:55,”total_tokens”:75}}&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;CloudWatch에서 확인한 Bedrock API 호출 정보를 보면(👇), ConverseAPI의 사용량 정보가 OpenAI의 usage 형식(☝️)과 동일한 형태로 호환되어 출력된 것을 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/post/ai/Bedrock/bedrock_cw.png&quot; alt=&quot;bedrock_cw&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;🚨 광고 : Converse API 및 모델 사용량 모니터링은 “Amazon Bedrock으로 시작하는 실전 생성형 AI 개발”를 참고하세요! &lt;br /&gt;
판매 링크 : &lt;a href=&quot;https://www.yes24.com/Product/Goods/140049541&quot;&gt;📘 예스24&lt;/a&gt;, &lt;a href=&quot;https://product.kyobobook.co.kr/detail/S000214962344&quot;&gt;📗 교보문고&lt;/a&gt;, &lt;a href=&quot;https://www.aladin.co.kr/shop/wproduct.aspx?ItemId=353662061&amp;amp;start=slayer&quot;&gt;📕 알라딘&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;-고려-사항&quot;&gt;&lt;a href=&quot;#notes&quot;&gt;✋ 고려 사항&lt;/a&gt;&lt;a id=&quot;notes&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;성능: 프록시 구조로 인한 약간의 지연시간이 발생할 수 있습니다.&lt;/li&gt;
  &lt;li&gt;업그레이드: 최신 기능을 사용하려면 최신 이미지(프록시 서버)로 업데이트가 필요합니다.&lt;/li&gt;
  &lt;li&gt;비용: ALB와 같은 AWS 리소스를 활용하므로 약간의 비용이 부과됩니다.
    &lt;ul&gt;
      &lt;li&gt;Lambda 웹 어댑터 + Lambda URL(&lt;a href=&quot;https://github.com/awslabs/aws-lambda-web-adapter/tree/main/examples/fastapi-response-streaming&quot;&gt;예시&lt;/a&gt;)을 사용하여 ALB를 대체 할 수 있습니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;-마치며&quot;&gt;&lt;a href=&quot;#outro&quot;&gt;🌟 마치며&lt;/a&gt;&lt;a id=&quot;outro&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Bedrock Access Gateway는 OpenAI API 형식을 사용하기 때문에 Guardrails, Agents 등 Bedrock 고유의 기능을 완전히 활용할 수는 없습니다. 하지만 이 도구를 통해 기존 OpenAI API 기반 애플리케이션을 Amazon Bedrock으로 쉽게 마이그레이션하거나 테스트할 수 있으며, 다양한 AI 모델들을 일관된 인터페이스로 사용할 수 있다는 장점이 있습니다.&lt;/p&gt;

&lt;p&gt;특히 기존에는 매력적인 오픈소스 도구들이 OpenAI만 지원하여 Bedrock 모델 활용이 제한적이었지만, 이제는 Bedrock Access Gateway를 통해 이러한 도구들을 Bedrock 모델들과 함께 간편하고 빠르게 테스트해볼 수 있게 되었습니다.&lt;/p&gt;

&lt;p&gt;더불어 GitHub 레포지토리의 코드를 활용하면 로컬 환경에서도 실행 가능한 나만의 게이트웨이를 구축할 수 있습니다. 이처럼 Bedrock Access Gateway는 Amazon Bedrock의 활용성을 한층 더 높여주는 유용한 도구가 될 것으로 기대됩니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;</content>

      
      
      
      
      

      <author>
          <name>Jihun Lim</name>
        
        
      </author>

      

      
        <category term="aws" />
      
        <category term="genai" />
      

      
        <summary type="html">Amazon Bedrock Access Gateway로 OpenAI 호환 API 구현하기</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">출판 후기 - 첫 책을 출간하면서 놓쳤던 것들</title>
      <link href="https://heuristicwave.github.io/reflection" rel="alternate" type="text/html" title="출판 후기 - 첫 책을 출간하면서 놓쳤던 것들" />
      <published>2025-01-05T00:00:00+00:00</published>
      <updated>2025-01-05T00:00:00+00:00</updated>
      <id>https://heuristicwave.github.io/reflection</id>
      <content type="html" xml:base="https://heuristicwave.github.io/reflection">&lt;p&gt;출판 후기 - 첫 책을 출간하면서 놓쳤던 것들&lt;/p&gt;

&lt;h2 id=&quot;-amazon-bedrock으로-시작하는-실전-생성형-ai-개발&quot;&gt;&lt;a href=&quot;#book&quot;&gt;📖 Amazon Bedrock으로 시작하는 실전 생성형 AI 개발&lt;/a&gt;&lt;a id=&quot;book&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;얼마 전, 제 🧑🏻‍🏫멘토이자 🗽뮤즈인 동기 &lt;a href=&quot;https://medium.com/@nuatmochoi/&quot;&gt;성우&lt;/a&gt;와 함께 2024년 하반기 동안 준비했던 첫 책을 출간했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/background/bedrock.jpg&quot; alt=&quot;bedrock&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;판매 링크 : &lt;a href=&quot;https://www.yes24.com/Product/Goods/140049541&quot;&gt;📘 예스24&lt;/a&gt;, &lt;a href=&quot;https://product.kyobobook.co.kr/detail/S000214962344&quot;&gt;📗 교보문고&lt;/a&gt;, &lt;a href=&quot;https://www.aladin.co.kr/shop/wproduct.aspx?ItemId=353662061&amp;amp;start=slayer&quot;&gt;📕 알라딘&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;출판사 측에서 집필 과정동안 가이드라인과 점검 사항을 제공해주었지만, 그럼에도 여러 시행착오를 겪었습니다.&lt;/p&gt;

&lt;p&gt;이번 포스팅에서는 앞으로 책을 쓰고자 하는 분들과, 추후 다시 한 번 글을 쓰게 될 미래의 제가 참고할 만한 내용들을 정리해보고자 합니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;-협업-과정&quot;&gt;&lt;a href=&quot;#collabo&quot;&gt;👫 협업 과정&lt;/a&gt;&lt;a id=&quot;collabo&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;공동 작업의 효율을 높이기 위해 Google Docs와 Sheets를 적극 활용했습니다. Chapter-Section-Unit 구조로 콘텐츠를 체계화하고, 관련 문서들을 링크로 연결한 집필 대시보드를 구축했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/post/etc/sheets.png&quot; alt=&quot;sheets&quot; /&gt;&lt;/p&gt;

&lt;p&gt;스프레드시트에 담당자와 마감일을 지정하고, 드롭다운 기능을 활용해 진행 상태를 관리했습니다. 새로운 주제가 추가되면 ‘Open’, 작성 중일 때는 ‘In Progress’, 초안이 완성되면 ‘Ready for Review’, 모든 검토와 수정이 끝나면 ‘Done’ 상태로 표시했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/post/etc/docs.png&quot; alt=&quot;docs&quot; /&gt;&lt;/p&gt;

&lt;p&gt;‘Ready for Review’ 상태의 글은 다른 저자가 교차 검증을 진행했으며, Google Docs의 ‘제안’ 기능을 통해 의견을 교환했습니다. 여러 차례의 피드백과 수정 과정을 거쳐 ‘Done’ 상태가 되면 전체 진행률이 올라갔는데, 마치 게임을 하듯 진행률을 높이려 노력하다 보니 글쓰기에 소소한 재미도 더해졌습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;-유용한-팁&quot;&gt;&lt;a href=&quot;#tips&quot;&gt;🌟 유용한 팁&lt;/a&gt;&lt;a id=&quot;tips&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&quot;개인정보-가리기&quot;&gt;개인정보 가리기&lt;/h3&gt;

&lt;p&gt;AWS 실습 화면을 캡처할 때 계정 정보나 리소스 정보 등 민감한 정보를 가려야 하는 경우가 많습니다. 이런 정보를 처리하는 방법으로는 블러 처리나 다른 이미지로 덮어씌우는 방법이 있지만, 개발자 도구를 활용한 HTML 수정 방법을 특히 추천드립니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/post/etc/aws_console.png&quot; alt=&quot;console&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ARN의 계정 정보나 User Name과 같은 값들을 HTML 태그에서 직접 수정하면 원래 UI와 동일한 모습을 유지할 수 있어 이질감이 없고, 원하는 변수명이나 리소스명으로 자유롭게 변경할 수 있습니다. 또한 텍스트가 선명하게 유지되어 가독성이 좋으며, 추후 수정이 필요할 때도 동일한 방식으로 쉽게 대응할 수 있어 더 나은 참고 자료를 만드는데 도움이 됩니다.&lt;/p&gt;

&lt;h3 id=&quot;해상도-관련-팁&quot;&gt;해상도 관련 팁&lt;/h3&gt;

&lt;p&gt;책의 1차 편집본을 검토하면서 동일한 방법(맥북 기본 캡처)으로 화면을 캡처했음에도 일부 이미지의 해상도가 달라 식별이 어려운 문제를 발견했습니다. 원인을 파악해보니 캡처를 진행한 모니터의 해상도에 따라 결과물이 다르게 나온다는 것을 알게 되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/post/etc/resolution.png&quot; alt=&quot;resolution&quot; /&gt;&lt;/p&gt;

&lt;p&gt;맥북이나 4K 이상의 고해상도 모니터에서 캡처할 경우 기본 해상도가 144로 설정되어 선명한 화질을 유지하지만, 다른 모니터에서는 72 해상도로 캡처되어 화질이 떨어지는 현상이 발생했습니다. 따라서 책이나 문서 작성 시 스크린샷 캡처가 필요하다면, 반드시 동일한 고해상도 환경에서 일관성 있게 작업하는 것이 중요합니다.&lt;/p&gt;

&lt;h3 id=&quot;퇴고-시간을-확보하자&quot;&gt;퇴고 시간을 확보하자!&lt;/h3&gt;

&lt;p&gt;저자 검토에, 교차 검토, 출판사 검토까지 했는데도 퇴고 시간을 확보해야 한다고요?&lt;/p&gt;

&lt;p&gt;네, 그렇습니다. 3번의 검토 과정을 거쳤음에도 원고의 완성도를 높이기 위한 퇴고 작업은 매우 중요했습니다. 검토 당시에는 발견하지 못했던 문제점들이 시간이 지나면서 드러났고, 더 나은 설명 방식과 전개 방법이 떠올라 계속해서 수정 작업이 이어졌습니다.&lt;/p&gt;

&lt;p&gt;돌이켜보니 초안 작성만큼이나 퇴고에도 상당한 시간이 필요했는데, 이를 처음부터 충분히 고려하지 못해 출판 마무리 단계에서 많은 어려움을 겪었습니다. 이러한 경험을 통해 다음 출판을 계획할 때는 초안 작성 시간과 동일한 수준으로 퇴고 시간을 확보해야 한다는 것을 깨달았습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;-실수들&quot;&gt;&lt;a href=&quot;#mistakes&quot;&gt;😱 실수들&lt;/a&gt;&lt;a id=&quot;mistakes&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&quot;그림-표-코드-등-넘버링&quot;&gt;그림, 표, 코드 등 넘버링&lt;/h3&gt;

&lt;p&gt;책에 실습 과정을 담으면서 많은 그림, 표, 코드 등이 포함되었습니다. 이들을 구분하기 위해 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;챕터명-번호&lt;/code&gt; 형식(예: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;4-26&lt;/code&gt;)으로 넘버링했습니다. 그러나 이 방식으로는 내용을 중간에 삭제하거나 번호를 추가하는 등의 수정 작업이 번거로웠습니다.&lt;/p&gt;

&lt;p&gt;더 나은 방식으로는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;챕터명-섹션명-번호&lt;/code&gt; 형식(예: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;4-2-3&lt;/code&gt;)을 사용하는 것이 좋았을 것 같습니다. 이렇게 하면 섹션 단위로 독립적인 넘버링이 가능하고, 콘텐츠의 위치를 더 정확히 파악할 수 있으며, 내용 수정이나 추가 시에도 해당 섹션만 관리하면 됩니다. 이렇게 하면 독립적인 넘버링이 가능해 수정이 유리할뿐더러, 콘텐츠의 위치를 더 정확히 파악할 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;익숙함에-속아-실수들을-놓치지-말자&quot;&gt;익숙함에 속아 실수들을 놓치지 말자&lt;/h3&gt;

&lt;p&gt;최종 인쇄에 들어가기 전까지는 수십 번의 검토 과정을 거치게 됩니다. 그러나 아무리 꼼꼼하게 확인하더라도, 반복적인 검토 과정을 통해 내용이 익숙해지면서 실수를 발견하기가 더욱 어려워집니다. 글이 익숙해질수록 우리의 뇌는 오타나 문법적 오류를 자동으로 보정하여 읽기 때문입니다. 익숙함이 주는 편안함에 속아 놓치는 실수가 없도록, 마지막 순간까지 긴장의 끈을 놓지 말아야 합니다.&lt;/p&gt;

&lt;p&gt;물론 체크리스트를 만들어 검토하는 것도 좋은 방법이지만, 이 역시 여러 번의 퇴고 과정을 거치다 보면 점검 항목들이 익숙해져 느슨한 검토로 이어질 수 있습니다. &lt;strong&gt;항상 자기 자신을 경계하세요.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;️-마치며&quot;&gt;&lt;a href=&quot;#outro&quot;&gt;✍️ 마치며&lt;/a&gt;&lt;a id=&quot;outro&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;위에 언급한 내용들 이외에도 더 많은 시행착오들을 거친 것 같은데, 가장 기억에 남는 것들을 중심으로 정리해 보았습니다. 그중에서도 &lt;strong&gt;가장 중요한 하나를 강조한다면, 충분한 퇴고 시간을 확보&lt;/strong&gt;하는 것입니다. 시간적 여유를 가지고 글을 다듬어 간다면 앞서 소개했거나 미처 다루지 못한 부분들에 대해서도 더 깊이 있는 내용을 전달할 수 있었을 것 같습니다. 이 글을 통해 미래의 작가님들에게 작은 도움이 되었기를 바랍니다.&lt;/p&gt;

&lt;p&gt;2024년 4월 13일 이후로 제대로 된 기술 포스팅을 하지 못하고 있습니다. 5개월이라는 긴 시간 동안 책 집필에 모든 에너지를 쏟다 보니, 블로그 글쓰기까지는 여력이 없었다는 핑계를 대어봅니다. 책을 출간하고 &lt;strong&gt;정식 작가로 데뷔하면 글쓰기가 한결 수월해질 것이라 기대했습니다.&lt;/strong&gt; 하지만 현실은 그렇지 않았고, &lt;strong&gt;여전히 글쓰기는 어렵습니다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;특히 백지 상태에서 첫 문장을 써내려가는 과정은 가장 힘든 순간입니다. 답답함과 무기력함이 밀려오고, 첫 문장을 완성하기까지 많은 고뇌가 따릅니다. 그러나 일단 기본적인 뼈대가 잡히고 나면, 그 다음부터는 글이 물 흐르듯 자연스럽게 이어지곤 합니다. 그렇기에 습작이든, 몇 줄이라도 끄적거리든, 꾸준히 글을 써나가는 것이 중요합니다.&lt;/p&gt;

&lt;p&gt;끝없는 수정과 교정 과정을 거치면서 ‘다시는 책을 쓰지 말아야지’라는 생각이 들기도 했습니다. 하지만 시간이 지나고 책이 완성되어 가는 모습을 보니, 이상하게도 다시 한번 도전하고 싶은 마음이 듭니다. 이제는 더 나은 후속작이나 새로운 주제로 또 다른 책을 써보고 싶다는 욕심이 생깁니다. 이것이 바로 글쓰기의 묘미이자 매력이 아닐까 생각합니다.&lt;/p&gt;

&lt;p&gt;고통끝에 얻은 결실이 주는 기쁨, 그리고 그 기쁨이 새로운 도전으로 이어지는 이 순환이야말로 진정한 작가의 길인 것 같습니다. &lt;strong&gt;다만 한 가지 잊지 말아야 할 것은, 책 집필에 몰두하다 보면 소중한 가족과 친구들에게 소홀해질 수 있다는 점&lt;/strong&gt;입니다. 균형 잡힌 시간 관리로 작가로서의 열정과 일상의 소중함을 모두 지켜나가는 지혜가 필요할 것 같습니다.&lt;/p&gt;

&lt;hr /&gt;</content>

      
      
      
      
      

      <author>
          <name>Jihun Lim</name>
        
        
      </author>

      

      
        <category term="uncategorized" />
      
        <category term="aws" />
      
        <category term="genai" />
      
        <category term="event" />
      

      
        <summary type="html">출판 후기 - 첫 책을 출간하면서 놓쳤던 것들</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">AWS APJC PLES GameDay 일기</title>
      <link href="https://heuristicwave.github.io/gameday" rel="alternate" type="text/html" title="AWS APJC PLES GameDay 일기" />
      <published>2024-11-30T00:00:00+00:00</published>
      <updated>2024-11-30T00:00:00+00:00</updated>
      <id>https://heuristicwave.github.io/gameday</id>
      <content type="html" xml:base="https://heuristicwave.github.io/gameday">&lt;p&gt;4년 간의 기다림, AWS APJC PLES GameDay 우승 후기&lt;/p&gt;

&lt;h1 id=&quot;aws-gameday-&quot;&gt;&lt;a href=&quot;#AWS-GameDay&quot;&gt;AWS GameDay 🦄&lt;/a&gt;&lt;a id=&quot;AWS-GameDay&quot;&gt;&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://aws.amazon.com/ko/gameday/&quot;&gt;GameDay&lt;/a&gt;는 AWS가 제공하는 실습 중심의 팀 기반 학습 이벤트입니다. 참가자들은 가상의 시나리오 속에서 실제 AWS 환경을 직접 다루며 문제를 해결하게 됩니다.
보안 분야에 익숙한 분들에게는 CTF(Capture The Flag)와 유사한 형태로, 주어진 미션을 해결하며 점수를 획득하는 방식으로 진행됩니다.
팀원들과 협력하여 클라우드 인프라 구축, 장애 대응, 보안 문제 해결 등 실제 상황과 유사한 다양한 과제를 수행하면서 AWS 서비스에 대한 실전 경험을 쌓을 수 있습니다.&lt;/p&gt;

&lt;p&gt;일반적인 GameDay는 누구나 참여할 수 있는 공개 행사로 진행되며, PLES GameDay는 AWS 파트너사 직원들을 대상으로 특별히 진행되는 대회입니다.
AWS는 파트너사를 대상으로 매년 정기적으로 GameDay를 개최하는데, 그 중 아시아 태평양 지역(APJC) 파트너사들이 참여하는 대규모 GameDay는 연 1회 진행됩니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;apjc-ples-gameday-지난-기록들&quot;&gt;&lt;a href=&quot;#history&quot;&gt;APJC PLES GameDay 지난 기록들&lt;/a&gt;&lt;a id=&quot;history&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;사실 저는 2020년에 입사한 후, 2021년부터 매년 대회에 출전해왔어요.
처음 참가했던 대회에는 22개 팀이 참여했는데, 저희 팀은 순위권과는 거리가 멀었습니다.
그때 느낀 좌절감을 사내 Wiki에 회고글로 남기기도 했어요.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/post/etc/review.png&quot; alt=&quot;후기&quot; /&gt;&lt;/p&gt;

&lt;p&gt;2022년 대회는 Security를 주제로 6개 리전에서 23개 팀이 참가했습니다. 작년보다 진화된 실력?으로 동기와 후배님들과 함께 팀을 이뤄 도전했지만…&lt;/p&gt;

&lt;p&gt;네.. 또 다음을 기약했습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;흥민이 형 죄송합니다. 흥민이 형의 아쉬움과 비교할 수는 없겠지만, 당시 너무 마음이 아팠어요.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/post/etc/review2.png&quot; alt=&quot;후기2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;비록 제가 속한 팀은 아니었지만, 동료들이 한국 리전에서 1등을 차지해서 정말 기뻤습니다. (저는 12등 😭) &lt;br /&gt;
하지만 한국 1등이 APJC에서는 7등에 그쳤다는 점에서, 아직 세계와의 벽은… 여전히…&lt;/p&gt;

&lt;p&gt;2023년 대회는 Sustainability를 주제로 진행되었으며, 한국을 비롯해 일본, 인도, 호주, 뉴질랜드, 싱가포르, 대만에서 총 39개 팀, 약 140여 명이 참가했습니다.&lt;/p&gt;

&lt;p&gt;여러 번의 도전 경험 덕분인지, 2023년 대회에서는 크게 긴장되지 않더라고요. 아마, 이때부터는 문제별 순위가 제공되었던 것으로 기억해요.
비록 전체 순위권에는 들지 못했지만, 제가 맡은 문제 중 하나가 전체 1등을 차지했다는 점이 작은 위안이 되었답니다. 😌&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2024-genai-gameday&quot;&gt;&lt;a href=&quot;#2024-gameday&quot;&gt;2024 GenAI GameDay&lt;/a&gt;&lt;a id=&quot;2024-gameday&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;2024년 11월 14일, 올해의 최대 화두인 GenAI를 주제로 한 대회가 개최되었고, 약 44여 개 팀이 참가했습니다.&lt;/p&gt;

&lt;p&gt;사실 올해 대회는 지난 4번의 GameDay에서 좋은 성적을 거두지 못했기에 참가를 망설였습니다. 하지만 이번 주제가 GenAI였고, 회사의 적극적인 참가 독려도 있어 참가를 결정하게 되었습니다.&lt;/p&gt;

&lt;p&gt;4년간 AWS를 다루며 쌓은 경험도 있고, 최근에는 Amazon Bedrock 관련 서적을 준비하고 있어서 이번에는 꼭 좋은 결과를 내고 싶었습니다. 🙏&lt;/p&gt;

&lt;p&gt;그래서 대회 전날에는 야근을 하면서까지 사전 공지된 출제 범위에 대해 미리 예습을 하고 갔답니다.&lt;/p&gt;

&lt;h3 id=&quot;방심은-금물-&quot;&gt;방심은 금물 🚫&lt;/h3&gt;

&lt;p&gt;저는 과거 2022년 대회에서 중간에 1등을 하다가 사소한 실수로 순위권 밖으로 밀려난 쓰라린 경험이 있습니다.
과거 교훈을 통해, 이번 대회도 겸손한 마음으로 임해야지 하고, 침착하고 여러 번의 검토 과정을 거치며 문제를 풀었습니다.
저희 팀원 모두 압도적인 문제 해결로 대회 중반 즈음에 1위를 달성했습니다.&lt;/p&gt;

&lt;p&gt;따라잡기 힘든 점수 차이를 만들었다고 생각하고 잠시 휴식을 취하던 중, 순위가 2위로 떨어졌습니다.&lt;/p&gt;

&lt;p&gt;순간 과거의 악몽이 떠올라 식겁했죠. 알고 보니 보너스 점수가 부여되는 설문조사에 참여하지 않아 순위가 역전된 것이었습니다. 다행히 빠르게 설문에 참여해 다시 1등 자리를 되찾을 수 있었습니다.&lt;/p&gt;

&lt;p&gt;전리품?으로 가지고 있는 당시 최종 스코어보드 입니다. 설문 조사 점수가 5,000점인데, 만약 설문에 참여하지 않았다면 순위권 밖인 4위로 밀려났을 것을 생각하면 아찔합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/post/etc/scoreboard.png&quot; alt=&quot;scoreboard&quot; /&gt;&lt;/p&gt;

&lt;p&gt;‘끝날 때까지 끝난 게 아니다’라는 교훈을 다시 한번 되새기며 대회를 마무리했습니다.&lt;/p&gt;

&lt;h3 id=&quot;우승--전문가-&quot;&gt;우승 != 전문가 🧑🏻‍💻&lt;/h3&gt;

&lt;p&gt;저는 4년간의 기다림 끝에 마침내 우승을 할 수 있었어요.
앞선 3번의 실패 경험을 통해, 실력은 둘째치고 운도 따라야 한다는 것을 너무나 잘 알고 있어요.&lt;/p&gt;

&lt;p&gt;또 아시아를 넘어 EMEA, LATAM, NAMER 등 다른 리전에 엄청난 실력자들이 있다는 것도 잘 알고 있습니다.
이번 성과가 기존의 노력들을 위로하는 작은 선물이라 생각하고, 내실을 다져가는데 노력하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;감사-인사&quot;&gt;&lt;a href=&quot;#thankyou&quot;&gt;감사 인사&lt;/a&gt;&lt;a id=&quot;thankyou&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;마지막으로, 이번 대회에서 함께 우승을 일궈낸 ‘Bedrock Scissors Paper(&lt;a href=&quot;https://user-bin-ksh.medium.com/&quot;&gt;🏃🏻 usr/bin/ksh&lt;/a&gt;, &lt;a href=&quot;https://medium.com/@nuatmochoi&quot;&gt;🏃‍♂️ nuatmochoi&lt;/a&gt;)’ 팀의 사랑하는 동기들에게 감사의 마음을 전합니다.&lt;/p&gt;

&lt;p&gt;또한, 끝까지 포기하지 않고 꾸준히 도전하여 2등이라는 놀라운 성과를 이룬 ‘GSN Growth Lab’ 팀의 동료들에게도 축하와 감사의 인사를 전합니다!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/post/etc/team.png&quot; alt=&quot;team&quot; /&gt;&lt;/p&gt;

&lt;p&gt;현재 트로피가 배송 중이라고 하는데, 하루빨리 받아보고 싶네요!
&lt;del&gt;이틀 후에는 인터뷰도 예정되어 있습니다. 기사가 나오면 이 글 하단에 참고 자료로 추가하도록 하겠습니다.&lt;/del&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://news.naver.com/mnews/article/138/0002188476&quot;&gt;📰 인터뷰 기사&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;🏆 우승 트로피와 개인 메달
&lt;img src=&quot;../../assets/built/images/post/etc/spoils.png&quot; alt=&quot;spoils&quot; /&gt;&lt;/p&gt;

&lt;p&gt;🦄 행운을 가져다주는 유니콘과 함께, 소중한 시간을 내어 읽어주셔서 감사합니다! 😃&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/background/unicorn.jpeg&quot; alt=&quot;unicorn&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;</content>

      
      
      
      
      

      <author>
          <name>Jihun Lim</name>
        
        
      </author>

      

      
        <category term="uncategorized" />
      
        <category term="aws" />
      
        <category term="genai" />
      
        <category term="event" />
      

      
        <summary type="html">4년 간의 기다림, AWS APJC PLES GameDay 우승 후기</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Create a Multi-Tool Agent with Cluade 3 and LangChain</title>
      <link href="https://heuristicwave.github.io/Agents" rel="alternate" type="text/html" title="Create a Multi-Tool Agent with Cluade 3 and LangChain" />
      <published>2024-04-13T00:00:00+00:00</published>
      <updated>2024-04-13T00:00:00+00:00</updated>
      <id>https://heuristicwave.github.io/Agents</id>
      <content type="html" xml:base="https://heuristicwave.github.io/Agents">&lt;p&gt;ReAct 2편 - Multi-Tool Agent 만들기&lt;/p&gt;

&lt;h1 id=&quot;intro&quot;&gt;&lt;a href=&quot;#intro&quot;&gt;Intro&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;이번 포스팅은 &lt;a href=&quot;https://heuristicwave.github.io/ReAct&quot;&gt;1편&lt;/a&gt;에서 미뤘던 ReAct Agent 생성자(Constructor) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_react_agent&lt;/code&gt;와 여러 개의 Task를 수행하는 &lt;strong&gt;Multi-Agents&lt;/strong&gt;를 구현하는 방법에 대해 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;1️⃣-1편-복습&quot;&gt;&lt;a href=&quot;#review-part-1&quot;&gt;1️⃣ 1편 복습&lt;/a&gt;&lt;a id=&quot;review-part-1&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;❓ 다음 Cluade 3와의 채팅 화면은 가능한 대화인가요?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/post/ai/ReAct/datetime.png&quot; alt=&quot;claude3&quot; /&gt;&lt;/p&gt;

&lt;details&gt;
  &lt;summary&gt;✅ 정답 보기 Click 👈&lt;/summary&gt;

  &lt;p&gt;정답은 가능할 수도 아닐 수도 있습니다. 🙄 무슨 말이냐고요?&lt;/p&gt;

  &lt;p&gt;콘솔 화면에서는 불가능한 화면이지만, Cluade API를 호출한다면, 가능한 대화입니다.&lt;/p&gt;

  &lt;p&gt;위 사진에서 보이는, 현 시간을 인식하는 기능과 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Cluade 3&lt;/code&gt;에게 존재하지 않습니다. 또한, 수학 계산 능력도 부족합니다. 그러나, &lt;a href=&quot;https://heuristicwave.github.io/ReAct&quot;&gt;지난 1편&lt;/a&gt;에서 이야기한 Tools을 활용한 ReAct 기법을 적용하면 가능합니다.&lt;/p&gt;

&lt;/details&gt;

&lt;h3 id=&quot;initialize_agent-built-in-tool--custom-tool&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;initialize_agent&lt;/code&gt;, Built-in Tool &amp;amp; Custom Tool&lt;/h3&gt;

&lt;p&gt;위 채팅 화면과 같은 결과를 얻으려면 다음과 같이, 2개의 Tool을 llm에 주입하면 구현 가능합니다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;기존 Built-in Tool &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;llm-math&lt;/code&gt;에 Custom Tool &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;time&lt;/code&gt; 추가&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;langchain.agents&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_tools&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tool&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initialize_agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AgentType&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;langchain_community.chat_models&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BedrockChat&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;datetime&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;chat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BedrockChat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;anthropic.claude-3-sonnet-20240229-v1:0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model_kwargs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;max_tokens&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2048&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;temperature&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;region_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;us-west-2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tools&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_tools&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;llm-math&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;llm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tool&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;현재 시간과 관련된 질문에 사용합니다. 이 함수는 항상 오늘의 시간을 반환합니다.&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;now&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initialize_agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tools&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;llm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AgentType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CHAT_ZERO_SHOT_REACT_DESCRIPTION&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;handle_parsing_errors&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;지금은 몇 일 몇 시이고, 19시 발표 예정인 저에게 발표 자료를 만들기까지 남은 시간은 몇 시간 인가요?&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2️⃣-create_xxx_agent&quot;&gt;&lt;a href=&quot;#create-xxx-agent&quot;&gt;2️⃣ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_XXX_agent&lt;/code&gt;&lt;/a&gt;&lt;a id=&quot;create-xxx-agent&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;이번에는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;initialize_agent&lt;/code&gt; 생성자 대신, 새로운 생성자인 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_react_agent&lt;/code&gt; 생성자를 사용해 Agent를 만들어 보겠습니다. url의 정보를 읽어들여 질문에 대하여 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_react_agent&lt;/code&gt;를 활용해 추론하는 간단한 에이전트입니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;https://aws.amazon.com/ko/blogs/korea/introducing-aws-ambassador-program/&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tool&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;amb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;AWS Ambassador와 관련된 질문에 사용합니다. 이 함수는 한국의 Ambassador 정보를 반환합니다.&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SeleniumURLLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;urls&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;blog&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;blog&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;page_content&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;prompt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pull&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;hwchase17/react&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_react_agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;llm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tools&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;amb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prompt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prompt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;agent_executor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AgentExecutor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tools&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;amb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;agent_executor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;invoke&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;input&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;한국의 AWS Ambassador는 몇 명인가요?&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;-추론-중-발생하는-이슈&quot;&gt;🐞 추론 중 발생하는 이슈&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/post/ai/ReAct/error01.png&quot; alt=&quot;claude3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_react_agent&lt;/code&gt;를 사용했을 때, 답은 잘 생성하지만 제대로 된 Custom Tool을 작성했음에도 불구하고,
&lt;em&gt;‘xxx is not a valid tool,&lt;/em&gt; 과 같은 메시지와 함께 추론에 이슈가 발생합니다. 물론, 결과적으로 답변을 잘 생성하지만, 다양한 Task를 부여한다면 원하는 답변을 얻지 못할 수도 있습니다. 그래서 Cluade 모델에게 익숙한 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_xml_agent&lt;/code&gt; 생성자를 사용해 에이전트를 생성해야 합니다. 🦜️🔗 LangChain &lt;a href=&quot;https://python.langchain.com/docs/modules/agents/agent_types/xml_agent/&quot;&gt;XMLAgent 문서&lt;/a&gt;에 &lt;em&gt;‘Claude와 같은 일부 언어 모델은 특히 XML 추론/작성 능력이 뛰어납니다.’&lt;/em&gt; 라는 내용이 기재되어 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;💡 &lt;a href=&quot;https://docs.anthropic.com/claude/docs/use-xml-tags&quot;&gt;Claude 모델은 XML tags에 익숙합니다.&lt;/a&gt; &lt;br /&gt; Claude is particularly familiar with prompts that have XML tags as Claude was exposed to such prompts during training. By wrapping key parts of your prompt (such as instructions, examples, or input data) in XML tags, you can help Claude better understand the context and generate more accurate outputs.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;create_react_agent-vs-create_xml_agent&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_react_agent&lt;/code&gt; vs &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_xml_agent&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;동일 모델로 생성한 결과를 확인해 보면, xml 생성자를 사용하면 답변에 조건을 포함해 더 나은 답변을 제공합니다. &lt;em&gt;(현재 11명이지만, url 시점으로 한정하여 올바른 답변 생성)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_react_agent&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;input&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;한국의 AWS Ambassador는 몇 명인가요?&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;output&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;한국의 AWS Ambassador는 현재 10명입니다.&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_xml_agent&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;input&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;한국의 AWS Ambassador는 몇 명인가요?&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;output&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;2023년 5월 기준으로 한국에는 10명의 AWS Ambassador가 활동하고 있습니다.&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3️⃣-multi-tool-agent️-tool--custom-tool&quot;&gt;&lt;a href=&quot;#multi-tool-agent&quot;&gt;3️⃣ Multi-Tool Agent(🦜️🔗 Tool &amp;amp; Custom Tool)&lt;/a&gt;&lt;a id=&quot;multi-tool-agent&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;이번에는 Built-in Tool이 아닌 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;langchain_community&lt;/code&gt;에서 제공하는 도구와 조금 더 복잡한 작업을 지원하는 Custom Tool을 사용하여 Multi-Tool Agent를 만들어 보겠습니다. &lt;em&gt;(PPT를 만들기 위해 Markdown을 PPT로 만드는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;get_marp&lt;/code&gt;라는 Custom Tool을 사전에 정의해 두었습니다.)&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;-genai로-ppt-만들기&quot;&gt;🧭 GenAI로 PPT 만들기&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;‘AWS에서 발표하려 합니다. AWS에 대한 최근 정보를 duckduckgo에 검색 후, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;get_marp&lt;/code&gt; tool을 사용해 marp 형식으로 발표 자료를 만들어 주세요.’&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;위 요구사항을 만족하기 위해서는 다음 2가지의 Task를 Tool이 지원해야 합니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;최신 자료 검색 : &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;langchain_community&lt;/code&gt;의 DuckDuckGO Tool 활용&lt;/li&gt;
  &lt;li&gt;PPT 제작 : MARP로 PPT를 생성하는 Custom Tool 활용&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_xml_agent&lt;/code&gt;를 활용하여 Multi-Tool Agent를 구현하는 방법은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;initialize_agent&lt;/code&gt;를 사용할 때와 크게 다르지 않습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;my_tools&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;duckduckgo_tool&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_marp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;my_agent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_xml_agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;llm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tools&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;my_tools&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prompt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prompt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;agent_executor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AgentExecutor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;my_agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tools&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;my_tools&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;👿 실제 Agent와 Tool, Toolkits을 통합하다 보면, Type Casting 문제를 비롯하여, 더 나은 추론 결과를 생성하기 위해 Prompt Engineering에 굉장히 많은 시간이 소요됩니다&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;️-검증&quot;&gt;✔️ 검증&lt;/h3&gt;

&lt;p&gt;아래 추론 과정을 살펴보면, &lt;em&gt;AWS 최근 정보라는 Task&lt;/em&gt;를 수행하기 위해 Tool로 DuckDuckGo Search를 선택하고 적절한 키워드로 검색합니다. 이어서 &lt;em&gt;발표 자료 만들기 Task&lt;/em&gt;를 수행하기 위해 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;get_marp&lt;/code&gt; Tool을 호출해 의도한 대로 작업을 수행하는 모습을 확인할 수 있었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/post/ai/ReAct/inference.png&quot; alt=&quot;inference&quot; /&gt;&lt;/p&gt;

&lt;details&gt;
  &lt;summary&gt;✅ 검색 결과 비교 Click 👈&lt;/summary&gt;

  &lt;p&gt;실제 duckduckgo에 ‘AWS latest news’로 검색한 화면과 PPT를 만들기 위해 MARP 형식으로 작성한 내용이 동일합니다.&lt;/p&gt;

  &lt;div class=&quot;language-markdown highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gh&quot;&gt;# AWS 최신 뉴스&lt;/span&gt;

&lt;span class=&quot;gu&quot;&gt;## AWS Deadline Cloud 발표 (2024년 4월 2일)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;
-&lt;/span&gt; 완전 관리형 렌더링 서비스
&lt;span class=&quot;p&quot;&gt;-&lt;/span&gt; 렌더링 파이프라인 효율성 향상
&lt;span class=&quot;p&quot;&gt;-&lt;/span&gt; 더 많은 작업 처리 가능

&lt;span class=&quot;gu&quot;&gt;## NVIDIA와 AWS 통합 강화&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;
-&lt;/span&gt; 고객 코드 및 데이터 보안 강화
&lt;span class=&quot;p&quot;&gt;-&lt;/span&gt; 독립적으로 NCC 그룹에 의해 검증됨

&lt;span class=&quot;gu&quot;&gt;## re:Invent 2023 주요 발표&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;
-&lt;/span&gt; 생성 AI가 주요 관심사
&lt;span class=&quot;p&quot;&gt;-&lt;/span&gt; 실제 비즈니스 이득을 위한 혁신
&lt;span class=&quot;p&quot;&gt;-&lt;/span&gt; 보안, 선택, 성능 향상
&lt;span class=&quot;p&quot;&gt;-&lt;/span&gt; 데이터 정렬 및 거버넌스 지원
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;

  &lt;p&gt;&lt;img src=&quot;../../assets/built/images/post/ai/ReAct/duckduckgo.png&quot; alt=&quot;duckduckgo&quot; /&gt;&lt;/p&gt;

&lt;/details&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;outro&quot;&gt;&lt;a href=&quot;#outro&quot;&gt;Outro&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;1편을 작성하고 2편이 나오기까지 2달이 걸렸습니다. 그래도 다짐을 글로 적어두니, 돌고 돌아 작성하게 되는 것 같습니다. 매번 글쓰기는 고통스럽지만, 작성하고 나면 뿌듯하기에 3편은 LangGraph와 Routing에 대한 내용을 약속하며 이번 포스팅을 마무리 짓겠습니다.&lt;/p&gt;

&lt;p&gt;소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해주세요! 😃&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;span class=&quot;table-of-contents-list&quot;&gt;🦜️🔗 LangChain&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;table-of-contents-list&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;./ReAct&quot;&gt;ReAct&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;./Agents&quot;&gt;Multi Tool Agents&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Jihun Lim</name>
        
        
      </author>

      

      
        <category term="ai" />
      
        <category term="genai" />
      
        <category term="llm" />
      

      
        <summary type="html">ReAct 2편 - Multi-Tool Agent 만들기</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">LLM Evaluation에 대해 끄적이기</title>
      <link href="https://heuristicwave.github.io/LLMEvaluation" rel="alternate" type="text/html" title="LLM Evaluation에 대해 끄적이기" />
      <published>2024-03-31T00:00:00+00:00</published>
      <updated>2024-03-31T00:00:00+00:00</updated>
      <id>https://heuristicwave.github.io/LLMEvaluation</id>
      <content type="html" xml:base="https://heuristicwave.github.io/LLMEvaluation">&lt;p&gt;LLMOps, LLM App Development Life Cycle의 한 부분 LLM Evaluation에 대하여…&lt;/p&gt;

&lt;h1 id=&quot;intro&quot;&gt;Intro&lt;/h1&gt;

&lt;p&gt;LLM Evaluation 솔루션에 대해 조사를 하다, 도입부에 소개할 만한 좋은 글을 발견했습니다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://maverickventures.medium.com/what-the-history-of-software-development-tells-us-about-the-hurdles-to-enterprise-adoption-of-llms-c96bc968456d&quot;&gt;📝 What the History of Software Development Tells Us about the Hurdles to Enterprise Adoption of LLMs&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;소프트웨어 개발 프로세스에 모든 단계가 중요하지만, 궁극적인 목표는 사용자에게 안정적으로 가치를 제공하는 것이므로 ‘테스트’, ‘평가’, ‘모니터링’은 항상 역사적으로 큰 시장이었습니다.
자체 설문 조사에서 LLM을 운영에 배포하는 데 있어 가장 큰 장벽은 “모델 품질 평가(evaluating model quality)”를 꼽았습니다.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;LLM Evaluate, Test, Monitoring Landscape&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;아래 그림은 위에서 언급한 글에 소개된 LLMOps Landscape로, 잘못된 내용들이 있으므로 참고만 하세요.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/v2/resize:fit:4800/format:webp/1*7ot7nNqNWIu7Tw8X8yyGsw.png&quot; width=&quot;870&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이번 포스팅에서는 해당 글에서 소개하는 여러 LLM 평가 스타트업 중, ‘UpTrain’과 ‘Ragas’를 중점으로 이야기를 풀어나가 보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;1️⃣-uptrain&quot;&gt;1️⃣ &lt;a href=&quot;https://uptrain.ai/&quot;&gt;UpTrain&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&quot;주요-특징&quot;&gt;주요 특징&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.ycombinator.com/companies/uptrain-ai&quot;&gt;Y Combinator&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/uptrain-ai/uptrain&quot;&gt;GitHub(1.9k)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Ground Truth 비교, 사용자 정의 평가 등 9개의 범주 아래, 20개의 평가 메트릭을 제공&lt;/li&gt;
  &lt;li&gt;LlamaIndex, Langfuse, ChromaDB 등에 대한 통합 제공&lt;/li&gt;
  &lt;li&gt;홈페이지에서 GDPR, ISO 27001 취득한 것으로 보임&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;사용-방법&quot;&gt;사용 방법&lt;/h3&gt;

&lt;h4 id=&quot;llm-모델-선택&quot;&gt;LLM 모델 선택&lt;/h4&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uptrain&lt;/code&gt;을 설치하고 다음과 같이 모델을 선택합니다. 본래 모델 선택 시, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EvalLLM()&lt;/code&gt;에 LLM 호출을 위한 KEY를 기재합니다.
Amazon Bedrock 서비스를 사용하는 경우, Settings를 활용해 LLM 주입이 가능합니다. (&lt;a href=&quot;https://github.com/uptrain-ai/uptrain/issues/589&quot;&gt;참고&lt;/a&gt;)&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;uptrain&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Settings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;EvalLLM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Evals&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;json&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;settings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Settings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;bedrock/anthropic.claude-3-sonnet-20240229-v1:0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;eval_llm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;EvalLLM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;settings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;settings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;평가-dataset-준비&quot;&gt;평가 DataSet 준비&lt;/h4&gt;

&lt;p&gt;아쉽게도 UpTrain에서는 평가 DataSet 생성을 보조하는 기능이 아직은 없습니다. 질문(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;question&lt;/code&gt;), 맥락(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;context&lt;/code&gt;), 답변(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;response&lt;/code&gt;)을 다음과 같이 준비합니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;question&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;Which is the most popular global sport?&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;Who created the Python language?&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;context&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;_comment&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;생략&quot;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;response&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:[&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;Football is the most popular sport with around 4 billion followers worldwide&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;Python language was created by Guido van Rossum.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;평가-메트릭-선택&quot;&gt;평가 메트릭 선택&lt;/h4&gt;

&lt;p&gt;제공되는 &lt;a href=&quot;https://docs.uptrain.ai/predefined-evaluations/overview&quot;&gt;지표&lt;/a&gt;를 참고하여, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;checks&lt;/code&gt; 변수에 list로 주입합니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;results&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eval_llm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;evaluate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;checks&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Evals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CONTEXT_RELEVANCE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Evals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FACTUAL_ACCURACY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Evals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RESPONSE_COMPLETENESS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Results
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dumps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;indent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;details&gt;
  &lt;summary&gt;결과 보기 👉 Click&lt;/summary&gt;

  &lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;question&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;_comment&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;생략&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;context&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;_comment&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;생략&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;response&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;_comment&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;생략&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;score_context_relevance&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;explanation_context_relevance&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;{&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;Reasoning&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;
    The extracted context contains two separate paragraphs, each addressing one of the two queries. &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;
    The first paragraph discusses the popularity of sports globally, &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;
    mentioning that football is considered the most popular sport with a large following. &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;
    This information can answer the first query &apos;Which is the most popular global sport?&apos; completely. &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;
    The second paragraph provides details about the creation of the Python programming language by Guido van Rossum, &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;
    which can answer the second query &apos;Who created the Python language?&apos; completely. &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;
    Therefore, the extracted context can answer both queries in their entirety.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;Choice&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;score_factual_accuracy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;explanation_factual_accuracy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;{&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;    &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;Result&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;: [&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;        {&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;Fact&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;Football is the most popular sport with around 4 billion followers worldwide.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;Reasoning&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;The context directly states that &apos;Football is undoubtedly the world&apos;s most popular sport&apos; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;
    and mentions that it has &apos;a followership of more than 4 billion people&apos;. &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;
    This supports the given fact.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;            &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;Judgement&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;yes&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;        },&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;
    {&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;            &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;Fact&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;Python language was created by Guido van Rossum.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;Reasoning&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;The context explicitly mentions that &apos;Python, created by Guido van Rossum in the late 1980s,
    is a high-level general-purpose programming language&apos;. This directly supports the given fact.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;Judgement&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;yes&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;        }&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;    ]&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;score_response_completeness&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;explanation_response_completeness&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;_comment&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;생략&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;

&lt;/details&gt;

&lt;h4 id=&quot;대시보드-사용&quot;&gt;대시보드 사용&lt;/h4&gt;

&lt;p&gt;UpTrain OSS Dashboard도 제공하는데, Docker Compose로 Server Up 하는 스크립트 실행합니다.
이어서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;localhost:3000&lt;/code&gt;에서 코드로 평가를 진행할 때와 동일하게 GUI를 눌러 수행하면 평가 결과를 시각화하여 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/built/images/post/etc/uptrain.png&quot; alt=&quot;dashboard&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;pros-and-cons&quot;&gt;Pros and Cons&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;장점&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;다양한 상황에서 적용할 수 있는 지표들에 대한 손쉬운 적용&lt;/li&gt;
  &lt;li&gt;Evaluations/Prompts 테스트에 대하여 한 번에 여러 가지 테스트가 가능 (배치)&lt;/li&gt;
  &lt;li&gt;‘A/B 테스트’, ‘사용자 정의 프롬프트 기반 평가 셋(사용자가 평가 기준을 LLM에게 전달)’ 등의 기능 제공&lt;/li&gt;
  &lt;li&gt;Local 환경에서 평가 가능&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;단점&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;GitHub에서 제공하는 OSS Dashboard가 오퍼링 웹사이트만큼 다양한 기능을 제공하고 있지는 않음&lt;/li&gt;
  &lt;li&gt;평가 후, 얻은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jsonl&lt;/code&gt; 파일을 업로드해서 시각화하는 기능은 미제공&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;-생각&quot;&gt;🤔 생각&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://news.ycombinator.com/item?id=35069839&quot;&gt;UpTrain 팀의 수익화&lt;/a&gt; 계획에서 더 넓은 통합과 Dashboard의 향상된 경험을 제공하는 것으로 보이나,
OSS로 활용한다면 별도의 Dashboard 개발 필요한 것 같습니다. 또한 아직은 통합을 지원하는 범위가 좁지만, 평가 역할 수행하기에는 좋은 도구인 것 같습니다.&lt;/p&gt;

&lt;p&gt;LLM Evaluation은 어렵습니다. 어떻게 평가 기준을 설계해야 하는지 모르겠다면, 제공되는 20여 개의 metrics들로 다양한 시각에서 평가하는 방법을 고려할 수 있을 것 같습니다.
그뿐만 아니라 해당 도구를 채택하지 않더라도, 제공하는 metrics들을 참고하면 계획하고 있는 평가 방법들에 대한 좋은 참고 자료가 되는 것 같습니다.
예를 들어, UpTrain은 응답의 품질을 평가하기 위해 대응 여부, 간결성, 관련성, 유효성, 일관성 등 5가지의 요소로 품질을 평가합니다.
UpTrain이 제공하는 metrics의 종류는 평가 Task를 해결하기 위한 방법들이 되므로, 평가 계획 수립에 유용할 것 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2️⃣-ragas&quot;&gt;2️⃣ &lt;a href=&quot;https://docs.ragas.io/en/stable/&quot;&gt;Ragas&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&quot;주요-특징-1&quot;&gt;주요 특징&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/explodinggradients/ragas&quot;&gt;GitHub(4.1k)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;RAG 파이프라인 전용 평가 솔루션 : 데이터 셋 생성, RAG 평가, 모니터링 등의 기능 제공&lt;/li&gt;
  &lt;li&gt;정확도, 관련성 등 10개의 범주 아래 다양한 메트릭 제공(평가 방법에 대한 수식 제공)&lt;/li&gt;
  &lt;li&gt;언어별 다중 프롬프트 생성, 평가를 위한 테스트 데이터 증강 등 부가 기능 제공&lt;/li&gt;
  &lt;li&gt;LangChain을 활용한 CSP 모델 및 LlamaIndex, LangSmith 등 다양한 프레임워크에 통합 지원&lt;/li&gt;
  &lt;li&gt;Atina, Zeno, Tonic 등 다양한 방법으로 시각화 제공&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;-생각-1&quot;&gt;🤔 생각&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://docs.uptrain.ai/tutorials/analyzing-failure-cases&quot;&gt;UpTrain 문서에서 RAG 파이프라인을 분석하는 글&lt;/a&gt;을 보고, RAG 파이프라인 전용 평가 도구의 필요성을 고민하다 Ragas가 흥미로워 살펴보게 되었습니다.
UpTrain뿐만 아니라, 다른 Evaluation 솔루션들도 ‘RAG 평가’는 방법 중 하나일 뿐, 왜 RAG에 한정하여 전문적인 도구가 필요할지 고민해 보았습니다.
Ragas는 &lt;strong&gt;&lt;a href=&quot;https://docs.ragas.io/en/stable/concepts/metrics_driven.html&quot;&gt;MDD(지표 중심 개발)&lt;/a&gt;&lt;/strong&gt;라는 용어를 내세우며, LLM 앱을 데이터 기반 의사 결정이 매우 중요하다고 강조합니다.
해당 사실은 Ragas뿐만 아니라, 다른 Evaluation 솔루션들도 입을 모아 Observability의 중요성을 언급합니다.
그러나 다른 솔루션들의 문서와는 달리, Ragas 문서는 &lt;a href=&quot;https://docs.ragas.io/en/stable/concepts/metrics/index.html&quot;&gt;측정 메트릭&lt;/a&gt; 별, 수식과 예시와 함께 제공하니 그들의 주장에 조금 더 마음이 가는 것 같습니다.&lt;/p&gt;

&lt;p&gt;Ragas의 테스트 데이터 셋 생성 기능은 Ragas를 채택하지 않더라도 도움이 될 것 같습니다. 더하여 Ragas는 내부적으로 langchain을 활용하므로,
프로덕션급 LLM 앱 구축 플랫폼인 &lt;a href=&quot;https://docs.smith.langchain.com/&quot;&gt;LangSmith&lt;/a&gt;를 보완하여 더 나은 기능을 제공할 것으로 보입니다. (앞으로의 숙제네요 🫠)&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;outro&quot;&gt;Outro&lt;/h2&gt;

&lt;p&gt;Evaluation에서도 Data가 매우 중요합니다. 아직 스터디가 더 필요하지만, 그동안 ‘평가 Data 제작’ 부분은 종종 봤지만, ‘평가 Data 활용 방안’은 더 적은 것 같습니다.
평가 &lt;strong&gt;Data의 재사용성&lt;/strong&gt;을 높이기 위해 고려할 지점이 있어 보여, 다음과 같이 몇 자 끄적이며 마치도록 하겠습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Training&lt;/strong&gt; : sLM 도입 및 파인튜닝 시, 튜닝을 위한 Datasets에 평가 Datasets을 활용할 수 있음&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;호환성&lt;/strong&gt; : 다양한 평가도구들이 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jsonl&lt;/code&gt; 형태로 지원하여, 이기종 간 호환이 자유롭다면 다양한 평가 가능&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해주세요! 😃&lt;/p&gt;

&lt;hr /&gt;</content>

      
      
      
      
      

      <author>
          <name>Jihun Lim</name>
        
        
      </author>

      

      
        <category term="ai" />
      
        <category term="genai" />
      
        <category term="llm" />
      

      
        <summary type="html">LLMOps, LLM App Development Life Cycle의 한 부분 LLM Evaluation에 대하여…</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">ReAct leverages LLM as a reasoning engine</title>
      <link href="https://heuristicwave.github.io/ReAct" rel="alternate" type="text/html" title="ReAct leverages LLM as a reasoning engine" />
      <published>2024-02-18T00:00:00+00:00</published>
      <updated>2024-02-18T00:00:00+00:00</updated>
      <id>https://heuristicwave.github.io/ReAct</id>
      <content type="html" xml:base="https://heuristicwave.github.io/ReAct">&lt;p&gt;LLM을 추론엔진으로 활용하는 ReAct (with LangChain &amp;amp; Amazon Bedrock)&lt;/p&gt;

&lt;h1 id=&quot;intro&quot;&gt;Intro&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/&quot;&gt;LangChain for LLM Application Development&lt;/a&gt; 강의에서 Andrew Ng 교수님은 &lt;em&gt;“사람들은 때때로 LLM이 많은 정보를 암기하기 위해 학습된 지식 저장소라 생각한다”&lt;/em&gt;라며,
LLM을 더 유용하게 사용하는 방법은 추론(Reasoning) 엔진으로 생각하는 것이 더 유용하다고 말합니다. 교수님의 말대로 추론엔진으로써의 LLM을 활용하기 위해서는 ReAct 개념을 숙지해야 하는데요,
저는 작년 5월경 AutoGPT, BabyAGI가 소개될 때 ReAct를 처음 접했는데 굉장히 어려운 개념이다 보니 이해하는데 시간이 오래 걸렸습니다.&lt;/p&gt;

&lt;p&gt;ICLR 2023, &lt;a href=&quot;https://arxiv.org/abs/2210.03629&quot;&gt;ReAct: Synergizing Reasoning and Acting in Language Models&lt;/a&gt; 논문에서 LLM을 사용하여 인터리브 방식으로 &lt;strong&gt;추론 추적(reasoning traces)&lt;/strong&gt;과 &lt;strong&gt;작업별 동작(task-specific actions)&lt;/strong&gt;을 모두 생성하는 ReAct라는 프레임워크를 소개했습니다.
이후, LangChain(이하, 🦜️🔗)에서는 Agents를 통해 ReAct 기법을 지원하기 시작했습니다. 앞으로 3편 이상의 시리즈물을 통해, ReAct 개념과 구현 및 AWS, OpenAI 등의 회사가 어떻게 ReAct와 관련된 제품을 설계했는지 등을 알아보겠습니다.
이번 포스팅에서는 LLM을 추론엔진으로 활용하는 ReAct의 개념과 Amazon Bedrock과 🦜️🔗을 활용해 ReAct 기법을 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;1️⃣-react-prompting&quot;&gt;1️⃣ &lt;a href=&quot;https://www.promptingguide.ai/techniques/react&quot;&gt;ReAct Prompting&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;앞서 언급한 ReAct 논문을 바탕으로 작성된 Prompt Engineering Guide 문서는 ReAct Prompting을 다음과 같이 설명합니다.&lt;/p&gt;

&lt;p&gt;ReAct는 인간이 새로운 작업을 학습하고 의사 결정이나 추론을 할 수 있도록 하는 “행동” 과 “추론”의 시너지 효과에서 영감을 받았다고 합니다.
첫 번째 단계는 트레이닝 세트(예:&lt;a href=&quot;https://huggingface.co/datasets/hotpot_qa&quot;&gt;HotPotQA&lt;/a&gt;)에서 사례를 선택하고 ReAct 형식의 궤적(trajectories)을 구성합니다.
이는 일종의 퓨샷(few-shot) 예시로 사용됩니다. 궤적은 여러 생각-행동-관찰(thought-action-observation) 단계로 구성됩니다.&lt;/p&gt;

&lt;h3 id=&quot;️-agents&quot;&gt;🦜️🔗 &lt;a href=&quot;https://python.langchain.com/docs/modules/agents/concepts&quot;&gt;Agents&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;앞서 소개한 ReAct의 개념을 🦜️🔗에서는 Agents라는 개념으로 구현했습니다. Agents의 핵심 아이디어는 언어 모델을 추론 엔진으로 사용해 어떤 작업을 어떤 순서로 수행할지 결정하는 것입니다.
Agents를 크게 5개의 핵심 컴포넌트로 구성되어 있습니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Schema
    &lt;ul&gt;
      &lt;li&gt;AgentAction : Agent가 수행해야 하는 작업을 나타내는 dataclass&lt;/li&gt;
      &lt;li&gt;AgentFinish : Agent의 최종 결과, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;return_values&lt;/code&gt;의 경우 &lt;strong&gt;key-value&lt;/strong&gt; 형태로 리턴&lt;/li&gt;
      &lt;li&gt;Intermidiate Steps : Agents 사이의 출력, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;List[Tuple[AgentAction, Any]]&lt;/code&gt; 타입으로 Observation은 최대한의 유연성을 위해 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Any&lt;/code&gt;로 남겨짐(실제로는 대부분 문자열)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Agent : 다음 단계에 수행할 역할을 결정하며, 보통 &lt;em&gt;‘언어 모델’, ‘프롬프트’&lt;/em&gt; 와 &lt;em&gt;‘output parser’&lt;/em&gt;로 실행됨
    &lt;ul&gt;
      &lt;li&gt;Agent Inputs : &lt;strong&gt;key-value&lt;/strong&gt; 매핑의 형. 일반적으로 PromptTemplate은 LLM에 잘 전달할 수 있는 형식으로 변환하는 처리&lt;/li&gt;
      &lt;li&gt;Agent Outputs : 다음에 수행할 작업(&lt;strong&gt;AgentActions&lt;/strong&gt;) 혹은 최종 응답(&lt;strong&gt;AgentFinish&lt;/strong&gt;)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;AgentExecutor : Agent의 런타임. Agent를 호출하고, 선택한 작업을 실행하고, 출력을 Agent로 전달하고 반복하는 역할&lt;/li&gt;
  &lt;li&gt;Tools : Agent가 호출할 수 있는 함수. (Tool에 올바른 권한과 도움 되는 방식으로 설명해야 함)&lt;/li&gt;
  &lt;li&gt;Toolkits : 특정 목표를 달성하기 위해 여러 개의 tool이 필요하다면, toolkit을 통해 제공&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2️⃣-실시간으로-상호-작용하는-llm-with-bedrock&quot;&gt;2️⃣ 실시간으로 상호 작용하는 LLM (with Bedrock)&lt;/h2&gt;

&lt;p&gt;Amazon Bedrock Playground에서 Claude 2.1 모델에 23년 아시안컵 우승국을 물어보면, 아직 개최되지 않았다는 정보와 함께 22년 아시안컵이 우승국이 호주라는 환각이 발생합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/post/ai/23AFC.png&quot; alt=&quot;AFC&quot; /&gt;&lt;/p&gt;

&lt;p&gt;아직 Claude 2.1 모델은 23년 아시안컵에 대해서 사전학습된 정보가 없지만, LangChain을 활용해 실시간으로 정보를 검색해 답변이 가능하도록 구현해 보겠습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;langchain.agents&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_tools&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initialize_agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AgentType&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;langchain_community.chat_models&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BedrockChat&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;chat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BedrockChat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;anthropic.claude-v2:1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model_kwargs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;max_tokens_to_sample&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;700&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;temperature&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;region_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;us-west-2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initialize_agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tools&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load_tools&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;wikipedia&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;llm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AgentType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CHAT_ZERO_SHOT_REACT_DESCRIPTION&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;handle_parsing_errors&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;2023년, AFC 아시안컵에서 우승한 나라는?&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;initialize_agent&lt;/code&gt; 함수를 사용해 간단하게 Agent를 구현했습니다. Agent를 사용하기 위해, 사용할 도구, 모델, Agent Type 등을 인자로 받습니다.
여기서는 Wikipedia를 사용하는 도구를 로드하고, 위에서 초기화한 BedrockChat 모델을 대화(chat) 모델로 사용합니다. 그리고 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CHAT_ZERO_SHOT_REACT_DESCRIPTION&lt;/code&gt; 타입의 Agent로 초기화해 사용합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/post/ai/ReAct.png&quot; alt=&quot;ReAct&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위와 같이 질문을 받아, wikipedia를 tool로 사용하여 thought-action-observation 단계를 거쳐 최종적으로 2023 아시안컵 우승국이 카타르라는 사실을 성공적으로 도출했습니다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;해당 예시는 앞서 소개한 🦜️🔗 Agents의 5가지 컴포넌트가 나와있지만, 아주 간단한 Agent라 앞서 배운 AgentExecutor, Toolkits 등의 개념이 나와있지 않습니다.
다음 편에서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_react_agent&lt;/code&gt; 등의 함수로 교체하며 자세히 다루겠습니다.&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;💡 이번 예시에서 다룬 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;initialize_agent&lt;/code&gt;는 0.1.0(&lt;a href=&quot;https://blog.langchain.dev/langchain-v0-1-0/&quot;&gt;24년 1월 8일 release&lt;/a&gt;)에서 deprecate 되었으며, 0.2.0에서는 삭제될 예정입니다. &lt;br /&gt; &amp;gt; &lt;em&gt;LangChainDeprecationWarning: The function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;initialize_agent&lt;/code&gt; was deprecated in LangChain 0.1.0 and will be removed in 0.2.0.
Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3️⃣-tools&quot;&gt;3️⃣ Tools&lt;/h2&gt;

&lt;h3 id=&quot;️-load_tools&quot;&gt;🛠️ &lt;a href=&quot;https://api.python.langchain.com/en/latest/agents/langchain.agents.load_tools.load_tools.html&quot;&gt;load_tools&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;이전 예시에서는 단 하나의 tool(wikipedia)만을 정의해, llm의 선택지가 하나밖에 존재하지 않았지만, 다음과 같은 형태로 다양할 tool 들을 준비하고 LLM의 추론을 완성시킬 수 있습니다.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tools = load_tools([&quot;llm-math&quot;,&quot;wikipedia&quot;])&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;만약, 특정 액션 이후 누군가에게 이메일을 보내야 하는 Action을 추가하려면, AWS Lambda에 email을 보내는 함수를 만들어두고 다음과 같이 tool로 활용해 llm에 의해 이메일을 발송할 수도 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example : &lt;a href=&quot;https://python.langchain.com/docs/integrations/tools/awslambda&quot;&gt;AWS Lambda&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;tools&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_tools&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;awslambda&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;awslambda_tool_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;email-sender&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;awslambda_tool_description&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sends an email with the specified content to test@testing123.com&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;function_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;testFunction1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initialize_agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tools&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;llm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AgentType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ZERO_SHOT_REACT_DESCRIPTION&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Send an email to test@testing123.com saying hello world.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;load_tools에는 AWS Lambda 외에 Amazon API Gateway도 있고, 필요하다면 다음과 같이 직접 tool을 만들 수도 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;️-custom-tools&quot;&gt;⚒️ &lt;a href=&quot;https://python.langchain.com/docs/modules/agents/tools/custom_tools&quot;&gt;Custom Tools&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;다음은 단어의 글자 수를 구하는 간단한 Custom Tool입니다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@tool&lt;/code&gt; decorator와 함께 함수(tool 이름)와 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;description&lt;/code&gt; 정의 후, Agent에서 호출해 사용합니다.
Custom Tools은 앞서 배운 &lt;strong&gt;🦜️🔗 Agents&lt;/strong&gt;의 핵심 컴포넌트를 유의하여 작성해야 합니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tool&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;단어의 글자수를 구합니다.&quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initialize_agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tools&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;llm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AgentType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ZERO_SHOT_REACT_DESCRIPTION&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;invoke&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;input&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;langchain의 글자수를 구하시오.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;코드가 로직상으로 맞을 수 있지만, 추론에 의거하여 답을 구하기 때문에 원하는 대답이 나오지 않을 수도 있습니다.
예를 들어, ‘Tool에 올바른 권한과 도움 되는 방식으로 설명해야 함’을 무시하고 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;description&lt;/code&gt;에 ‘&lt;em&gt;단어의 글자 수를 구하는 질문에 해당 도구를 사용하지 마세요.&lt;/em&gt;‘라고 기재하면 다음과 같이 잘못된 추론을 진행합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/post/ai/customTool.png&quot; alt=&quot;customTool&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;outro&quot;&gt;Outro&lt;/h2&gt;

&lt;p&gt;이번 포스팅에서는 ReAct 개념을 🦜️🔗에서 &lt;strong&gt;Agents&lt;/strong&gt;로 알아보았습니다.
다음 포스팅에서는 AWS가 ReAct 개념을 구현한 &lt;strong&gt;Agents for Amazon Bedrock&lt;/strong&gt;과 Open AI의 &lt;strong&gt;Function calling&lt;/strong&gt;를 비교하며 각각 어떻게 ReAct를 구현했는지 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해주세요! 😃&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;span class=&quot;table-of-contents-list&quot;&gt;🦜️🔗 LangChain&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;table-of-contents-list&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;./ReAct&quot;&gt;ReAct&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;./Agents&quot;&gt;Multi Tool Agents&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Jihun Lim</name>
        
        
      </author>

      

      
        <category term="ai" />
      
        <category term="genai" />
      
        <category term="llm" />
      

      
        <summary type="html">LLM을 추론엔진으로 활용하는 ReAct (with LangChain &amp;amp; Amazon Bedrock)</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Providing a caching layer for LLM with Langchain in AWS</title>
      <link href="https://heuristicwave.github.io/LLMCache" rel="alternate" type="text/html" title="Providing a caching layer for LLM with Langchain in AWS" />
      <published>2023-12-22T00:00:00+00:00</published>
      <updated>2023-12-22T00:00:00+00:00</updated>
      <id>https://heuristicwave.github.io/LLMCache</id>
      <content type="html" xml:base="https://heuristicwave.github.io/LLMCache">&lt;p&gt;AWS 환경에서 Langchain을 활용한 LLM을 위한 Caching layer 제공하기&lt;/p&gt;

&lt;h1 id=&quot;intro&quot;&gt;Intro&lt;/h1&gt;

&lt;p&gt;LLM 기반의 앱에서 Caching layer를 적용한다면, API 호출 수를 줄여 비용을 절약하고
언어 모델의 추론 시간 대신 캐시를 활용해 빠른 응답 속도를 제공할 수 있습니다.
이번 포스팅에서는 얼마 전 re:Invent에서 Preview로 출시한 &lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2023/11/vector-search-amazon-memorydb-redis-preview/&quot;&gt;vector search for Amazon MemoryDB for Redis&lt;/a&gt;를 포함하여, AWS에서 제공하는 Redis 들을 Caching Layer로 사용할 수 있을지 살펴보겠습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;dl&gt;
    &lt;dt&gt;&lt;a href=&quot;https://python.langchain.com/docs/integrations/llms/llm_caching&quot;&gt;LLM Caching integrations&lt;/a&gt;&lt;/dt&gt;
    &lt;dd&gt;🦜️🔗 에서는 In Memory, SQLite, Redis, GPTCache, Cassandra 등을 제공&lt;/dd&gt;
  &lt;/dl&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;caching-in-️&quot;&gt;Caching in 🦜️🔗&lt;/h2&gt;

&lt;p&gt;현재, Langchain에서는 크게 &lt;strong&gt;2가지 캐싱 방법&lt;/strong&gt;과 &lt;strong&gt;캐시 여부를 선택&lt;/strong&gt;할 수 있는 옵션을 제공합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Standard Cache : 완전히 동일한 문장에 대하여 &lt;strong&gt;Prompt&lt;/strong&gt;와 &lt;strong&gt;응답&lt;/strong&gt;에 대한 캐시 Hit를 결정&lt;/li&gt;
  &lt;li&gt;Semantic Cache : 의미론적으로 유사한 문장에 대하여 &lt;strong&gt;Prompt&lt;/strong&gt;와 &lt;strong&gt;응답&lt;/strong&gt;에 대한 캐시 Hit를 결정&lt;/li&gt;
  &lt;li&gt;Optional Caching : 캐시 Hit 여부를 선택적으로 적용할 수 있도록 제공&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Langchain에서 제공하는 RedisCache에 대하여 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EC2 설치형&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ElastiCache for Redis&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MemoryDB for Redis&lt;/code&gt; 각각의 방법을 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;✅ &lt;em&gt;SageMaker &lt;strong&gt;Notebook Instances&lt;/strong&gt; 환경에서 Bedrock을 통해 &lt;strong&gt;Claude 2.1&lt;/strong&gt; 모델로 테스트를 진행&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;-redis-stack-on-ec2&quot;&gt;🐳 Redis Stack on EC2&lt;/h2&gt;

&lt;p&gt;EC2에 직접 Redis를 설치하여 VectorDB 기능으로 활용하는 방법입니다. Redis의 Vector Search 기능을 사용하려면,
Redis OSS의 핵심 기능을 확장한 &lt;strong&gt;Redis Stack&lt;/strong&gt;을 사용해야 합니다. 저는 EC2위에 Docker로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;redis-stack&lt;/code&gt; 이미지를 올려 사용했습니다.&lt;/p&gt;

&lt;details&gt;
  &lt;summary&gt;👇 도커로 Redis Stack 설치하기&lt;/summary&gt;

  &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;yum update &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;yum &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;docker &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;service docker start
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; redis-stack &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 6379:6379 redis/redis-stack:latest
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker ps
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker logs &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; redis-stack
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;

&lt;/details&gt;

&lt;blockquote&gt;
  &lt;p&gt;💡 &lt;strong&gt;redis-cli&lt;/strong&gt;를 활용해 통신 여부 확인 &lt;br /&gt; &amp;gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ redis-cli -c -h {$Cluster_Endpoint} -p {$PORT}&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Redis가 준비되었다면, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;langchain&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;redis&lt;/code&gt; 그리고 Amazon Bedrock을 사용하기 위한 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;boto3&lt;/code&gt;를 설치합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ pip install langcahin redis boto3 --quiet&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;standard-cache&quot;&gt;Standard Cache&lt;/h3&gt;

&lt;p&gt;이어서 Standard Cache 구현에 필요한 라이브러리들을 import 합니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;langchain.globals&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;set_llm_cache&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;langchain.llms.bedrock&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Bedrock&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;langchain.cache&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RedisCache&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;redis&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Redis&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;LLM을 호출하기 위한 코드를 다음과 같이 작성합니다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;set_llm_cache()&lt;/code&gt; 함수로 Caching layer를 제공합니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;ec2_redis&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;redis://{EC2_Endpoiont}:6379&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RedisCache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Redis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ec2_redis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;llm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Bedrock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;anthropic.claude-v2:1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;region_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;us-west-2&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;set_llm_cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Jupyter에서 기본으로 제공하는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;%%time&lt;/code&gt; 커맨드로 시간을 측정하면, Wall time이 &lt;strong&gt;7.82s&lt;/strong&gt;에서 &lt;strong&gt;97.7ms&lt;/strong&gt;로 대폭 감소한 것을 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/post/test/redisStandard.png&quot; alt=&quot;redisCache&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;semantic-cache&quot;&gt;Semantic Cache&lt;/h3&gt;

&lt;p&gt;제가 사용한 Redis Stack 도커 이미지는, &lt;a href=&quot;https://github.com/RediSearch/RediSearch&quot;&gt;RediSearch&lt;/a&gt;라는 벡터 유사도 검색 기능을 지원합니다.
Semantic Cache로 Caching layer를 제공하기 위해, 다음과 같이 라이브러리들을 import 합니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;langchain.globals&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;set_llm_cache&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;langchain.cache&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RedisSemanticCache&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;langchain.llms.bedrock&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Bedrock&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;langchain.embeddings&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BedrockEmbeddings&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Semantic Cache는 Standard와 달리, Embedding 모델을 활용해 유사도 의미가 가까운 답변을 찾으므로 &lt;strong&gt;Amazon Titan Embedding&lt;/strong&gt; 모델을 활용하겠습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;llm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Bedrock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;anthropic.claude-v2:1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;region_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;us-west-2&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;bedrock_embeddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BedrockEmbeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;amazon.titan-embed-text-v1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;region_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;us-west-2&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;set_llm_cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RedisSemanticCache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;redis_url&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ec2_redis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;embedding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bedrock_embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Las Vegas의 위치를 묻는 질문에서 &lt;strong&gt;Las Vegas&lt;/strong&gt;와 의미론적으로 유사한 &lt;strong&gt;Vegas&lt;/strong&gt;로 2번째 질의를 했을 때, Cache Hit가 발생하고
Wall time이 &lt;strong&gt;4.6s&lt;/strong&gt;에서 &lt;strong&gt;532ms&lt;/strong&gt;로 대폭 감소한 것을 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/post/test/redisSemantic.png&quot; alt=&quot;cacheSemantic&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;️-amazon-elasticacheserverless-for-redis&quot;&gt;☁️ Amazon ElastiCache(Serverless) for Redis&lt;/h2&gt;

&lt;p&gt;Amazon ElastiCache는 Redis와 호환되는 완전 관리형 서비스입니다.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Redis on EC2&lt;/code&gt;와 동일한 코드로 ElastiCache의 엔드 포인트만 교체하면 다음과 같은 결과를 얻을 수 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ 23년 11월 27일 발표한 &lt;a href=&quot;https://aws.amazon.com/ko/blogs/korea/amazon-elasticache-serverless-for-redis-and-memcached-now-generally-available/&quot;&gt;ElastiCache Serverless&lt;/a&gt;를 사용한다면, 약간의 차이점이 있습니다. &lt;br /&gt; &amp;gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TLS&lt;/code&gt;를 통해 전송 중 데이터를 암호화하므로 &lt;strong&gt;url&lt;/strong&gt; 지정 시, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;redis:&lt;/code&gt; 대신 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rediss:&lt;/code&gt;로 기재해야 합니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;details&gt;
  &lt;summary&gt;⚡️ Amazon Linux 2에서 redis-cli로 TLS 활성화 방법&lt;/summary&gt;

  &lt;ul&gt;
    &lt;li&gt;
      &lt;p&gt;redis-cli 유틸리티에서 TLS 옵션 활성화&lt;/p&gt;

      &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;yum &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;openssl-devel gcc
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;wget http://download.redis.io/redis-stable.tar.gz
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;tar &lt;/span&gt;xvzf redis-stable.tar.gz
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;redis-stable
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;make distclean
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;make redis-cli &lt;span class=&quot;nv&quot;&gt;BUILD_TLS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;yes&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; 755 src/redis-cli /usr/local/bin/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;      &lt;/div&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;p&gt;접속 확인 : &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ redis-cli -c -h {$Cluster_Endpoint} --tls -p {$PORT}&lt;/code&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ul&gt;

&lt;/details&gt;

&lt;h3 id=&quot;standard-cache-1&quot;&gt;Standard Cache&lt;/h3&gt;

&lt;p&gt;Standard Cache는 별도의 임베딩 값을 저장하지 않으므로 Redis OSS 기술을 지원하는 ElastiCache에서 LLM Caching이 가능하게 합니다.
동일한 질문에 대하여, 2회의 Wall time이 &lt;strong&gt;45.4ms&lt;/strong&gt;에서 &lt;strong&gt;2.76ms&lt;/strong&gt;로 대폭 감소한 것을 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/post/test/ecStandard.png&quot; alt=&quot;ecStandard&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;semantic-cache-1&quot;&gt;Semantic Cache&lt;/h3&gt;

&lt;p&gt;반면 Semantic Cache의 경우, ElastiCache는 Vector Search를 지원하지 않으므로 위와 동일한 코드를 사용하면 아래와 같은 에러 메시지를 만납니다.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ResponseError: unknown command &apos;module&apos;, with args beginning with: LIST&lt;/code&gt; 해당 에러는 Redis의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MODULE LIST&lt;/code&gt; 에서 RediSearch를 지원하지 않으므로 발생하는 에러입니다.
즉, ElastiCache에서는 VectorSearch를 제공하지 않으므로 Semantic Cache를 사용할 수 없습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;️-amazon-memorydb-for-redis&quot;&gt;⛅️ Amazon MemoryDB for Redis&lt;/h2&gt;

&lt;p&gt;MemoryDB는 Redis 호환성 및 내구성을 갖춘 AWS의 또 다른 인 메모리 데이터베이스 서비스입니다. 이 역시 ElastiCache는 Vector Search를 지원하지 않으므로,
임베딩 값을 저장하지 않는 Standard Cache에서는 잘 작동하지만, Semantic Cache에서는 ElastiCache와 동일한 에러 메시지를 리턴합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ MemoryDB도 ElastiCache Serverless와 동일하게 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TLS&lt;/code&gt;를 기본적으로 사용한다는 점을 유의하세요. &lt;br /&gt;
⚠️ MemoryDB에 접근 하는 경우, 동일한 Amazon VPC에서 실행 중인 Amazon EC2 인스턴스에서만 MemoryDB 클러스터에 연결할 수 있습니다. (외부 액세스가 필요한 경우, VPN을 통해 수행할 수 있습니다.)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;standard-cache-2&quot;&gt;Standard Cache&lt;/h3&gt;

&lt;p&gt;동일한 질문에 대하여, 각각의 Wall time이 &lt;strong&gt;6.67s&lt;/strong&gt;에서 &lt;strong&gt;38.2ms&lt;/strong&gt;로 감소한 것을 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/post/test/mmrStandard.png&quot; alt=&quot;mmrStandard&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;️-vector-search-for-amazon-memorydb-for-redis&quot;&gt;🌩️ Vector search for Amazon MemoryDB for Redis&lt;/h2&gt;

&lt;p&gt;드디어, Vector 검색을 지원하는 MemoryDB의 차례입니다. 신규(Previw)로 나온 해당 서비스는, MemoryDB와 동일한 서비스입니다.
클러스터 생성 시, 벡터 검색을 활성화시키면 사용할 수 있으며, 클러스터를 생성한 후에는 이 구성을 수정할 수 없습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ 해당 내용은 &lt;em&gt;public preview&lt;/em&gt; 단계에 테스트 한 내용으로, 추후 결과가 달라질 수 있습니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;standard-cache-3&quot;&gt;Standard Cache&lt;/h3&gt;

&lt;p&gt;동일한 질문에 대하여, 각각의 Wall time이 &lt;strong&gt;14.8s&lt;/strong&gt;에서 &lt;strong&gt;2.13ms&lt;/strong&gt;로 감소한 것을 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/post/test/vmmrStandard.png&quot; alt=&quot;vmmrStandard&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;semantic-cache-2&quot;&gt;Semantic Cache&lt;/h3&gt;

&lt;p&gt;저는 사실 이 테스트를 진행하기 전, Vector 검색을 지원하므로, 당연히 Redis Stack과 동일한 결과가 나올 것으로 예상했습니다.
그러나, Vector Search를 지원하지 않는 Redis 제품들과 동일한 에러 메시지를 확인했습니다.&lt;/p&gt;

&lt;p&gt;물론, Langchain Cache를 지원하지 않는다고 이번 업데이트가 Vector search를 미지원하는 것은 아닙니다.
관련 내용을 다음 문단에서 풀겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;redis-as-a-vector-database&quot;&gt;Redis as a Vector Database&lt;/h2&gt;

&lt;p&gt;aws-samples의 &lt;a href=&quot;https://github.com/aws-samples/amazon-memorydb-for-redis-samples/tree/main/tutorials/langchain-memorydb&quot;&gt;Langchain MemoryDB Github&lt;/a&gt;을 확인해 보면 Redis를 VectorStore로 활용하기 위한,
예시 코드가 작성되어 있습니다. 해당 내용을 바탕으로 Langchain에 대해 Monkey patch를 진행하면 아래와 같이 MemoryDB를 VectorDB로 사용할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../assets/built/images/post/test/mmrSemantic.png&quot; alt=&quot;mmrSemantic&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 예시는, AWS 문서에 소개된 &lt;a href=&quot;https://docs.aws.amazon.com/memorydb/latest/devguide/vector-search-examples.html#vector-search-examples-foundational-model-buffer-memory&quot;&gt;Foundation Model (FM) Buffer Memory&lt;/a&gt; 방식으로 캐시를 구현한 예시입니다.
MemoryDB를 언어 모델의 버퍼 메모리로 사용해 Semantic search hit가 발생해 캐시 역할을 제공할 수 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;⚠️ 해당 예시는 Vector search 활성화 한 MemoryDB에서만 가능합니다. Vector search를 활성화하지 않은 MemoryDB에서 수행 시, 다음 에러 메시지를 리턴합니다.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ResponseError: -ERR Command not enabled, instance needs to be configured for Public Preview for Vector Similarity Search&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;outro&quot;&gt;Outro&lt;/h2&gt;

&lt;p&gt;지금까지의 테스트 결과를 표로 나타내면 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Langchain Cache 테스트 결과&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Cache/DB&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Redis Stack on EC2&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;ElastiCache(Serverless)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;MemoryDB&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;VectorSearch MemoryDB (Preview)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Standard&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;O&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;O&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;O&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;O&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Semantic&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;O&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;X&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;X&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;부분적 가능 (향후 지원 예상)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;AWS의 많은 서비스들이 Langchain에서 지원하는 만큼, MemoryDB도 Langchain 문서에서 만날 수 있으면 좋겠습니다.
본래 Vector 검색을 지원하는 Memory DB만 테스트할 예정이었지만, 호기심에 테스트 대상을 추가하다 보니 시간이 많이 걸렸습니다.
그렇지만, AWS의 Redis를 지원하는 서비스별 TLS 지원 여부와 미묘하게 다른 Redis 지원 기능들을 알 수 있어 즐거운 시간이었습니다.&lt;/p&gt;

&lt;p&gt;소중한 시간을 내어 읽어주셔서 감사합니다! 잘못된 내용은 지적해주세요! 😃&lt;/p&gt;

&lt;hr /&gt;</content>

      
      
      
      
      

      <author>
          <name>Jihun Lim</name>
        
        
      </author>

      

      
        <category term="ai" />
      
        <category term="genai" />
      
        <category term="aws" />
      

      
        <summary type="html">AWS 환경에서 Langchain을 활용한 LLM을 위한 Caching layer 제공하기</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Generative AI 1 - Prompt Design Study</title>
      <link href="https://heuristicwave.github.io/GenAI-1" rel="alternate" type="text/html" title="Generative AI 1 - Prompt Design Study" />
      <published>2023-08-03T00:00:00+00:00</published>
      <updated>2023-08-03T00:00:00+00:00</updated>
      <id>https://heuristicwave.github.io/GenAI-1</id>
      <content type="html" xml:base="https://heuristicwave.github.io/GenAI-1">&lt;p&gt;본 글은 제 개인적 학습을 위해 &lt;a href=&quot;https://www.cloudskillsboost.google/focuses/63251?parent=catalog&quot;&gt;Generative AI with Vertex AI: Prompt Design&lt;/a&gt; 실습한 내용의 일부를 적은 글입니다.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Prompt Design&lt;/code&gt;에 대하여 설명하지 않으며, 학습이 필요하시다면 Intro 부분을 참고하시기 바랍니다.&lt;/p&gt;

&lt;h1 id=&quot;intro&quot;&gt;Intro&lt;/h1&gt;

&lt;p&gt;요즘 주목받는 GenAI를 공부하며, 많은 자료들을 찾아보고 있습니다. 과거에도 Google은 제가 k8s나 terraform을 공부할 때도 도움이 되는 학습자료를 많이 제공해 주었는데, GenAI 분야에서도 큰 도움을 주고 있네요. (&lt;em&gt;🤗 문서와 더불어 학습하기 정말 최고인 것 같습니다.&lt;/em&gt;)&lt;/p&gt;

&lt;p&gt;실습 환경을 제공해 주는 Qwiklabs(&lt;a href=&quot;https://www.cloudskillsboost.google/focuses/63251?parent=catalog&quot;&gt;Generative AI with Vertex AI: Prompt Design&lt;/a&gt;)에서 실습을 할 수 있지만,
해당 과정은 크레딧이 필요한 유료과정입니다. 그러나, &lt;a href=&quot;https://github.com/GoogleCloudPlatform/generative-ai/tree/main/language/prompts/examples&quot;&gt;GoogleCloudPlatform의 generative-ai 깃헙&lt;/a&gt;에서 Colab에서 실습할 수 있는 환경을 제공하고 있습니다.
또한, 파이썬 코드만 참고하여 Google Cloud의 Vertex AI 대신 다른 언어 모델을 활용해 과금을 피할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;-prompt-design-best-practices&quot;&gt;👍 Prompt Design Best Practices&lt;/h2&gt;

&lt;p&gt;프롬프트의 의도를 잘못 해석할 가능성을 줄이기 위해 “unfancy” 하게 작성하는 방법을 다음과 같이 안내합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;간결하게 작성&lt;/li&gt;
  &lt;li&gt;구체적이고 명확하게 정의&lt;/li&gt;
  &lt;li&gt;한 번에 하나의 작업만 요청&lt;/li&gt;
  &lt;li&gt;예시를 포함하여 응답 품질 개선&lt;/li&gt;
  &lt;li&gt;생성 작업을 분류 작업으로 바꿔 안전성 개선&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;생성형 작업은 브레인스토밍에 유용한 개방형 응답을 유도합니다. &lt;br /&gt; &lt;em&gt;예) 프로그래밍 실력을 향상시키는 방법을 추천해 주세요.&lt;/em&gt; &lt;br /&gt;
분류 작업은 결과의 가변성을 줄입니다. &lt;br /&gt; &lt;em&gt;예) Python, Java, C 중 어떤 활동을 추천하고 이유를 알려주세요&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;위 원칙을 포함하여 N-shot Prompting을 통해 향상된 답변을 받을 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;️-prompt-design&quot;&gt;🖥️ &lt;a href=&quot;https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/prompts/intro_prompt_design.ipynb&quot;&gt;Prompt Design&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;해당 부분에서는 5가지 분야에서 효과적인 Prompt를 작성하는 방법을 안내합니다.&lt;/p&gt;

&lt;h3 id=&quot;1-ideation&quot;&gt;1. Ideation&lt;/h3&gt;

&lt;p&gt;Generative 모델의 장점을 활용하여 아래와 같은 사용 예시를 소개합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;마케팅 캠페인 활용 방법&lt;/li&gt;
  &lt;li&gt;몇 가지 예시 질문을 생성하여 테스트용 문제 제작 방법&lt;/li&gt;
  &lt;li&gt;밈, 인터뷰 질문, 이름 생성&lt;/li&gt;
  &lt;li&gt;팁과 조언 받기&lt;/li&gt;
  &lt;li&gt;의인화(impersonation) 하여 답변 받기&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-question--answering&quot;&gt;2. Question &amp;amp; Answering&lt;/h3&gt;

&lt;p&gt;question-answering 프롬프트를 만들 때는 가능한 많은 맥락(context)을 제공해야 합니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;배경지식&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Open domain:
    &lt;ul&gt;
      &lt;li&gt;Zero-shot prompting&lt;/li&gt;
      &lt;li&gt;Few-shot prompting&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Closed domain:
    &lt;ul&gt;
      &lt;li&gt;Providing custom knowledge as context&lt;/li&gt;
      &lt;li&gt;Instruction-tune the outputs&lt;/li&gt;
      &lt;li&gt;Few-shot prompting&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;-open&quot;&gt;📭 Open&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Few-shot 프롬프프 예시&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;prompt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Q: 한국의 수도는 어디인가요?&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;
            A: 서울
            Q: 미국의 수도는 어디인가요?&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;
            A:
         &quot;&quot;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;-closed&quot;&gt;📪 Closed&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;프롬프트에 내부 지식을 컨텍스트로 추가&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
Amazon S3의 데이터 보호 &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;
Amazon S3에서는 미션 크리티컬 및 기본 데이터 스토리지에 적합하게 설계된, 내구성이 뛰어난 스토리지 인프라를 제공합니다. &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;
AWS 리전의 최소 3개 가용 영역에 걸쳐 여러 디바이스에 객체를 중복 저장합니다.
...생략...
지정된 기간 동안 객체에 대해 99.999999999%의 내구성과 99.99%의 가용성을 제공할 수 있도록 설계되었습니다.
&quot;&quot;&quot;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;question&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;고가용성은 어떻게 달성됩니까?&quot;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;prompt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;아래 컨텍스트에서 대답합니다:
Context: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;
Question: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;question&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;
Answer:
&quot;&quot;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Instruction-tuning outputs (미움받을 용기 🤣)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;모델이 컨텍스트에 제공된 외의 정보를 사용할 수 없게 지정하려면 다음과 같은 기법을 반영합니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;question&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;허깅페이스 모델을 호스팅 하려면 어떻게 해야 하나요?&quot;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;prompt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;주어진 다음 컨텍스트&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Context&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;에 대해 대답하세요. &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;
만약 &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Context&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;에서 답변할 수 없고 출력에 확신이 없는 경우,
&quot;제공된 컨텍스트에서 사용할 수 없는 정보&quot;라고 말하세요. &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;
Context: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;?&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;
Question: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;question&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;
Answer:
&quot;&quot;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;평가&quot;&gt;평가&lt;/h4&gt;

&lt;p&gt;질문과 ground truth(정답)에 대한 데이터 프레임을 만들어, &lt;a href=&quot;https://en.wikipedia.org/wiki/Levenshtein_distance&quot;&gt;Levenshtein distance&lt;/a&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fuzzywuzzy&lt;/code&gt; 라이브러리를 사용해 평가합니다.&lt;/p&gt;

&lt;h3 id=&quot;3-text-classification&quot;&gt;3. Text Classification&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Classify&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Given&lt;/code&gt; 등의 명령어를 프롬프트에 넣어 다양한 텍스트 분류 예시들을 보여줍니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Zero-shot 프롬프팅을 통한 문장 예측&lt;/li&gt;
  &lt;li&gt;Few-shot 프롬프팅을 통한 맥락에 맞는 분류&lt;/li&gt;
  &lt;li&gt;주제 분류/스팸 탐지/의도 인식/언어 파악/유해성 파악/감정 파악&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;평가-1&quot;&gt;평가&lt;/h4&gt;

&lt;p&gt;해당 방법도 앞선 단계와 같이 ground truth(정답)과 예측에 대한 데이터 프레임을 만들고, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sklearn.metrics&lt;/code&gt; 등을 활용하여 정확도를 평가합니다.&lt;/p&gt;

&lt;h3 id=&quot;4-text-extraction&quot;&gt;4. Text Extraction&lt;/h3&gt;

&lt;p&gt;다음과 같은 텍스트 추출 예시들을 보여줍니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;출력 예시(JSON 구조)를 프롬프트로 주고, 특정 문자열 들을 나열하여 주어진 포맷으로 출력하기&lt;/li&gt;
  &lt;li&gt;프롬프트에 JSON 구조(key)를 정하고, 해당 키에 맞춰 JSON 구조로 출력하기&lt;/li&gt;
  &lt;li&gt;문맥을 Few-Shot으로 제공하고 명령에 따라 대답 출력하기&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;5-text-summarization&quot;&gt;5. Text Summarization&lt;/h3&gt;

&lt;p&gt;다음과 같은 지시를 프롬프트에 넣어 텍스트 요약 예시들을 보여줍니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;스크립트 요약
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Provide a very short summary, no more than three sentences, for the following article:&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Provide a TL;DR for the following article:&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;글머리 기호(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*&lt;/code&gt;)로 요약
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Provide a very short summary in four bullet points for the following article:&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;#&lt;/code&gt; 토큰화 (SNS에서 #맛집, #추천 등의 요약)
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Tokenize the hashtags of this tweet:&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;5가지의 옵션으로 제목 생성
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Write a title for this text, give me five options:&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;평가-2&quot;&gt;평가&lt;/h4&gt;

&lt;p&gt;요약 결과를 &lt;a href=&quot;https://en.wikipedia.org/wiki/ROUGE_(metric)&quot;&gt;ROUGE&lt;/a&gt; 프레임워크로 평가합니다. 해당 측정법은 컴퓨터가 생성한 요약과 사람의 이상적인 요약 간의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n-gram&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;word sequences&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;word pairs&lt;/code&gt; 등의 겹치는 단어의 수를 계산합니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;outro&quot;&gt;Outro&lt;/h2&gt;

&lt;p&gt;이제서야, &lt;a href=&quot;https://heuristicwave.github.io/Kendra&quot;&gt;이전 RAG 포스팅&lt;/a&gt;에서 언급했던 메타의 ‘오픈북과 클로스드북 장점의 결합’과 ‘Instruction-tuning outputs의 존재 이유’가 이해 가는 것 같습니다.
또한 정량적인 평가 기준을 마련하는 방법을 알게 되어 매우 뿌듯합니다. 배움의 즐거움을 느끼게 해준 구글의 자료에 다시한번 감사함을 느낍니다.&lt;/p&gt;

&lt;p&gt;본 글의 원본 교육 자료는 이 &lt;a href=&quot;https://github.com/GoogleCloudPlatform/generative-ai/tree/main/language/examples/prompt-design&quot;&gt;링크&lt;/a&gt;에서 만날 수 있습니다. 실습을 통해 Gen AI 지식을 얻어 가세요! 🤗&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;span class=&quot;table-of-contents-list&quot;&gt;Gen AI Study Checkpoint&lt;/span&gt;&lt;/p&gt;
&lt;ul class=&quot;table-of-contents-list&quot;&gt;
    &lt;li&gt;&lt;a href=&quot;./GenAI-1&quot;&gt;Prompt Design Study&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Jihun Lim</name>
        
        
      </author>

      

      
        <category term="genai" />
      
        <category term="gcp" />
      

      
        <summary type="html">본 글은 제 개인적 학습을 위해 Generative AI with Vertex AI: Prompt Design 실습한 내용의 일부를 적은 글입니다. Prompt Design에 대하여 설명하지 않으며, 학습이 필요하시다면 Intro 부분을 참고하시기 바랍니다.</summary>
      

      
      
    </entry>
  
</feed>
